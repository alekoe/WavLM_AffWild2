{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f42e82b-46c1-48bc-a9bd-eda21ca7a3c1",
   "metadata": {},
   "source": [
    "\n",
    "# Aggregate WavLM features to adapt to label files\n",
    "\n",
    "### For AffWild2 Train and Validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760109a1-c818-49cf-abd3-377b65994a33",
   "metadata": {},
   "source": [
    "##### https://github.com/microsoft/unilm/tree/master/wavlm\n",
    "##### https://github.com/audeering/w2v2-how-to/blob/main/notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8263d9b9-de2b-418e-b1a5-da5323499a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814ae24",
   "metadata": {},
   "source": [
    "### Process All Feature Files - Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f98fa97-9848-495d-b793-ba9687c3dc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/etsmtl/akoerich/DEV/WavLM\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "191da55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1-30-1280x720.wavlmpooled',\n",
       " '10-60-1280x720.wavlmpooled',\n",
       " '100-29-1080x1920.wavlmpooled',\n",
       " '101-30-1080x1920.wavlmpooled',\n",
       " '104-17-720x480.wavlmpooled',\n",
       " '106-30-720x1280.wavlmpooled',\n",
       " '107-30-640x480.wavlmpooled',\n",
       " '110-30-270x480.wavlmpooled',\n",
       " '111-25-1920x1080.wavlmpooled',\n",
       " '112-30-640x360.wavlmpooled',\n",
       " '113-60-1280x720.wavlmpooled',\n",
       " '114-30-1280x720.wavlmpooled',\n",
       " '115-30-1280x720.wavlmpooled',\n",
       " '116-30-1280x720.wavlmpooled',\n",
       " '117-25-1920x1080.wavlmpooled',\n",
       " '118-30-640x480.wavlmpooled',\n",
       " '119-30-848x480.wavlmpooled',\n",
       " '12-24-1920x1080.wavlmpooled',\n",
       " '121-24-1920x1080.wavlmpooled',\n",
       " '122-60-1920x1080-1.wavlmpooled',\n",
       " '122-60-1920x1080-2.wavlmpooled',\n",
       " '122-60-1920x1080-3.wavlmpooled',\n",
       " '122-60-1920x1080-4.wavlmpooled',\n",
       " '122-60-1920x1080-5.wavlmpooled',\n",
       " '124-30-720x1280.wavlmpooled',\n",
       " '125-25-1280x720.wavlmpooled',\n",
       " '126-30-1080x1920.wavlmpooled',\n",
       " '127-30-1280x720.wavlmpooled',\n",
       " '129-24-1280x720.wavlmpooled',\n",
       " '13-30-1920x1080.wavlmpooled',\n",
       " '131-30-1920x1080.wavlmpooled',\n",
       " '132-30-426x240.wavlmpooled',\n",
       " '136-30-1920x1080.wavlmpooled',\n",
       " '138-30-1280x720.wavlmpooled',\n",
       " '139-14-720x480.wavlmpooled',\n",
       " '140-30-632x360.wavlmpooled',\n",
       " '15-24-1920x1080.wavlmpooled',\n",
       " '16-30-1920x1080.wavlmpooled',\n",
       " '17-24-1920x1080.wavlmpooled',\n",
       " '18-24-1920x1080.wavlmpooled',\n",
       " '19-24-1920x1080.wavlmpooled',\n",
       " '20-24-1920x1080.wavlmpooled',\n",
       " '201.wavlmpooled',\n",
       " '202.wavlmpooled',\n",
       " '203.wavlmpooled',\n",
       " '206.wavlmpooled',\n",
       " '207.wavlmpooled',\n",
       " '208.wavlmpooled',\n",
       " '21-24-1920x1080.wavlmpooled',\n",
       " '210.wavlmpooled',\n",
       " '211.wavlmpooled',\n",
       " '213.wavlmpooled',\n",
       " '214.wavlmpooled',\n",
       " '215.wavlmpooled',\n",
       " '216.wavlmpooled',\n",
       " '218.wavlmpooled',\n",
       " '22-30-1920x1080.wavlmpooled',\n",
       " '220.wavlmpooled',\n",
       " '221.wavlmpooled',\n",
       " '223.wavlmpooled',\n",
       " '224.wavlmpooled',\n",
       " '225.wavlmpooled',\n",
       " '226.wavlmpooled',\n",
       " '227.wavlmpooled',\n",
       " '228.wavlmpooled',\n",
       " '229.wavlmpooled',\n",
       " '23-24-1920x1080.wavlmpooled',\n",
       " '230.wavlmpooled',\n",
       " '231.wavlmpooled',\n",
       " '232.wavlmpooled',\n",
       " '233.wavlmpooled',\n",
       " '235.wavlmpooled',\n",
       " '236.wavlmpooled',\n",
       " '237.wavlmpooled',\n",
       " '238.wavlmpooled',\n",
       " '24-30-1920x1080-1.wavlmpooled',\n",
       " '24-30-1920x1080-2.wavlmpooled',\n",
       " '240.wavlmpooled',\n",
       " '242.wavlmpooled',\n",
       " '243.wavlmpooled',\n",
       " '245.wavlmpooled',\n",
       " '246.wavlmpooled',\n",
       " '247.wavlmpooled',\n",
       " '248.wavlmpooled',\n",
       " '249.wavlmpooled',\n",
       " '25-25-600x480.wavlmpooled',\n",
       " '250.wavlmpooled',\n",
       " '251.wavlmpooled',\n",
       " '252.wavlmpooled',\n",
       " '253.wavlmpooled',\n",
       " '254.wavlmpooled',\n",
       " '255.wavlmpooled',\n",
       " '256.wavlmpooled',\n",
       " '257.wavlmpooled',\n",
       " '258.wavlmpooled',\n",
       " '259.wavlmpooled',\n",
       " '26-60-1280x720.wavlmpooled',\n",
       " '260.wavlmpooled',\n",
       " '261.wavlmpooled',\n",
       " '262.wavlmpooled',\n",
       " '264.wavlmpooled',\n",
       " '269.wavlmpooled',\n",
       " '27-60-1280x720.wavlmpooled',\n",
       " '271.wavlmpooled',\n",
       " '272.wavlmpooled',\n",
       " '273.wavlmpooled',\n",
       " '275.wavlmpooled',\n",
       " '276.wavlmpooled',\n",
       " '277.wavlmpooled',\n",
       " '278.wavlmpooled',\n",
       " '279.wavlmpooled',\n",
       " '28-30-1280x720-1.wavlmpooled',\n",
       " '28-30-1280x720-2.wavlmpooled',\n",
       " '28-30-1280x720-3.wavlmpooled',\n",
       " '28-30-1280x720-4.wavlmpooled',\n",
       " '280.wavlmpooled',\n",
       " '282.wavlmpooled',\n",
       " '283.wavlmpooled',\n",
       " '285.wavlmpooled',\n",
       " '287.wavlmpooled',\n",
       " '288.wavlmpooled',\n",
       " '29-24-1280x720.wavlmpooled',\n",
       " '290.wavlmpooled',\n",
       " '291.wavlmpooled',\n",
       " '292.wavlmpooled',\n",
       " '293.wavlmpooled',\n",
       " '294.wavlmpooled',\n",
       " '295.wavlmpooled',\n",
       " '296.wavlmpooled',\n",
       " '297.wavlmpooled',\n",
       " '298.wavlmpooled',\n",
       " '299.wavlmpooled',\n",
       " '305.wavlmpooled',\n",
       " '306.wavlmpooled',\n",
       " '308.wavlmpooled',\n",
       " '309.wavlmpooled',\n",
       " '31-30-1920x1080.wavlmpooled',\n",
       " '312.wavlmpooled',\n",
       " '314.wavlmpooled',\n",
       " '315.wavlmpooled',\n",
       " '317.wavlmpooled',\n",
       " '318.wavlmpooled',\n",
       " '319.wavlmpooled',\n",
       " '320.wavlmpooled',\n",
       " '321.wavlmpooled',\n",
       " '323.wavlmpooled',\n",
       " '324.wavlmpooled',\n",
       " '325.wavlmpooled',\n",
       " '326.wavlmpooled',\n",
       " '327.wavlmpooled',\n",
       " '328.wavlmpooled',\n",
       " '329.wavlmpooled',\n",
       " '33-30-1920x1080.wavlmpooled',\n",
       " '330.wavlmpooled',\n",
       " '331.wavlmpooled',\n",
       " '332.wavlmpooled',\n",
       " '334.wavlmpooled',\n",
       " '335.wavlmpooled',\n",
       " '337.wavlmpooled',\n",
       " '339.wavlmpooled',\n",
       " '34-25-1920x1080.wavlmpooled',\n",
       " '341.wavlmpooled',\n",
       " '345.wavlmpooled',\n",
       " '346.wavlmpooled',\n",
       " '347.wavlmpooled',\n",
       " '348.wavlmpooled',\n",
       " '350.wavlmpooled',\n",
       " '354.wavlmpooled',\n",
       " '355.wavlmpooled',\n",
       " '359.wavlmpooled',\n",
       " '360.wavlmpooled',\n",
       " '361.wavlmpooled',\n",
       " '362.wavlmpooled',\n",
       " '363.wavlmpooled',\n",
       " '364.wavlmpooled',\n",
       " '368.wavlmpooled',\n",
       " '369.wavlmpooled',\n",
       " '37-30-1280x720.wavlmpooled',\n",
       " '371.wavlmpooled',\n",
       " '372.wavlmpooled',\n",
       " '373.wavlmpooled',\n",
       " '374.wavlmpooled',\n",
       " '376.wavlmpooled',\n",
       " '377.wavlmpooled',\n",
       " '378.wavlmpooled',\n",
       " '383.wavlmpooled',\n",
       " '384.wavlmpooled',\n",
       " '385.wavlmpooled',\n",
       " '386.wavlmpooled',\n",
       " '387.wavlmpooled',\n",
       " '388.wavlmpooled',\n",
       " '389.wavlmpooled',\n",
       " '39-25-424x240.wavlmpooled',\n",
       " '391.wavlmpooled',\n",
       " '392.wavlmpooled',\n",
       " '393.wavlmpooled',\n",
       " '394.wavlmpooled',\n",
       " '395.wavlmpooled',\n",
       " '398.wavlmpooled',\n",
       " '399.wavlmpooled',\n",
       " '4-30-1920x1080.wavlmpooled',\n",
       " '400.wavlmpooled',\n",
       " '402.wavlmpooled',\n",
       " '403.wavlmpooled',\n",
       " '406.wavlmpooled',\n",
       " '407.wavlmpooled',\n",
       " '409.wavlmpooled',\n",
       " '41-24-1280x720.wavlmpooled',\n",
       " '412.wavlmpooled',\n",
       " '415.wavlmpooled',\n",
       " '418.wavlmpooled',\n",
       " '419.wavlmpooled',\n",
       " '420.wavlmpooled',\n",
       " '421.wavlmpooled',\n",
       " '423.wavlmpooled',\n",
       " '424.wavlmpooled',\n",
       " '425.wavlmpooled',\n",
       " '426.wavlmpooled',\n",
       " '427.wavlmpooled',\n",
       " '428.wavlmpooled',\n",
       " '429.wavlmpooled',\n",
       " '430.wavlmpooled',\n",
       " '433.wavlmpooled',\n",
       " '435.wavlmpooled',\n",
       " '44-25-426x240.wavlmpooled',\n",
       " '440.wavlmpooled',\n",
       " '441.wavlmpooled',\n",
       " '446.wavlmpooled',\n",
       " '447.wavlmpooled',\n",
       " '448.wavlmpooled',\n",
       " '449.wavlmpooled',\n",
       " '45-24-1280x720.wavlmpooled',\n",
       " '47-30-654x480.wavlmpooled',\n",
       " '48-30-720x1280.wavlmpooled',\n",
       " '5-60-1920x1080-1.wavlmpooled',\n",
       " '5-60-1920x1080-2.wavlmpooled',\n",
       " '5-60-1920x1080-3.wavlmpooled',\n",
       " '5-60-1920x1080-4.wavlmpooled',\n",
       " '50-30-1920x1080.wavlmpooled',\n",
       " '51-30-1280x720.wavlmpooled',\n",
       " '53-30-360x480.wavlmpooled',\n",
       " '54-30-1080x1920.wavlmpooled',\n",
       " '57-25-426x240.wavlmpooled',\n",
       " '59-30-1280x720.wavlmpooled',\n",
       " '60-30-1920x1080.wavlmpooled',\n",
       " '61-24-1920x1080.wavlmpooled',\n",
       " '63-30-1920x1080.wavlmpooled',\n",
       " '64-24-640x360.wavlmpooled',\n",
       " '65-30-400x228.wavlmpooled',\n",
       " '66-25-1080x1920.wavlmpooled',\n",
       " '67-24-640x360.wavlmpooled',\n",
       " '68-24-1920x1080.wavlmpooled',\n",
       " '71-30-1920x1080.wavlmpooled',\n",
       " '72-30-1280x720.wavlmpooled',\n",
       " '75-30-960x720.wavlmpooled',\n",
       " '76-30-640x280.wavlmpooled',\n",
       " '77-30-1280x720.wavlmpooled',\n",
       " '78-30-960x720.wavlmpooled',\n",
       " '79-30-960x720.wavlmpooled',\n",
       " '8-30-1280x720.wavlmpooled',\n",
       " '81-30-576x360.wavlmpooled',\n",
       " '82-25-854x480.wavlmpooled',\n",
       " '83-24-1920x1080.wavlmpooled',\n",
       " '84-30-1920x1080.wavlmpooled',\n",
       " '85-24-1280x720.wavlmpooled',\n",
       " '86-24-1920x1080.wavlmpooled',\n",
       " '87-25-1920x1080.wavlmpooled',\n",
       " '89-30-1080x1920.wavlmpooled',\n",
       " '90-30-1080x1920.wavlmpooled',\n",
       " '92-24-1920x1080.wavlmpooled',\n",
       " '93-24-640x360.wavlmpooled',\n",
       " '94-30-1920x1080.wavlmpooled',\n",
       " '95-24-1920x1080.wavlmpooled',\n",
       " '96-30-1280x720.wavlmpooled',\n",
       " '97-29-1920x1080.wavlmpooled',\n",
       " '98-30-360x360.wavlmpooled',\n",
       " '99-30-720x720.wavlmpooled',\n",
       " 'video54.wavlmpooled',\n",
       " 'video56.wavlmpooled',\n",
       " 'video57.wavlmpooled',\n",
       " 'video58.wavlmpooled',\n",
       " 'video59.wavlmpooled',\n",
       " 'video60.wavlmpooled',\n",
       " 'video61.wavlmpooled',\n",
       " 'video62.wavlmpooled',\n",
       " 'video63.wavlmpooled',\n",
       " 'video64.wavlmpooled',\n",
       " 'video66.wavlmpooled',\n",
       " 'video67.wavlmpooled',\n",
       " 'video69.wavlmpooled',\n",
       " 'video70.wavlmpooled',\n",
       " 'video71.wavlmpooled',\n",
       " 'video75.wavlmpooled',\n",
       " 'video76.wavlmpooled',\n",
       " 'video77.wavlmpooled',\n",
       " 'video78.wavlmpooled',\n",
       " 'video79.wavlmpooled',\n",
       " 'video80.wavlmpooled',\n",
       " 'video81.wavlmpooled',\n",
       " 'video82.wavlmpooled',\n",
       " 'video83.wavlmpooled',\n",
       " 'video84.wavlmpooled',\n",
       " 'video85.wavlmpooled',\n",
       " 'video86_1.wavlmpooled',\n",
       " 'video86_2.wavlmpooled',\n",
       " 'video86_3.wavlmpooled',\n",
       " 'video87.wavlmpooled',\n",
       " 'video88.wavlmpooled',\n",
       " 'video89.wavlmpooled',\n",
       " 'video90.wavlmpooled',\n",
       " 'video91.wavlmpooled',\n",
       " 'video92.wavlmpooled',\n",
       " 'video93.wavlmpooled',\n",
       " 'video94.wavlmpooled',\n",
       " 'video95.wavlmpooled',\n",
       " 'video96.wavlmpooled']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to feature files\n",
    "path_train_valid_wavs = 'features/train_valid_pooled_new'\n",
    "extension_wav         = 'wavlmpooled'\n",
    "\n",
    "train_valid_wavs = [file for file in os.listdir(path_train_valid_wavs) if file.endswith(extension_wav)]\n",
    "\n",
    "sorted_train_valid_wavs = sorted(train_valid_wavs)\n",
    "sorted_train_valid_wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bd67beb-5411-4b53-83e5-8e0ccdc81951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_train_valid_wavs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aad4e9-9b3a-434a-b67b-f2b3380a8179",
   "metadata": {},
   "source": [
    "### Process Label Files - Train and Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5ca138a-b62d-4acf-9241-62894e5e91e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1-30-1280x720.txt',\n",
       " '10-60-1280x720.txt',\n",
       " '10-60-1280x720_right.txt',\n",
       " '100-29-1080x1920.txt',\n",
       " '101-30-1080x1920.txt',\n",
       " '104-17-720x480.txt',\n",
       " '105.txt',\n",
       " '106-30-720x1280.txt',\n",
       " '106.txt',\n",
       " '107-30-640x480.txt',\n",
       " '107.txt',\n",
       " '108.txt',\n",
       " '110-30-270x480.txt',\n",
       " '110.txt',\n",
       " '111-25-1920x1080.txt',\n",
       " '111.txt',\n",
       " '112-30-640x360.txt',\n",
       " '112.txt',\n",
       " '113-60-1280x720.txt',\n",
       " '113.txt',\n",
       " '114-30-1280x720.txt',\n",
       " '114.txt',\n",
       " '115-30-1280x720.txt',\n",
       " '116-30-1280x720.txt',\n",
       " '116.txt',\n",
       " '117-25-1920x1080.txt',\n",
       " '117.txt',\n",
       " '118-30-640x480.txt',\n",
       " '118.txt',\n",
       " '119-30-848x480.txt',\n",
       " '12-24-1920x1080.txt',\n",
       " '120-30-1280x720.txt',\n",
       " '120.txt',\n",
       " '121-24-1920x1080.txt',\n",
       " '121.txt',\n",
       " '122-60-1920x1080-1.txt',\n",
       " '122-60-1920x1080-2.txt',\n",
       " '122-60-1920x1080-3.txt',\n",
       " '122-60-1920x1080-4.txt',\n",
       " '122-60-1920x1080-5.txt',\n",
       " '122.txt',\n",
       " '123-25-1920x1080.txt',\n",
       " '123.txt',\n",
       " '124-30-720x1280.txt',\n",
       " '125-25-1280x720.txt',\n",
       " '125.txt',\n",
       " '126-30-1080x1920.txt',\n",
       " '126.txt',\n",
       " '127-30-1280x720.txt',\n",
       " '127.txt',\n",
       " '128.txt',\n",
       " '129-24-1280x720.txt',\n",
       " '129.txt',\n",
       " '13-30-1920x1080.txt',\n",
       " '131-30-1920x1080.txt',\n",
       " '131.txt',\n",
       " '132-30-426x240.txt',\n",
       " '132.txt',\n",
       " '133.txt',\n",
       " '134.txt',\n",
       " '135-24-1920x1080_left.txt',\n",
       " '135-24-1920x1080_right.txt',\n",
       " '135.txt',\n",
       " '136-30-1920x1080.txt',\n",
       " '136.txt',\n",
       " '137.txt',\n",
       " '138-30-1280x720.txt',\n",
       " '138.txt',\n",
       " '139-14-720x480.txt',\n",
       " '140-30-632x360.txt',\n",
       " '140.txt',\n",
       " '141.txt',\n",
       " '143.txt',\n",
       " '144.txt',\n",
       " '146.txt',\n",
       " '148.txt',\n",
       " '149.txt',\n",
       " '15-24-1920x1080.txt',\n",
       " '150.txt',\n",
       " '151.txt',\n",
       " '153.txt',\n",
       " '154.txt',\n",
       " '155.txt',\n",
       " '156.txt',\n",
       " '157.txt',\n",
       " '158.txt',\n",
       " '16-30-1920x1080.txt',\n",
       " '160.txt',\n",
       " '161.txt',\n",
       " '162.txt',\n",
       " '163.txt',\n",
       " '166.txt',\n",
       " '167.txt',\n",
       " '168.txt',\n",
       " '169.txt',\n",
       " '17-24-1920x1080.txt',\n",
       " '172.txt',\n",
       " '175.txt',\n",
       " '176.txt',\n",
       " '177.txt',\n",
       " '178.txt',\n",
       " '179.txt',\n",
       " '18-24-1920x1080.txt',\n",
       " '181.txt',\n",
       " '182.txt',\n",
       " '183.txt',\n",
       " '184.txt',\n",
       " '185.txt',\n",
       " '186.txt',\n",
       " '187.txt',\n",
       " '188.txt',\n",
       " '189.txt',\n",
       " '19-24-1920x1080.txt',\n",
       " '190.txt',\n",
       " '191.txt',\n",
       " '192.txt',\n",
       " '193.txt',\n",
       " '195.txt',\n",
       " '196.txt',\n",
       " '198.txt',\n",
       " '20-24-1920x1080.txt',\n",
       " '200.txt',\n",
       " '201.txt',\n",
       " '202.txt',\n",
       " '203.txt',\n",
       " '206.txt',\n",
       " '207.txt',\n",
       " '208.txt',\n",
       " '21-24-1920x1080.txt',\n",
       " '210.txt',\n",
       " '211.txt',\n",
       " '213.txt',\n",
       " '214.txt',\n",
       " '215.txt',\n",
       " '216.txt',\n",
       " '218.txt',\n",
       " '22-30-1920x1080.txt',\n",
       " '220.txt',\n",
       " '221.txt',\n",
       " '223.txt',\n",
       " '224.txt',\n",
       " '225.txt',\n",
       " '226.txt',\n",
       " '227.txt',\n",
       " '228.txt',\n",
       " '229.txt',\n",
       " '23-24-1920x1080.txt',\n",
       " '230.txt',\n",
       " '231.txt',\n",
       " '232.txt',\n",
       " '233.txt',\n",
       " '235.txt',\n",
       " '236.txt',\n",
       " '237.txt',\n",
       " '238.txt',\n",
       " '24-30-1920x1080-1.txt',\n",
       " '24-30-1920x1080-2.txt',\n",
       " '240.txt',\n",
       " '242.txt',\n",
       " '243.txt',\n",
       " '245.txt',\n",
       " '246.txt',\n",
       " '247.txt',\n",
       " '248.txt',\n",
       " '249.txt',\n",
       " '25-25-600x480.txt',\n",
       " '250.txt',\n",
       " '251.txt',\n",
       " '252.txt',\n",
       " '253.txt',\n",
       " '254.txt',\n",
       " '255.txt',\n",
       " '256.txt',\n",
       " '257.txt',\n",
       " '258.txt',\n",
       " '259.txt',\n",
       " '26-60-1280x720.txt',\n",
       " '260.txt',\n",
       " '261.txt',\n",
       " '262.txt',\n",
       " '264.txt',\n",
       " '269.txt',\n",
       " '27-60-1280x720.txt',\n",
       " '271.txt',\n",
       " '272.txt',\n",
       " '273.txt',\n",
       " '275.txt',\n",
       " '276.txt',\n",
       " '277.txt',\n",
       " '278.txt',\n",
       " '279.txt',\n",
       " '28-30-1280x720-1.txt',\n",
       " '28-30-1280x720-2.txt',\n",
       " '28-30-1280x720-3.txt',\n",
       " '28-30-1280x720-4.txt',\n",
       " '280.txt',\n",
       " '282.txt',\n",
       " '283.txt',\n",
       " '285.txt',\n",
       " '287.txt',\n",
       " '288.txt',\n",
       " '29-24-1280x720.txt',\n",
       " '290.txt',\n",
       " '291.txt',\n",
       " '292.txt',\n",
       " '293.txt',\n",
       " '294.txt',\n",
       " '295.txt',\n",
       " '296.txt',\n",
       " '297.txt',\n",
       " '298.txt',\n",
       " '299.txt',\n",
       " '30-30-1920x1080_left.txt',\n",
       " '30-30-1920x1080_right.txt',\n",
       " '305.txt',\n",
       " '306.txt',\n",
       " '308.txt',\n",
       " '309.txt',\n",
       " '31-30-1920x1080.txt',\n",
       " '312.txt',\n",
       " '314.txt',\n",
       " '315.txt',\n",
       " '317.txt',\n",
       " '318.txt',\n",
       " '319.txt',\n",
       " '320.txt',\n",
       " '321.txt',\n",
       " '323.txt',\n",
       " '324.txt',\n",
       " '325.txt',\n",
       " '326.txt',\n",
       " '327.txt',\n",
       " '328.txt',\n",
       " '329.txt',\n",
       " '33-30-1920x1080.txt',\n",
       " '330.txt',\n",
       " '331.txt',\n",
       " '332.txt',\n",
       " '334.txt',\n",
       " '335.txt',\n",
       " '337.txt',\n",
       " '339.txt',\n",
       " '34-25-1920x1080.txt',\n",
       " '341.txt',\n",
       " '345.txt',\n",
       " '346.txt',\n",
       " '347.txt',\n",
       " '348.txt',\n",
       " '35-30-1920x1080.txt',\n",
       " '350.txt',\n",
       " '354.txt',\n",
       " '355.txt',\n",
       " '359.txt',\n",
       " '360.txt',\n",
       " '361.txt',\n",
       " '362.txt',\n",
       " '363.txt',\n",
       " '364.txt',\n",
       " '368.txt',\n",
       " '369.txt',\n",
       " '37-30-1280x720.txt',\n",
       " '371.txt',\n",
       " '372.txt',\n",
       " '373.txt',\n",
       " '374.txt',\n",
       " '376.txt',\n",
       " '377.txt',\n",
       " '378.txt',\n",
       " '383.txt',\n",
       " '384.txt',\n",
       " '385.txt',\n",
       " '386.txt',\n",
       " '387.txt',\n",
       " '388.txt',\n",
       " '389.txt',\n",
       " '39-25-424x240.txt',\n",
       " '391.txt',\n",
       " '392.txt',\n",
       " '393.txt',\n",
       " '394.txt',\n",
       " '395.txt',\n",
       " '398.txt',\n",
       " '399.txt',\n",
       " '4-30-1920x1080.txt',\n",
       " '400.txt',\n",
       " '402.txt',\n",
       " '403.txt',\n",
       " '406.txt',\n",
       " '407.txt',\n",
       " '409.txt',\n",
       " '41-24-1280x720.txt',\n",
       " '412.txt',\n",
       " '415.txt',\n",
       " '418.txt',\n",
       " '419.txt',\n",
       " '420.txt',\n",
       " '421.txt',\n",
       " '423.txt',\n",
       " '424.txt',\n",
       " '425.txt',\n",
       " '426.txt',\n",
       " '427.txt',\n",
       " '428.txt',\n",
       " '429.txt',\n",
       " '430.txt',\n",
       " '433.txt',\n",
       " '435.txt',\n",
       " '44-25-426x240.txt',\n",
       " '440.txt',\n",
       " '441.txt',\n",
       " '446.txt',\n",
       " '447.txt',\n",
       " '448.txt',\n",
       " '449.txt',\n",
       " '45-24-1280x720.txt',\n",
       " '46-30-484x360_left.txt',\n",
       " '46-30-484x360_right.txt',\n",
       " '47-30-654x480.txt',\n",
       " '476.txt',\n",
       " '477.txt',\n",
       " '478.txt',\n",
       " '479.txt',\n",
       " '48-30-720x1280.txt',\n",
       " '480.txt',\n",
       " '486.txt',\n",
       " '487.txt',\n",
       " '488.txt',\n",
       " '489.txt',\n",
       " '490.txt',\n",
       " '491.txt',\n",
       " '492.txt',\n",
       " '493.txt',\n",
       " '494.txt',\n",
       " '495.txt',\n",
       " '496.txt',\n",
       " '497.txt',\n",
       " '498.txt',\n",
       " '499.txt',\n",
       " '5-60-1920x1080-1.txt',\n",
       " '5-60-1920x1080-2.txt',\n",
       " '5-60-1920x1080-3.txt',\n",
       " '5-60-1920x1080-4.txt',\n",
       " '50-30-1920x1080.txt',\n",
       " '500.txt',\n",
       " '51-30-1280x720.txt',\n",
       " '52-30-1280x720_left.txt',\n",
       " '52-30-1280x720_right.txt',\n",
       " '53-30-360x480.txt',\n",
       " '54-30-1080x1920.txt',\n",
       " '57-25-426x240.txt',\n",
       " '59-30-1280x720.txt',\n",
       " '60-30-1920x1080.txt',\n",
       " '61-24-1920x1080.txt',\n",
       " '63-30-1920x1080.txt',\n",
       " '64-24-640x360.txt',\n",
       " '65-30-400x228.txt',\n",
       " '66-25-1080x1920.txt',\n",
       " '67-24-640x360.txt',\n",
       " '68-24-1920x1080.txt',\n",
       " '69-25-854x480.txt',\n",
       " '7-60-1920x1080.txt',\n",
       " '71-30-1920x1080.txt',\n",
       " '72-30-1280x720.txt',\n",
       " '75-30-960x720.txt',\n",
       " '76-30-640x280.txt',\n",
       " '77-30-1280x720.txt',\n",
       " '78-30-960x720.txt',\n",
       " '79-30-960x720.txt',\n",
       " '8-30-1280x720.txt',\n",
       " '81-30-576x360.txt',\n",
       " '82-25-854x480.txt',\n",
       " '83-24-1920x1080.txt',\n",
       " '84-30-1920x1080.txt',\n",
       " '85-24-1280x720.txt',\n",
       " '86-24-1920x1080.txt',\n",
       " '87-25-1920x1080.txt',\n",
       " '89-30-1080x1920.txt',\n",
       " '9-15-1920x1080.txt',\n",
       " '90-30-1080x1920.txt',\n",
       " '92-24-1920x1080.txt',\n",
       " '93-24-640x360.txt',\n",
       " '94-30-1920x1080.txt',\n",
       " '95-24-1920x1080.txt',\n",
       " '96-30-1280x720.txt',\n",
       " '97-29-1920x1080.txt',\n",
       " '98-30-360x360.txt',\n",
       " '99-30-720x720.txt',\n",
       " 'video1.txt',\n",
       " 'video54.txt',\n",
       " 'video55_left.txt',\n",
       " 'video55_right.txt',\n",
       " 'video56.txt',\n",
       " 'video57.txt',\n",
       " 'video58.txt',\n",
       " 'video59.txt',\n",
       " 'video59_right.txt',\n",
       " 'video60.txt',\n",
       " 'video61.txt',\n",
       " 'video62.txt',\n",
       " 'video63.txt',\n",
       " 'video64.txt',\n",
       " 'video66.txt',\n",
       " 'video67.txt',\n",
       " 'video69.txt',\n",
       " 'video70.txt',\n",
       " 'video71.txt',\n",
       " 'video74_left.txt',\n",
       " 'video74_right.txt',\n",
       " 'video75.txt',\n",
       " 'video76.txt',\n",
       " 'video77.txt',\n",
       " 'video78.txt',\n",
       " 'video79.txt',\n",
       " 'video80.txt',\n",
       " 'video81.txt',\n",
       " 'video82.txt',\n",
       " 'video83.txt',\n",
       " 'video84.txt',\n",
       " 'video85.txt',\n",
       " 'video86_1.txt',\n",
       " 'video86_2.txt',\n",
       " 'video86_3.txt',\n",
       " 'video87.txt',\n",
       " 'video88.txt',\n",
       " 'video89.txt',\n",
       " 'video90.txt',\n",
       " 'video91.txt',\n",
       " 'video92.txt',\n",
       " 'video93.txt',\n",
       " 'video94.txt',\n",
       " 'video95.txt',\n",
       " 'video96.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to feature files\n",
    "path_train_valid_labels = '../Affwild/6thABAWAnnotations/VA_Estimation_Challenge/Train_Validation_Set'\n",
    "extension_lab = 'txt'\n",
    "\n",
    "train_valid_labels = [file for file in os.listdir(path_train_valid_labels) if file.endswith(extension_lab)]\n",
    "\n",
    "sorted_train_valid_labels = sorted(train_valid_labels)\n",
    "sorted_train_valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "454d18d7-46f7-4421-a568-6dc956948d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_train_valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feeff019-97cc-4e70-a331-1711d641d3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316, 432)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_train_valid_wavs), len(sorted_train_valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66545e80-7bec-40ac-abf3-16951e80766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/etsmtl/akoerich/DEV/WavLM\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "928f3079-ec46-46bf-8ff2-c43d76373270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-30-1280x720\n",
      "10-60-1280x720\n",
      "100-29-1080x1920\n",
      "101-30-1080x1920\n",
      "104-17-720x480\n",
      "106-30-720x1280\n",
      "107-30-640x480\n",
      "110-30-270x480\n",
      "111-25-1920x1080\n",
      "112-30-640x360\n",
      "113-60-1280x720\n",
      "114-30-1280x720\n",
      "115-30-1280x720\n",
      "116-30-1280x720\n",
      "117-25-1920x1080\n",
      "118-30-640x480\n",
      "119-30-848x480\n",
      "12-24-1920x1080\n",
      "121-24-1920x1080\n",
      "122-60-1920x1080-1\n",
      "122-60-1920x1080-2\n",
      "122-60-1920x1080-3\n",
      "122-60-1920x1080-4\n",
      "122-60-1920x1080-5\n",
      "124-30-720x1280\n",
      "125-25-1280x720\n",
      "126-30-1080x1920\n",
      "127-30-1280x720\n",
      "129-24-1280x720\n",
      "13-30-1920x1080\n",
      "131-30-1920x1080\n",
      "132-30-426x240\n",
      "136-30-1920x1080\n",
      "138-30-1280x720\n",
      "139-14-720x480\n",
      "140-30-632x360\n",
      "15-24-1920x1080\n",
      "16-30-1920x1080\n",
      "17-24-1920x1080\n",
      "18-24-1920x1080\n",
      "19-24-1920x1080\n",
      "20-24-1920x1080\n",
      "201\n",
      "202\n",
      "203\n",
      "206\n",
      "207\n",
      "208\n",
      "21-24-1920x1080\n",
      "210\n",
      "211\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "218\n",
      "22-30-1920x1080\n",
      "220\n",
      "221\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "23-24-1920x1080\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "24-30-1920x1080-1\n",
      "24-30-1920x1080-2\n",
      "240\n",
      "242\n",
      "243\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "25-25-600x480\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "26-60-1280x720\n",
      "260\n",
      "261\n",
      "262\n",
      "264\n",
      "269\n",
      "27-60-1280x720\n",
      "271\n",
      "272\n",
      "273\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "28-30-1280x720-1\n",
      "28-30-1280x720-2\n",
      "28-30-1280x720-3\n",
      "28-30-1280x720-4\n",
      "280\n",
      "282\n",
      "283\n",
      "285\n",
      "287\n",
      "288\n",
      "29-24-1280x720\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "305\n",
      "306\n",
      "308\n",
      "309\n",
      "31-30-1920x1080\n",
      "312\n",
      "314\n",
      "315\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "33-30-1920x1080\n",
      "330\n",
      "331\n",
      "332\n",
      "334\n",
      "335\n",
      "337\n",
      "339\n",
      "34-25-1920x1080\n",
      "341\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "350\n",
      "354\n",
      "355\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "368\n",
      "369\n",
      "37-30-1280x720\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "376\n",
      "377\n",
      "378\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "39-25-424x240\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "398\n",
      "399\n",
      "4-30-1920x1080\n",
      "400\n",
      "402\n",
      "403\n",
      "406\n",
      "407\n",
      "409\n",
      "41-24-1280x720\n",
      "412\n",
      "415\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "433\n",
      "435\n",
      "44-25-426x240\n",
      "440\n",
      "441\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "45-24-1280x720\n",
      "47-30-654x480\n",
      "48-30-720x1280\n",
      "5-60-1920x1080-1\n",
      "5-60-1920x1080-2\n",
      "5-60-1920x1080-3\n",
      "5-60-1920x1080-4\n",
      "50-30-1920x1080\n",
      "51-30-1280x720\n",
      "53-30-360x480\n",
      "54-30-1080x1920\n",
      "57-25-426x240\n",
      "59-30-1280x720\n",
      "60-30-1920x1080\n",
      "61-24-1920x1080\n",
      "63-30-1920x1080\n",
      "64-24-640x360\n",
      "65-30-400x228\n",
      "66-25-1080x1920\n",
      "67-24-640x360\n",
      "68-24-1920x1080\n",
      "71-30-1920x1080\n",
      "72-30-1280x720\n",
      "75-30-960x720\n",
      "76-30-640x280\n",
      "77-30-1280x720\n",
      "78-30-960x720\n",
      "79-30-960x720\n",
      "8-30-1280x720\n",
      "81-30-576x360\n",
      "82-25-854x480\n",
      "83-24-1920x1080\n",
      "84-30-1920x1080\n",
      "85-24-1280x720\n",
      "86-24-1920x1080\n",
      "87-25-1920x1080\n",
      "89-30-1080x1920\n",
      "90-30-1080x1920\n",
      "92-24-1920x1080\n",
      "93-24-640x360\n",
      "94-30-1920x1080\n",
      "95-24-1920x1080\n",
      "96-30-1280x720\n",
      "97-29-1920x1080\n",
      "98-30-360x360\n",
      "99-30-720x720\n",
      "video54\n",
      "video56\n",
      "video57\n",
      "video58\n",
      "video59\n",
      "video60\n",
      "video61\n",
      "video62\n",
      "video63\n",
      "video64\n",
      "video66\n",
      "video67\n",
      "video69\n",
      "video70\n",
      "video71\n",
      "video75\n",
      "video76\n",
      "video77\n",
      "video78\n",
      "video79\n",
      "video80\n",
      "video81\n",
      "video82\n",
      "video83\n",
      "video84\n",
      "video85\n",
      "video86_1\n",
      "video86_2\n",
      "video86_3\n",
      "video87\n",
      "video88\n",
      "video89\n",
      "video90\n",
      "video91\n",
      "video92\n",
      "video93\n",
      "video94\n",
      "video95\n",
      "video96\n",
      "316 matches\n"
     ]
    }
   ],
   "source": [
    "# Function to remove file extensions\n",
    "def remove_extension(filename):\n",
    "    return os.path.splitext(filename)[0]\n",
    "\n",
    "# Remove extensions from both lists\n",
    "cleaned_train_valid_wavs   = [remove_extension(filename) for filename in sorted_train_valid_wavs]\n",
    "cleaned_train_valid_labels = [remove_extension(filename) for filename in sorted_train_valid_labels]\n",
    "\n",
    "counter = 0\n",
    "matched_train_valid_list = []\n",
    "\n",
    "# Check and print filenames\n",
    "for filename in cleaned_train_valid_wavs:\n",
    "    if filename in cleaned_train_valid_labels:\n",
    "        print(filename)\n",
    "        matched_train_valid_list.append(filename)\n",
    "        counter = counter + 1\n",
    "    #else:\n",
    "    #    print(f\"{filename} not found in labels\")\n",
    "\n",
    "print( str(counter) + \" matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3868f663-8c31-4d1b-8043-1899ba62fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_df = pd.DataFrame(matched_train_valid_list)\n",
    "matched_df.to_csv('matched_train_valid_pooled_new.csv', index = False, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92c8c914-a496-47e3-8c05-f7d89695567d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matched_train_valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e4c14db-a614-4f45-a7fc-029c41c49966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features_path, labels_path):\n",
    "        self.features_path = features_path\n",
    "        self.labels_path = labels_path\n",
    "        #self.file_list = os.listdir(features_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        #return len(self.file_list)\n",
    "        return len(self.features_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the filenames\n",
    "        #filename = self.file_list[idx]\n",
    "        #features_file_path = os.path.join(self.features_path, filename)\n",
    "        #labels_file_path = os.path.join(self.labels_path, filename)\n",
    "        features_file_path = self.features_path\n",
    "        labels_file_path   = self.labels_path\n",
    "\n",
    "        # Load features and labels\n",
    "        features_df = pd.read_csv(features_file_path)\n",
    "        labels_df   = pd.read_csv(labels_file_path)\n",
    "\n",
    "        # Ensure the same length for features and labels\n",
    "        min_length = min(len(features_df), len(labels_df))\n",
    "        features_df = features_df.head(min_length)\n",
    "        labels_df = labels_df.head(min_length)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        features_tensor = torch.tensor(features_df.values, dtype=torch.float32)\n",
    "        labels_tensor = torch.tensor(labels_df.values, dtype=torch.float32)\n",
    "\n",
    "        return features_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4c1f8-2995-4c08-b7c5-4371ddbf6f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4079f49f-f929-4f14-9874-9dab0b61d918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c23410c-8e8a-49f2-b796-c60eda971f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 1-30-1280x720\n",
      "Processing: 10-60-1280x720\n",
      "Processing: 100-29-1080x1920\n",
      "Processing: 101-30-1080x1920\n",
      "Processing: 104-17-720x480\n",
      "Processing: 106-30-720x1280\n",
      "Processing: 107-30-640x480\n",
      "Processing: 110-30-270x480\n",
      "Processing: 111-25-1920x1080\n",
      "Processing: 112-30-640x360\n",
      "Processing: 113-60-1280x720\n",
      "Processing: 114-30-1280x720\n",
      "Processing: 115-30-1280x720\n",
      "Processing: 116-30-1280x720\n",
      "Processing: 117-25-1920x1080\n",
      "Processing: 118-30-640x480\n",
      "Processing: 119-30-848x480\n",
      "Processing: 12-24-1920x1080\n",
      "Processing: 121-24-1920x1080\n",
      "Processing: 122-60-1920x1080-1\n",
      "Processing: 122-60-1920x1080-2\n",
      "Processing: 122-60-1920x1080-3\n",
      "Processing: 122-60-1920x1080-4\n",
      "Processing: 122-60-1920x1080-5\n",
      "Processing: 124-30-720x1280\n",
      "Processing: 125-25-1280x720\n",
      "Processing: 126-30-1080x1920\n",
      "Processing: 127-30-1280x720\n",
      "Processing: 129-24-1280x720\n",
      "Processing: 13-30-1920x1080\n",
      "Processing: 131-30-1920x1080\n",
      "Processing: 132-30-426x240\n",
      "Processing: 136-30-1920x1080\n",
      "Processing: 138-30-1280x720\n",
      "Processing: 139-14-720x480\n",
      "Processing: 140-30-632x360\n",
      "Processing: 15-24-1920x1080\n",
      "Processing: 16-30-1920x1080\n",
      "Processing: 17-24-1920x1080\n",
      "Processing: 18-24-1920x1080\n",
      "Processing: 19-24-1920x1080\n",
      "Processing: 20-24-1920x1080\n",
      "Processing: 201\n",
      "Processing: 202\n",
      "Processing: 203\n",
      "Processing: 206\n",
      "Processing: 207\n",
      "Processing: 208\n",
      "Processing: 21-24-1920x1080\n",
      "Processing: 210\n",
      "Processing: 211\n",
      "Processing: 213\n",
      "Processing: 214\n",
      "Processing: 215\n",
      "Processing: 216\n",
      "Processing: 218\n",
      "Processing: 22-30-1920x1080\n",
      "Processing: 220\n",
      "Processing: 221\n",
      "Processing: 223\n",
      "Processing: 224\n",
      "Processing: 225\n",
      "Processing: 226\n",
      "Processing: 227\n",
      "Processing: 228\n",
      "Processing: 229\n",
      "Processing: 23-24-1920x1080\n",
      "Processing: 230\n",
      "Processing: 231\n",
      "Processing: 232\n",
      "Processing: 233\n",
      "Processing: 235\n",
      "Processing: 236\n",
      "Processing: 237\n",
      "Processing: 238\n",
      "Processing: 24-30-1920x1080-1\n",
      "Processing: 24-30-1920x1080-2\n",
      "Processing: 240\n",
      "Processing: 242\n",
      "Processing: 243\n",
      "Processing: 245\n",
      "Processing: 246\n",
      "Processing: 247\n",
      "Processing: 248\n",
      "Processing: 249\n",
      "Processing: 25-25-600x480\n",
      "Processing: 250\n",
      "Processing: 251\n",
      "Processing: 252\n",
      "Processing: 253\n",
      "Processing: 254\n",
      "Processing: 255\n",
      "Processing: 256\n",
      "Processing: 257\n",
      "Processing: 258\n",
      "Processing: 259\n",
      "Processing: 26-60-1280x720\n",
      "Processing: 260\n",
      "Processing: 261\n",
      "Processing: 262\n",
      "Processing: 264\n",
      "Processing: 269\n",
      "Processing: 27-60-1280x720\n",
      "Processing: 271\n",
      "Processing: 272\n",
      "Processing: 273\n",
      "Processing: 275\n",
      "Processing: 276\n",
      "Processing: 277\n",
      "Processing: 278\n",
      "Processing: 279\n",
      "Processing: 28-30-1280x720-1\n",
      "Processing: 28-30-1280x720-2\n",
      "Processing: 28-30-1280x720-3\n",
      "Processing: 28-30-1280x720-4\n",
      "Processing: 280\n",
      "Processing: 282\n",
      "Processing: 283\n",
      "Processing: 285\n",
      "Processing: 287\n",
      "Processing: 288\n",
      "Processing: 29-24-1280x720\n",
      "Processing: 290\n",
      "Processing: 291\n",
      "Processing: 292\n",
      "Processing: 293\n",
      "Processing: 294\n",
      "Processing: 295\n",
      "Processing: 296\n",
      "Processing: 297\n",
      "Processing: 298\n",
      "Processing: 299\n",
      "Processing: 305\n",
      "Processing: 306\n",
      "Processing: 308\n",
      "Processing: 309\n",
      "Processing: 31-30-1920x1080\n",
      "Processing: 312\n",
      "Processing: 314\n",
      "Processing: 315\n",
      "Processing: 317\n",
      "Processing: 318\n",
      "Processing: 319\n",
      "Processing: 320\n",
      "Processing: 321\n",
      "Processing: 323\n",
      "Processing: 324\n",
      "Processing: 325\n",
      "Processing: 326\n",
      "Processing: 327\n",
      "Processing: 328\n",
      "Processing: 329\n",
      "Processing: 33-30-1920x1080\n",
      "Processing: 330\n",
      "Processing: 331\n",
      "Processing: 332\n",
      "Processing: 334\n",
      "Processing: 335\n",
      "Processing: 337\n",
      "Processing: 339\n",
      "Processing: 34-25-1920x1080\n",
      "Processing: 341\n",
      "Processing: 345\n",
      "Processing: 346\n",
      "Processing: 347\n",
      "Processing: 348\n",
      "Processing: 350\n",
      "Processing: 354\n",
      "Processing: 355\n",
      "Processing: 359\n",
      "Processing: 360\n",
      "Processing: 361\n",
      "Processing: 362\n",
      "Processing: 363\n",
      "Processing: 364\n",
      "Processing: 368\n",
      "Processing: 369\n",
      "Processing: 37-30-1280x720\n",
      "Processing: 371\n",
      "Processing: 372\n",
      "Processing: 373\n",
      "Processing: 374\n",
      "Processing: 376\n",
      "Processing: 377\n",
      "Processing: 378\n",
      "Processing: 383\n",
      "Processing: 384\n",
      "Processing: 385\n",
      "Processing: 386\n",
      "Processing: 387\n",
      "Processing: 388\n",
      "Processing: 389\n",
      "Processing: 39-25-424x240\n",
      "Processing: 391\n",
      "Processing: 392\n",
      "Processing: 393\n",
      "Processing: 394\n",
      "Processing: 395\n",
      "Processing: 398\n",
      "Processing: 399\n",
      "Processing: 4-30-1920x1080\n",
      "Processing: 400\n",
      "Processing: 402\n",
      "Processing: 403\n",
      "Processing: 406\n",
      "Processing: 407\n",
      "Processing: 409\n",
      "Processing: 41-24-1280x720\n",
      "Processing: 412\n",
      "Processing: 415\n",
      "Processing: 418\n",
      "Processing: 419\n",
      "Processing: 420\n",
      "Processing: 421\n",
      "Processing: 423\n",
      "Processing: 424\n",
      "Processing: 425\n",
      "Processing: 426\n",
      "Processing: 427\n",
      "Processing: 428\n",
      "Processing: 429\n",
      "Processing: 430\n",
      "Processing: 433\n",
      "Processing: 435\n",
      "Processing: 44-25-426x240\n",
      "Processing: 440\n",
      "Processing: 441\n",
      "Processing: 446\n",
      "Processing: 447\n",
      "Processing: 448\n",
      "Processing: 449\n",
      "Processing: 45-24-1280x720\n",
      "Processing: 47-30-654x480\n",
      "Processing: 48-30-720x1280\n",
      "Processing: 5-60-1920x1080-1\n",
      "Processing: 5-60-1920x1080-2\n",
      "Processing: 5-60-1920x1080-3\n",
      "Processing: 5-60-1920x1080-4\n",
      "Processing: 50-30-1920x1080\n",
      "Processing: 51-30-1280x720\n",
      "Processing: 53-30-360x480\n",
      "Processing: 54-30-1080x1920\n",
      "Processing: 57-25-426x240\n",
      "Processing: 59-30-1280x720\n",
      "Processing: 60-30-1920x1080\n",
      "Processing: 61-24-1920x1080\n",
      "Processing: 63-30-1920x1080\n",
      "Processing: 64-24-640x360\n",
      "Processing: 65-30-400x228\n",
      "Processing: 66-25-1080x1920\n",
      "Processing: 67-24-640x360\n",
      "Processing: 68-24-1920x1080\n",
      "Processing: 71-30-1920x1080\n",
      "Processing: 72-30-1280x720\n",
      "Processing: 75-30-960x720\n",
      "Processing: 76-30-640x280\n",
      "Processing: 77-30-1280x720\n",
      "Processing: 78-30-960x720\n",
      "Processing: 79-30-960x720\n",
      "Processing: 8-30-1280x720\n",
      "Processing: 81-30-576x360\n",
      "Processing: 82-25-854x480\n",
      "Processing: 83-24-1920x1080\n",
      "Processing: 84-30-1920x1080\n",
      "Processing: 85-24-1280x720\n",
      "Processing: 86-24-1920x1080\n",
      "Processing: 87-25-1920x1080\n",
      "Processing: 89-30-1080x1920\n",
      "Processing: 90-30-1080x1920\n",
      "Processing: 92-24-1920x1080\n",
      "Processing: 93-24-640x360\n",
      "Processing: 94-30-1920x1080\n",
      "Processing: 95-24-1920x1080\n",
      "Processing: 96-30-1280x720\n",
      "Processing: 97-29-1920x1080\n",
      "Processing: 98-30-360x360\n",
      "Processing: 99-30-720x720\n",
      "Processing: video54\n",
      "Processing: video56\n",
      "Processing: video57\n",
      "Processing: video58\n",
      "Processing: video59\n",
      "Processing: video60\n",
      "Processing: video61\n",
      "Processing: video62\n",
      "Processing: video63\n",
      "Processing: video64\n",
      "Processing: video66\n",
      "Processing: video67\n",
      "Processing: video69\n",
      "Processing: video70\n",
      "Processing: video71\n",
      "Processing: video75\n",
      "Processing: video76\n",
      "Processing: video77\n",
      "Processing: video78\n",
      "Processing: video79\n",
      "Processing: video80\n",
      "Processing: video81\n",
      "Processing: video82\n",
      "Processing: video83\n",
      "Processing: video84\n",
      "Processing: video85\n",
      "Processing: video86_1\n",
      "Processing: video86_2\n",
      "Processing: video86_3\n",
      "Processing: video87\n",
      "Processing: video88\n",
      "Processing: video89\n",
      "Processing: video90\n",
      "Processing: video91\n",
      "Processing: video92\n",
      "Processing: video93\n",
      "Processing: video94\n",
      "Processing: video95\n",
      "Processing: video96\n",
      "\n",
      "\n",
      "Processed: 316\n"
     ]
    }
   ],
   "source": [
    "counter = 0 \n",
    "\n",
    "for wavs in matched_train_valid_list:\n",
    "\n",
    "    counter = counter + 1\n",
    "    \n",
    "    print(\"Processing: \" + str(wavs))\n",
    "    #if os.path.exists(cleaned_train_valid_wavs) & os.path.exists(path_train_valid_labels+cleaned_train_valid_labels):\n",
    "\n",
    "        #print(\"Processing: \" + cleaned_train_valid_wavs)\n",
    "\n",
    "        # Process audio signal\n",
    "        #signal, sr = sf.read(wavs) \n",
    "        #signal2    = signal.reshape(-1, 1).astype(np.float32)\n",
    "        #signal2    = min_max_scaler.fit_transform(signal2).reshape(-1)\n",
    "        #df         = pd.DataFrame(interface.process_signal(signal2, sampling_rate))\n",
    "             \n",
    "        \n",
    "        #df_arousal = df['arousal'].to_numpy()\n",
    "        #df_valence = df['valence'].to_numpy()\n",
    "        \n",
    "        #df_arousal = df[['arousal']].to_numpy()\n",
    "        #df_valence = df[['valence']].to_numpy()\n",
    "        #df_arousal = min_max_scaler.fit_transform(df_arousal).reshape(-1)\n",
    "        #df_valence = min_max_scaler.fit_transform(df_valence).reshape(-1)\n",
    "        \n",
    "        # Process ground-truth\n",
    "        #df_gt         = pd.read_csv(labs).astype(np.float32)\n",
    "        #df_arousal_gt = df_gt['arousal'].to_numpy()\n",
    "        #df_valence_gt = df_gt['valence'].to_numpy()\n",
    "\n",
    "        #if len(df_arousal_gt) > len(df_arousal):\n",
    "        #    df_arousal = np.pad(df_arousal, (0,len(df_arousal_gt)-len(df_arousal)), 'constant', constant_values=(0,1)) \n",
    "        #    df_valence = np.pad(df_valence, (0,len(df_valence_gt)-len(df_valence)), 'constant', constant_values=(0,1)) \n",
    "            \n",
    "        #if len(df_arousal_gt) < len(df_arousal):\n",
    "        #    df_arousal_gt = np.pad(df_arousal_gt, (0,len(df_arousal)-len(df_arousal_gt)), 'constant', constant_values=(0,1))   \n",
    "        #    df_valence_gt = np.pad(df_valence_gt, (0,len(df_valence)-len(df_valence_gt)), 'constant', constant_values=(0,1))   \n",
    "\n",
    "     #   input(\"Press Enter to continue...\")\n",
    "\n",
    "        \n",
    "     #   if len(df_arousal_gt) != len(df_arousal):\n",
    "     #       print(\"Different lengths: \" + wavs + \": \" + str(len(df_arousal_gt)) + \" and \" + str(len(df_arousal)) )\n",
    "    #    else:\n",
    "    #        print( audmetric.concordance_cc(df_arousal_gt, df_arousal), audmetric.concordance_cc(df_valence_gt, df_valence) )\n",
    "\n",
    "   #     i += 1\n",
    "    #    if (i%1 == 0.000):\n",
    "   #         print(\"Files read: \"+str(i) )\n",
    "\n",
    "   # else: \n",
    "  #      if os.path.exists(wavs) == False: \n",
    "   #         print(\"Does not exist: \" + wavs)\n",
    "\n",
    "   #     if os.path.exists(labs) == False:\n",
    "   #         print(\"Does not exist: \" + labs )\n",
    "   #     else:\n",
    "   #         print(\"No idea\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nProcessed: \" + str(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d09d4c-8145-491e-a6fb-8426df3e230b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97abd2cf-fdf3-4bdf-8b4d-4df9f9449f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to feature files\n",
    "path_train = 'features/valid'\n",
    "extension = 'wavlm'\n",
    "\n",
    "train_files = [file for file in os.listdir(path_train) if file.endswith(extension)]\n",
    "\n",
    "sorted_train_files = sorted(train_files)\n",
    "sorted_train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f349d9a-eb3d-4069-995f-db92de8ee13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to feature files\n",
    "path_devel = '../Affwild/6thABAWAnnotations/VA_Estimation_Challenge/Validation_Set'\n",
    "extension = 'txt'\n",
    "\n",
    "devel_files = [file for file in os.listdir(path_devel) if file.endswith(extension)]\n",
    "\n",
    "sorted_devel_files = sorted(devel_files)\n",
    "sorted_devel_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22481e4e-bad7-40e0-b0ed-87ae08153d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78443d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to feature files\n",
    "path_devel = 'Recola2018_16k/features/devel'\n",
    "extension = 'wavlmbasefeatpoolloso'\n",
    "\n",
    "devel_files = [file for file in os.listdir(path_devel) if file.endswith(extension)]\n",
    "\n",
    "sorted_devel_files = sorted(devel_files)\n",
    "sorted_devel_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55350230-a9a7-4200-bcc9-548f35b8dfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce8bdec-3aa7-4805-ad99-9d892e27f38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5599ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file in sorted_train_files:\n",
    "    df = pd.read_csv(os.path.join(path_train, file))\n",
    "    \n",
    "    # pad to make single feature files compatible with labels (2 lines for Recola)\n",
    "    # pad the dataframe with a copy of the first row \n",
    "    new_row = pd.DataFrame(df.loc[0])\n",
    "\n",
    "    # simply concatenate both dataframes\n",
    "    df = pd.concat([new_row.T, df]).reset_index(drop = True)\n",
    "    \n",
    "    # pad the dataframe with a copy of the last row \n",
    "    new_row2 = pd.DataFrame(df.loc[len(df)-1])\n",
    "    \n",
    "    # simply concatenate both dataframes\n",
    "    df = pd.concat([df,new_row2.T]).reset_index(drop = True)\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "df_train_feat = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Drop first index column (unamed 0)\n",
    "df_train_feat.drop(df_train_feat.columns[[0]], axis=1, inplace=True)\n",
    "\n",
    "df_train_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_devel = []\n",
    "for file in sorted_devel_files:\n",
    "    df_devel = pd.read_csv(os.path.join(path_devel, file))\n",
    "    \n",
    "    # pad to make single feature files compatible with labels (2 lines for Recola)\n",
    "    # pad the dataframe with a copy of the first row \n",
    "    new_row = pd.DataFrame(df_devel.loc[0])\n",
    "\n",
    "    # simply concatenate both dataframes\n",
    "    df_devel = pd.concat([new_row.T, df_devel]).reset_index(drop = True)\n",
    "    \n",
    "    # pad the dataframe with a copy of the last row \n",
    "    new_row2 = pd.DataFrame(df_devel.loc[len(df_devel)-1])\n",
    "    \n",
    "    # simply concatenate both dataframes\n",
    "    df_devel = pd.concat([df_devel,new_row2.T]).reset_index(drop = True)\n",
    "    \n",
    "    dfs_devel.append(df_devel)\n",
    "    \n",
    "df_devel_feat = pd.concat(dfs_devel, ignore_index=True)\n",
    "\n",
    "# Drop first index column (unamed 0)\n",
    "df_devel_feat.drop(df_devel_feat.columns[[0]], axis=1, inplace=True)\n",
    "\n",
    "df_devel_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a43bb",
   "metadata": {},
   "source": [
    "### Process label files - Train and Devel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dec438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to label files\n",
    "path_train_arousal_labels = 'Recola2018_16k/labels/arousal/Train/'\n",
    "extension = 'arff'\n",
    "\n",
    "train_files_arousal_labels = [file for file in os.listdir(path_train_arousal_labels) if file.endswith(extension)]\n",
    "\n",
    "sorted_train_arousal_labels = sorted(train_files_arousal_labels)\n",
    "sorted_train_arousal_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac9076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to label files\n",
    "path_train_valence_labels = 'Recola2018_16k/labels/valence/Train/'\n",
    "extension = 'arff'\n",
    "\n",
    "train_files_valence_labels = [file for file in os.listdir(path_train_valence_labels) if file.endswith(extension)]\n",
    "\n",
    "sorted_train_valence_labels = sorted(train_files_valence_labels)\n",
    "sorted_train_valence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b775647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to label files\n",
    "path_devel_arousal_labels = 'Recola2018_16k/labels/arousal/Devel/'\n",
    "extension = 'arff'\n",
    "\n",
    "devel_files_arousal_labels = [file for file in os.listdir(path_devel_arousal_labels) if file.endswith(extension)]\n",
    "\n",
    "sorted_devel_arousal_labels = sorted(devel_files_arousal_labels)\n",
    "sorted_devel_arousal_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c74198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to label files\n",
    "path_devel_valence_labels = 'Recola2018_16k/labels/valence/Devel/'\n",
    "extension = 'arff'\n",
    "\n",
    "devel_files_valence_labels = [file for file in os.listdir(path_devel_valence_labels) if file.endswith(extension)]\n",
    "\n",
    "sorted_devel_valence_labels = sorted(devel_files_valence_labels)\n",
    "sorted_devel_valence_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f848d6a",
   "metadata": {},
   "source": [
    "### Create dataframes for label files - Train and Devel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4396d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = []\n",
    "for file in sorted_train_arousal_labels:\n",
    "    df2 = pd.read_csv(os.path.join(path_train_arousal_labels, file), sep=\",\", header=None)\n",
    "    df2.drop(df2.columns[[0,1]], axis=1, inplace=True)\n",
    "\n",
    "    # Add subject column\n",
    "    subject = file.split('.')[0]\n",
    "    df2.insert(0, \"Subject\", subject, True)\n",
    "    \n",
    "    dfl.append(df2)\n",
    "\n",
    "df_train_arousal_lab = pd.concat(dfl, ignore_index=True)\n",
    "df_train_arousal_lab = df_train_arousal_lab.rename(columns={2:\"arousal\"})\n",
    "df_train_arousal_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d695381",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = []\n",
    "for file in sorted_train_valence_labels:\n",
    "    df2 = pd.read_csv(os.path.join(path_train_valence_labels, file), sep=\",\", header=None)\n",
    "    df2.drop(df2.columns[[0,1]], axis=1, inplace=True)\n",
    "    \n",
    "    # Add subject column\n",
    "    subject = file.split('.')[0]\n",
    "    df2.insert(0, \"Subject\", subject, True)\n",
    "\n",
    "    dfl.append(df2)\n",
    "    \n",
    "df_train_valence_lab = pd.concat(dfl, ignore_index=True)\n",
    "df_train_valence_lab = df_train_valence_lab.rename(columns={2:\"valence\"})\n",
    "df_train_valence_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38456f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = []\n",
    "for file in sorted_devel_arousal_labels:\n",
    "    df2 = pd.read_csv(os.path.join(path_devel_arousal_labels, file), sep=\",\", header=None)\n",
    "    df2.drop(df2.columns[[0,1]], axis=1, inplace=True)\n",
    "\n",
    "    # Add subject column\n",
    "    subject = file.split('.')[0]\n",
    "    df2.insert(0, \"Subject\", subject, True)\n",
    "\n",
    "    dfl.append(df2)\n",
    "    \n",
    "df_devel_arousal_lab = pd.concat(dfl, ignore_index=True)\n",
    "df_devel_arousal_lab = df_devel_arousal_lab.rename(columns={2:\"arousal\"})\n",
    "df_devel_arousal_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c8bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = []\n",
    "for file in sorted_devel_valence_labels:\n",
    "    df2 = pd.read_csv(os.path.join(path_devel_valence_labels, file), sep=\",\", header=None)\n",
    "    df2.drop(df2.columns[[0,1]], axis=1, inplace=True)\n",
    "    \n",
    "    # Add subject column\n",
    "    subject = file.split('.')[0]\n",
    "    df2.insert(0, \"Subject\", subject, True)\n",
    "\n",
    "    dfl.append(df2)\n",
    "    \n",
    "df_devel_valence_lab = pd.concat(dfl, ignore_index=True)\n",
    "df_devel_valence_lab = df_devel_valence_lab.rename(columns={2:\"valence\"})\n",
    "df_devel_valence_lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e572759e",
   "metadata": {},
   "source": [
    "## Train a GRU regression model for arousal / valence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73becbce",
   "metadata": {},
   "source": [
    "### Initialize PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a681dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a081309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30c879",
   "metadata": {},
   "source": [
    "### Define CCC loss function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199927ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CCCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CCCLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        mean_pred = torch.mean(pred)\n",
    "        mean_target = torch.mean(target)\n",
    "\n",
    "        covar = torch.mean((pred - mean_pred) * (target - mean_target))\n",
    "        var_pred = torch.var(pred)\n",
    "        var_target = torch.var(target)\n",
    "\n",
    "        ccc = 2 * covar / (var_pred + var_target + (mean_pred - mean_target)**2)\n",
    "        return (1 - ccc)  # Minimize 1 - CCC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed9e59",
   "metadata": {},
   "source": [
    "### Define a PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f58e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1be5d6",
   "metadata": {},
   "source": [
    "#### Must choose arousal or valence here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2814445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_train_feat.iloc[:, 1:770].values.astype(np.float32)\n",
    "\n",
    "# Arousal\n",
    "# labels   = df_train_arousal_lab['arousal'].values.astype(np.float32)\n",
    "# subjects = df_train_arousal_lab['Subject'].values\n",
    "\n",
    "# Valence\n",
    "labels   = df_train_valence_lab['valence'].values.astype(np.float32)\n",
    "subjects = df_train_valence_lab['Subject'].values\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "features_tensor = torch.from_numpy(features)\n",
    "labels_tensor   = torch.from_numpy(labels)\n",
    "\n",
    "# Assuming you want a sequence length of 1\n",
    "# features_tensor = features_tensor.unsqueeze(1)\n",
    "\n",
    "######\n",
    "# Reshape features tensor with sequence length of 50\n",
    "sequence_length = 1\n",
    "num_features    = features.shape[1]\n",
    "num_samples     = features.shape[0]\n",
    "\n",
    "# Calculate the number of sequences that can be formed\n",
    "num_sequences = num_samples // sequence_length\n",
    "\n",
    "# Truncate the tensor to fit the full sequences\n",
    "features_tensor = features_tensor[:num_sequences * sequence_length, :]\n",
    "labels_tensor = labels_tensor[:num_sequences * sequence_length]\n",
    "\n",
    "# Reshape the tensor\n",
    "features_tensor = features_tensor.view(num_sequences, sequence_length, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573014d",
   "metadata": {},
   "source": [
    "### Model parameters and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80750ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_size   = num_features\n",
    "hidden_size  = 32 #128, 64, 32, 16\n",
    "num_layers   = 5\n",
    "output_size  = 1  # Single output for regression between -1 and +1\n",
    "dropout_prob = 0.25 \n",
    "\n",
    "# Train the model\n",
    "num_epochs     = 1000\n",
    "batch_size     = 7501\n",
    "validate_every = 1  # Validate every 2 epochs\n",
    "patience       = 15  # Stop training if validation loss doesn't improve for 5 consecutive validations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f8871",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8260475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Define the GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, dropout_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        output = self.fc(gru_out[:, -1, :])  # Take the output from the last time step\n",
    "        return output\n",
    "\n",
    "# model = GRUModel(input_size, hidden_size, output_size, dropout_prob)\n",
    "\n",
    "# =======================\n",
    "# Define the Convolutional GRU model\n",
    "class ConvGRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
    "        super(ConvGRUModel, self).__init__()\n",
    "        self.convgru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.convgru(x)\n",
    "        output = self.fc(gru_out[:, -1, :])  # Take the output from the last time step\n",
    "        return output\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        # Reset parameters of the GRU layer\n",
    "        for name, param in self.convgru.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "        # Reset parameters of the fully connected layer\n",
    "        self.fc.reset_parameters()\n",
    "\n",
    "model = ConvGRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "#=============================\n",
    "# Define the Convolutional GRU model with Tanh activation at the output\n",
    "class ConvGRUModelTanh(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
    "        super(ConvGRUModelTanh, self).__init__()\n",
    "        self.convgru = nn.GRU(input_size=input_size, hidden_size=hidden_size, dropout=dropout_prob, num_layers=num_layers, batch_first=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.convgru(x)\n",
    "        output = self.fc(gru_out[:, -1, :])  # Take the output from the last time step\n",
    "        output = self.tanh(output)  # Apply Tanh activation\n",
    "        return output\n",
    "\n",
    "#model = ConvGRUModelTanh(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "#=============================\n",
    "# Define the Convolutional BLSTM model with dropout\n",
    "class ConvBLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob=0.2):\n",
    "        super(ConvBLSTMModel, self).__init__()\n",
    "        self.convblstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                                 batch_first=True, bidirectional=True, dropout=dropout_prob)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # Multiply by 2 for bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        blstm_out, _ = self.convblstm(x)\n",
    "        output = self.dropout(blstm_out[:, -1, :])  # Apply dropout before the fully connected layer\n",
    "        output = self.fc(output)  # Take the output from the last time step\n",
    "        return output\n",
    "\n",
    "# model = ConvBLSTMModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "#=============================\n",
    "\n",
    "# Move the selected model to the GPU\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911b020",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "#### Must choose arousal or valence here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0583a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = TensorDataset(X_train, y_train.unsqueeze(1))\n",
    "#train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "import csv\n",
    "\n",
    "def Model_Train(X_train, X_test, y_train, y_test, y_subject, batch_size, num_epochs, model): \n",
    "        \n",
    "    model.reset_parameters()\n",
    "    \n",
    "    # Move the selected model to the GPU\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Initialize a list to store the training loss values\n",
    "    train_loss_values      = []\n",
    "    validation_loss_values = []\n",
    "    \n",
    "    # Initialize a list to store the best training epoch and the best validation loss\n",
    "    best_validation_loss     = float('inf')\n",
    "    early_stop_counter       = 0\n",
    "    \n",
    "    # Aroural\n",
    "    # best_model_path          = 'best_model_recola_arousal_loso.pth'  # Define the path to save the best model\n",
    "    # best_model_epochs        = 'best_model_recola_arousal_loso_epochs.csv'  # Define the path to save the best model\n",
    "    \n",
    "    # Valence\n",
    "    best_model_path          = 'best_model_recola_valence_loso.pth'  # Define the path to save the best model\n",
    "    best_model_epochs        = 'best_model_recola_valence_loso_epochs.csv'  # Define the path to save the best model\n",
    "\n",
    "    #criterion = nn.MSELoss()\n",
    "    criterion = CCCLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    #train_dataset = TensorDataset(X_train, y_train.unsqueeze(1))\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "    train_loader  = DataLoader(train_dataset, batch_size = batch_size, shuffle=False)\n",
    "    \n",
    "    epoch_best = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "    \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X.to(device))\n",
    "            #loss    = criterion(outputs, batch_y.to(device))\n",
    "            loss    = criterion(outputs, batch_y.unsqueeze(1).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # print every 10 epochs only\n",
    "        #if epoch % 10 == 0:\n",
    "        \n",
    "        print(f'Training epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "        average_epoch_loss = epoch_loss / len(train_loader)\n",
    "        train_loss_values.append(average_epoch_loss)\n",
    "    \n",
    "        # Validate the model every validate_every epochs using the test partition\n",
    "        if epoch % validate_every == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_outputs = model(X_test.to(device))\n",
    "                validation_loss = criterion(test_outputs, y_test.unsqueeze(1).to(device))  # Adjust target size\n",
    "\n",
    "            validation_loss_values.append(validation_loss.item())\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {validation_loss.item():.4f}')\n",
    "        \n",
    "            if validation_loss < best_validation_loss:\n",
    "                best_validation_loss = validation_loss\n",
    "                early_stop_counter = 0\n",
    "            \n",
    "                # Save the model with the best validation loss\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f'Saved model with best validation loss to {best_model_path}')\n",
    "                epoch_best = epoch       \n",
    "                \n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1} as validation loss has not improved for {patience} consecutive validations.')\n",
    "                print(f'Best training epoch {epoch_best} at validation.')\n",
    "                break\n",
    "            \n",
    "            model.train()  # Set the model back to training mode\n",
    "            \n",
    "    # write to file subject / best epoch train        \n",
    "    df_temp = pd.DataFrame( [ [ y_subject, epoch_best, best_validation_loss.cpu() ] ] )\n",
    "    df_temp.to_csv(best_model_epochs, mode='a', header=False )        \n",
    "            \n",
    "    Plot_Train_Val_Curvs(train_loss_values, validation_loss_values, validate_every)         \n",
    "            \n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    model.to('cpu')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92af1d2",
   "metadata": {},
   "source": [
    "### Evaluate the trained model, selecting the best validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# Load the best model for testing\n",
    "#best_model = ConvGRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "def Model_Test_Best(X_test, y_test, best_model): \n",
    "    \n",
    "    criterion = CCCLoss()\n",
    "    \n",
    "    #best_model = ConvBLSTMModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "    #best_model.load_state_dict(torch.load(best_model_path))\n",
    "    best_model = best_model.to(device)\n",
    "\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = best_model(X_test.to(device))\n",
    "        test_loss    = criterion(test_outputs, y_test.unsqueeze(1).to(device))\n",
    "\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "    \n",
    "    return test_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763658a0",
   "metadata": {},
   "source": [
    "### Plot training and validation CCC loss X epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab1b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Plot_Train_Val_Curvs(train_loss_values, validation_loss_values, validate_every):\n",
    "    # Plot the training and validation loss values\n",
    "    epochs = range(1, len(train_loss_values) + 1)\n",
    "    plt.plot(epochs, train_loss_values, label='Training Loss')\n",
    "    plt.plot(range(0, len(validation_loss_values) * validate_every, validate_every), validation_loss_values, label='Validation Loss', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b9d74",
   "metadata": {},
   "source": [
    "### Define LOSO Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8428b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Create model and grouping object\n",
    "best_model = model \n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "def experiment(\n",
    "    feat,\n",
    "    targ,\n",
    "    groups,\n",
    "):        \n",
    "    truths = []\n",
    "    preds  = []\n",
    "\n",
    "    for train_index, test_index in logo.split(\n",
    "        feat, \n",
    "        targ, \n",
    "        groups=groups,\n",
    "    ):\n",
    "        train_x = feat.iloc[train_index]\n",
    "        train_y = targ[train_index]\n",
    "\n",
    "        test_x = feat.iloc[test_index]\n",
    "        test_y = targ[test_index]\n",
    "        \n",
    "        print(str(train_index) + \" \" + str(test_index)) \n",
    "        print(\"Validation subject: \" + str(groups[test_index[0]])) \n",
    "        \n",
    "        # Convert to pytorch\n",
    "        \n",
    "        ### TRAIN\n",
    "        \n",
    "        features = train_x.values.astype(np.float32)\n",
    "        labels   = train_y.values.astype(np.float32)\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        features_tensor = torch.from_numpy(features)\n",
    "        labels_tensor   = torch.from_numpy(labels)\n",
    "\n",
    "        # Assuming you want a sequence length of 1\n",
    "        # features_tensor = features_tensor.unsqueeze(1)\n",
    "\n",
    "        ######\n",
    "        # Reshape features tensor with sequence length of 50\n",
    "        sequence_length = 1\n",
    "        num_features    = features.shape[1]\n",
    "        num_samples     = features.shape[0]\n",
    "\n",
    "        # Calculate the number of sequences that can be formed\n",
    "        num_sequences = num_samples // sequence_length\n",
    "\n",
    "        # Truncate the tensor to fit the full sequences\n",
    "        features_tensor = features_tensor[:num_sequences * sequence_length, :]\n",
    "        labels_tensor = labels_tensor[:num_sequences * sequence_length]\n",
    "\n",
    "        # Reshape the tensor\n",
    "        features_tensor = features_tensor.view(num_sequences, sequence_length, num_features)\n",
    "\n",
    "        ### DEVEL\n",
    "\n",
    "        features2 = test_x.values.astype(np.float32)\n",
    "        labels2   = test_y.values.astype(np.float32)\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        features_tensor2 = torch.from_numpy(features2)\n",
    "        labels_tensor2   = torch.from_numpy(labels2)\n",
    "\n",
    "        ######\n",
    "        # Reshape features tensor with sequence length of 50\n",
    "        sequence_length = 1\n",
    "        num_features = features2.shape[1]\n",
    "        num_samples  = features2.shape[0]\n",
    "\n",
    "        # Calculate the number of sequences that can be formed\n",
    "        num_sequences = num_samples // sequence_length\n",
    "\n",
    "        # Truncate the tensor to fit the full sequences\n",
    "        features_tensor2 = features_tensor2[:num_sequences * sequence_length, :]\n",
    "        labels_tensor2   = labels_tensor2[:num_sequences * sequence_length]\n",
    "\n",
    "        # Reshape the tensor\n",
    "        features_tensor2 = features_tensor2.view(num_sequences, sequence_length, num_features)\n",
    "        ######\n",
    "        \n",
    "        # Train model\n",
    "        \n",
    "        subject = str(groups[test_index[0]])\n",
    "        \n",
    "        best_model = Model_Train(features_tensor, features_tensor2, labels_tensor, labels_tensor2, subject, batch_size, num_epochs, model)\n",
    "        \n",
    "        print(\"This is the best model\" + str(best_model))\n",
    "        \n",
    "        # Test model\n",
    "        predict_y =  Model_Test_Best(features_tensor2, labels_tensor2, best_model)\n",
    "        \n",
    "        truths.append(test_y)\n",
    "        preds.append(predict_y.cpu().squeeze(1))\n",
    "        \n",
    "    # combine subject folds\n",
    "    truth = pd.concat(truths)\n",
    "    truth.name = 'truth'\n",
    "    \n",
    "    \n",
    "    \n",
    "    pred = pd.Series(\n",
    "        np.concatenate(preds),\n",
    "        index=truth.index,\n",
    "        name='prediction',\n",
    "    )\n",
    "    \n",
    "    return truth, pred"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c78150c8",
   "metadata": {},
   "source": [
    "feat = df_train_feat.iloc[:, 1:770]\n",
    "targ = df_train_valence_lab['valence']\n",
    "groups = df_train_valence_lab['Subject']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2bf68870",
   "metadata": {},
   "source": [
    "    truths = []\n",
    "    preds  = []\n",
    "\n",
    "    for train_index, test_index in logo.split(\n",
    "        feat, \n",
    "        targ, \n",
    "        groups=groups,\n",
    "    ):\n",
    "        train_x = feat.iloc[train_index]\n",
    "        train_y = targ[train_index]\n",
    "\n",
    "        test_x = feat.iloc[test_index]\n",
    "        test_y = targ[test_index]\n",
    "        \n",
    "        # print(str(train_index) + \" \" + str(test_index)) \n",
    "        print(\"Validation subject: \" + str(groups[test_index[0]])) \n",
    "        \n",
    "        # Convert to pytorch\n",
    "        \n",
    "        ### TRAIN\n",
    "        \n",
    "        features = train_x.values.astype(np.float32)\n",
    "        labels   = train_y.values.astype(np.float32)\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        features_tensor = torch.from_numpy(features)\n",
    "        labels_tensor   = torch.from_numpy(labels)\n",
    "\n",
    "        # Assuming you want a sequence length of 1\n",
    "        # features_tensor = features_tensor.unsqueeze(1)\n",
    "\n",
    "        ######\n",
    "        # Reshape features tensor with sequence length of 50\n",
    "        sequence_length = 1\n",
    "        num_features    = features.shape[1]\n",
    "        num_samples     = features.shape[0]\n",
    "\n",
    "        # Calculate the number of sequences that can be formed\n",
    "        num_sequences = num_samples // sequence_length\n",
    "\n",
    "        # Truncate the tensor to fit the full sequences\n",
    "        features_tensor = features_tensor[:num_sequences * sequence_length, :]\n",
    "        labels_tensor = labels_tensor[:num_sequences * sequence_length]\n",
    "\n",
    "        # Reshape the tensor\n",
    "        features_tensor = features_tensor.view(num_sequences, sequence_length, num_features)\n",
    "\n",
    "        ### VALIDATION\n",
    "\n",
    "        features2 = test_x.values.astype(np.float32)\n",
    "        labels2   = test_y.values.astype(np.float32)\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        features_tensor2 = torch.from_numpy(features2)\n",
    "        labels_tensor2   = torch.from_numpy(labels2)\n",
    "\n",
    "        ######\n",
    "        # Reshape features tensor with sequence length of 50\n",
    "        sequence_length = 1\n",
    "        num_features = features2.shape[1]\n",
    "        num_samples  = features2.shape[0]\n",
    "\n",
    "        # Calculate the number of sequences that can be formed\n",
    "        num_sequences = num_samples // sequence_length\n",
    "\n",
    "        # Truncate the tensor to fit the full sequences\n",
    "        features_tensor2 = features_tensor2[:num_sequences * sequence_length, :]\n",
    "        labels_tensor2   = labels_tensor2[:num_sequences * sequence_length]\n",
    "\n",
    "        # Reshape the tensor\n",
    "        features_tensor2 = features_tensor2.view(num_sequences, sequence_length, num_features)\n",
    "        ######\n",
    "        \n",
    "        # Train model\n",
    "        \n",
    "        best_model = Model_Train(features_tensor, features_tensor2, labels_tensor, labels_tensor2, batch_size, num_epochs, model)\n",
    "        \n",
    "        print(\"This is the best model\" + str(best_model))\n",
    "        \n",
    "        # Test model\n",
    "        predict_y =  Model_Test_Best(features_tensor2, labels_tensor2, best_model)\n",
    "        \n",
    "        truths.append(test_y)\n",
    "        preds.append(predict_y.cpu().squeeze(1))\n",
    "        \n",
    "        \n",
    "    # combine subject folds\n",
    "    truth = pd.concat(truths)\n",
    "    truth.name = 'truth'\n",
    "    \n",
    "    \n",
    "    \n",
    "    pred = pd.Series( np.concatenate(preds),\n",
    "        index=truth.index,\n",
    "        name='prediction')\n",
    "    \n",
    "    \n",
    "    return truth, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206bebf",
   "metadata": {},
   "source": [
    "### Select Arousal or Valence"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ed60a1c",
   "metadata": {},
   "source": [
    "# Arousal\n",
    "\n",
    "truth_wavlm, pred_wavlm = experiment(\n",
    "    df_train_feat.iloc[:, 1:770],\n",
    "    df_train_arousal_lab['arousal'],\n",
    "    df_train_arousal_lab['Subject'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1fa969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valence\n",
    "\n",
    "truth_wavlm, pred_wavlm = experiment(\n",
    "    df_train_feat.iloc[:, 1:770],\n",
    "    df_train_valence_lab['valence'],\n",
    "    df_train_valence_lab['Subject'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import audmetric\n",
    "\n",
    "audmetric.concordance_cc(truth_wavlm, pred_wavlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3ef173",
   "metadata": {},
   "source": [
    "### Load devel dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b13114",
   "metadata": {},
   "source": [
    "#### Must select arousal or valence here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a7b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = df_devel_feat.iloc[:, 1:770].values.astype(np.float32)\n",
    "#features2 = df_devel_feat.values.astype(np.float32)\n",
    "\n",
    "\n",
    "# Arousal\n",
    "# labels2   = df_devel_arousal_lab['arousal'].values.astype(np.float32)\n",
    "\n",
    "# Valence\n",
    "labels2   = df_devel_valence_lab['valence'].values.astype(np.float32)\n",
    "\n",
    "# Normalize the features between -1 and 1 (adjust scaling based on your data)\n",
    "# features2 = (features - np.min(features)) / (np.max(features) - np.min(features)) * 2 - 1\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "features_tensor2 = torch.from_numpy(features2)\n",
    "labels_tensor2   = torch.from_numpy(labels2)\n",
    "\n",
    "######\n",
    "# Reshape features tensor with sequence length of 50\n",
    "sequence_length = 1\n",
    "num_features = features2.shape[1]\n",
    "num_samples  = features2.shape[0]\n",
    "\n",
    "# Calculate the number of sequences that can be formed\n",
    "num_sequences = num_samples // sequence_length\n",
    "\n",
    "# Truncate the tensor to fit the full sequences\n",
    "features_tensor2 = features_tensor2[:num_sequences * sequence_length, :]\n",
    "labels_tensor2   = labels_tensor2[:num_sequences * sequence_length]\n",
    "\n",
    "# Reshape the tensor\n",
    "features_tensor2 = features_tensor2.view(num_sequences, sequence_length, num_features)\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207694e1",
   "metadata": {},
   "source": [
    "### Load best model and predict\n",
    "\n",
    "#### Must select arousal or valence here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d3b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model for testing\n",
    "\n",
    "# Arousal\n",
    "# best_model_path = 'best_model_recola_arousal_loso.pth'  # Define the path to save the best model\n",
    "\n",
    "# Valence\n",
    "best_model_path = 'best_model_recola_valence_loso.pth'  # Define the path to save the best model\n",
    "\n",
    "criterion = CCCLoss()\n",
    "\n",
    "###############################################################################################\n",
    "best_model = ConvGRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "# best_model = ConvBLSTMModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "###############################################################################################\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "best_model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_outputs = best_model(features_tensor2.to(device))\n",
    "    test_loss    = criterion(test_outputs, labels_tensor2.unsqueeze(1).to(device))\n",
    "\n",
    "print(f'Test Loss: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab0d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = test_outputs.cpu().squeeze(1)\n",
    "truth = labels_tensor2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37240539",
   "metadata": {},
   "source": [
    "### Arousal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ba994",
   "metadata": {},
   "source": [
    "#### Smooth function to smooth predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b7dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "def F_Smooth(prediction, win_width):\n",
    "\n",
    "    df_pred = pd.DataFrame(prediction)\n",
    "    width = win_width\n",
    "    lag1 = df_pred.shift(1)\n",
    "    lag3 = df_pred.shift(width - 1)\n",
    "    window = lag3.rolling(window=width)\n",
    "    means = window.mean()\n",
    "    df_smoothed = concat([means, lag1, df_pred], axis=1)\n",
    "    df_smoothed.columns = ['mean', 't-1', 't+1']\n",
    "    df_smoothed['mean'] = df_smoothed['mean'].fillna(df_smoothed['t+1'])\n",
    "    \n",
    "    return df_smoothed['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117f67aa",
   "metadata": {},
   "source": [
    "#### Predict on Devel set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4462382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import audmetric\n",
    "\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, pred)))\n",
    "print(\"MSE = \" + str(audmetric.mean_absolute_error(truth, pred)))\n",
    "print(\"MAE = \" + str(audmetric.mean_squared_error(truth, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba17125",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 5)))  + \"  ( 5) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 10))) + \"  (10) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 15))) + \"  (15) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 20))) + \"  (20) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 25))) + \"  (25) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 30))) + \"  (30) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 35))) + \"  (35) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 40))) + \"  (40) \")\n",
    "\n",
    "print(\"MSE = \" + str(audmetric.mean_absolute_error(truth, F_Smooth(pred, 20)))+ \" (20)\")\n",
    "print(\"MAE = \" + str(audmetric.mean_squared_error(truth, F_Smooth(pred, 20)))+ \" (20)\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "785242d4",
   "metadata": {},
   "source": [
    "##### Best results so far Arousal Devel with LOSO at training:\n",
    "\n",
    "Test Loss: 0.2893\n",
    "/DEV/WavLM/Recola2018_16k/models/best_model_recola_arousal_loso_03032024.pth\n",
    "/DEV/WavLM/Recola2018_16k/models/best_model_recola_arousal_loso_epochs_03032024.csv\n",
    "\n",
    "CCC = 0.7107511144810469\n",
    "\n",
    "MSE = 0.11390312016010284\n",
    "MAE = 0.02063915506005287\n",
    "\n",
    "With smooth\n",
    "CCC = 0.7486076793101336  ( 5) \n",
    "CCC = 0.7687387950728284  (10) \n",
    "CCC = 0.7789464628838636  (15) \n",
    "CCC = 0.7829851476119157  (20) \n",
    "CCC = 0.7825649170753243  (25) \n",
    "CCC = 0.7789514500423856  (30) \n",
    "CCC = 0.7727902909048278  (35) \n",
    "CCC = 0.7648225374016705  (40)\n",
    "\n",
    "MSE = 0.09611338156505336 (20)\n",
    "MAE = 0.014646904760840154 (20)\n",
    "\n",
    "best_model = ConvGRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size   = num_features\n",
    "hidden_size  = 32 #128, 64, 32, 16\n",
    "num_layers   = 6\n",
    "output_size  = 1  # Single output for regression between -1 and +1\n",
    "dropout_prob = 0.20 \n",
    "\n",
    "# Train the model\n",
    "num_epochs     = 1000\n",
    "batch_size     = 7501\n",
    "validate_every = 1  # Validate every 2 epochs\n",
    "patience       = 15  # Stop training if validation loss doesn't improve for 5 consecutive validations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f63e3e",
   "metadata": {},
   "source": [
    "### Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import audmetric\n",
    "\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, pred)))\n",
    "print(\"MSE = \" + str(audmetric.mean_absolute_error(truth, pred)))\n",
    "print(\"MAE = \" + str(audmetric.mean_squared_error(truth, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7aa70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 5))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 10))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 15))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 20))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 25))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 30))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 35))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 40))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 45))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 50))))\n",
    "\n",
    "print(\"MSE = \" + str(audmetric.mean_absolute_error(truth, F_Smooth(pred, 25))))\n",
    "print(\"MAE = \" + str(audmetric.mean_squared_error(truth, F_Smooth(pred, 25))))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1b5d3f8",
   "metadata": {},
   "source": [
    "##### Best results so far Valence Devel:\n",
    "\n",
    "Test Loss: 0.6367\n",
    "/DEV/WavLM/Recola2018_16k/models/best_model_recola_valence_loso_03032024.pth\n",
    "/DEV/WavLM/Recola2018_16k/models/best_model_recola_valence_loso_epochs_03032024.csv\n",
    "\n",
    "CCC = 0.35387330344214085\n",
    "\n",
    "MSE = 0.10845673829317093\n",
    "MAE = 0.01992058753967285\n",
    "\n",
    "With Smooth\n",
    "CCC = 0.4158365728866413\n",
    "CCC = 0.4485323174502916\n",
    "CCC = 0.4689329307149778\n",
    "CCC = 0.48165148669266095\n",
    "CCC = 0.4888527510582969\n",
    "CCC = 0.4921716187434845\n",
    "CCC = 0.4926799810218907\n",
    "CCC = 0.49087949801229924\n",
    "CCC = 0.48723823316745624\n",
    "CCC = 0.4821973971633789\n",
    "\n",
    "MSE = 0.08750272508878654\n",
    "MAE = 0.012485581522185363\n",
    "\n",
    "best_model = ConvGRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size   = num_features\n",
    "hidden_size  = 32 #128, 64, 32, 16\n",
    "num_layers   = 5\n",
    "output_size  = 1  # Single output for regression between -1 and +1\n",
    "dropout_prob = 0.25 \n",
    "\n",
    "# Train the model\n",
    "num_epochs     = 1000\n",
    "batch_size     = 7501\n",
    "validate_every = 1  # Validate every 2 epochs\n",
    "patience       = 15  # Stop training if validation loss doesn't improve for 5 consecutive validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac44fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "predict_y = F_Smooth(pred, 5)\n",
    "\n",
    "dt = 1\n",
    "t = np.arange(0, len(predict_y), dt)\n",
    "s1 = df_devel_arousal_lab['arousal']\n",
    "s2 = predict_y                # white noise 2\n",
    "\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "axs[0].plot(t, s1, t, s2)\n",
    "axs[0].set_xlim(0, len(predict_y))\n",
    "axs[0].set_xlabel('time')\n",
    "axs[0].set_ylabel('s1 and s2')\n",
    "axs[0].grid(True)\n",
    "\n",
    "cxy, f = axs[1].cohere(s1, s2, 256, 1. / dt)\n",
    "axs[1].set_ylabel('coherence')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820cf17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "plt.figure(figsize=(200,6))\n",
    "#fig.tight_layout()\n",
    "\n",
    "predict_y = F_Smooth(pred, 20)\n",
    "\n",
    "dt = 1\n",
    "t = np.arange(0, len(predict_y), dt)\n",
    "s1 = df_devel_valence_lab['valence']\n",
    "s2 = predict_y               # white noise 2\n",
    "\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "axs[0].plot(t, s2, t, s1)\n",
    "axs[0].set_xlim(0, len(predict_y))\n",
    "axs[0].set_xlabel('time')\n",
    "axs[0].set_ylabel('s1 and s2')\n",
    "axs[0].grid(True)\n",
    "\n",
    "cxy, f = axs[1].cohere(s1, s2, 256, 1. / dt)\n",
    "axs[1].set_ylabel('coherence')\n",
    "\n",
    "#plt.figure(figsize=(20,6))\n",
    "#fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa1d665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f0378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436aea6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
