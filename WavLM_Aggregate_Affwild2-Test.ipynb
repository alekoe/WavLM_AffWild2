{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f42e82b-46c1-48bc-a9bd-eda21ca7a3c1",
   "metadata": {},
   "source": [
    "\n",
    "# Aggregate WavLM features to adapt to label files\n",
    "\n",
    "### For AffWild2 Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760109a1-c818-49cf-abd3-377b65994a33",
   "metadata": {},
   "source": [
    "##### https://github.com/microsoft/unilm/tree/master/wavlm\n",
    "##### https://github.com/audeering/w2v2-how-to/blob/main/notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8263d9b9-de2b-418e-b1a5-da5323499a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814ae24",
   "metadata": {},
   "source": [
    "### Process Feature Files - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f98fa97-9848-495d-b793-ba9687c3dc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/etsmtl/akoerich/DEV/WavLM\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "191da55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10-60-1280x720.wavlm',\n",
       " '130-25-1280x720.wavlm',\n",
       " '135-24-1920x1080.wavlm',\n",
       " '30-30-1920x1080.wavlm',\n",
       " '46-30-484x360.wavlm',\n",
       " '49-30-1280x720.wavlm',\n",
       " '52-30-1280x720.wavlm',\n",
       " '6-30-1920x1080.wavlm',\n",
       " 'video10_1.wavlm',\n",
       " 'video2.wavlm',\n",
       " 'video29.wavlm',\n",
       " 'video49.wavlm',\n",
       " 'video5.wavlm',\n",
       " 'video55.wavlm',\n",
       " 'video59.wavlm',\n",
       " 'video74.wavlm']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to feature files\n",
    "path_train_wavs = 'features/missing_left_right'\n",
    "#path_train_wavs = 'features/valid'\n",
    "\n",
    "extension_wav = 'wavlm'\n",
    "\n",
    "train_wavs = [file for file in os.listdir(path_train_wavs) if file.endswith(extension_wav)]\n",
    "\n",
    "sorted_train_wavs = sorted(train_wavs)\n",
    "sorted_train_wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfd0ab59-5185-4399-8850-e24f716fdb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_train_wavs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aad4e9-9b3a-434a-b67b-f2b3380a8179",
   "metadata": {},
   "source": [
    "### Process Label Files - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5ca138a-b62d-4acf-9241-62894e5e91e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10-60-1280x720.txt',\n",
       " '130-25-1280x720.txt',\n",
       " '135-24-1920x1080.txt',\n",
       " '30-30-1920x1080.txt',\n",
       " '46-30-484x360.txt',\n",
       " '49-30-1280x720.txt',\n",
       " '52-30-1280x720.txt',\n",
       " '6-30-1920x1080.txt',\n",
       " 'video10_1.txt',\n",
       " 'video2.txt',\n",
       " 'video29.txt',\n",
       " 'video49.txt',\n",
       " 'video5.txt',\n",
       " 'video55.txt',\n",
       " 'video59.txt',\n",
       " 'video74.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to feature files\n",
    "#path_train_labels = '../Affwild/6thABAWAnnotations/VA_Estimation_Challenge/Train_Set'\n",
    "#path_train_labels = '/projets2/AS84330/Datasets/Affwild/VA_annotations/Test_Set'\n",
    "path_train_labels = 'features/missing_left_right'\n",
    "\n",
    "extension_lab = 'txt'\n",
    "\n",
    "train_labels = [file for file in os.listdir(path_train_labels) if file.endswith(extension_lab)]\n",
    "\n",
    "sorted_train_labels = sorted(train_labels)\n",
    "sorted_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "809b046c-19bd-4ea7-a115-71ea889a41b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66545e80-7bec-40ac-abf3-16951e80766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/etsmtl/akoerich/DEV/WavLM\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "928f3079-ec46-46bf-8ff2-c43d76373270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-60-1280x720\n",
      "130-25-1280x720\n",
      "135-24-1920x1080\n",
      "30-30-1920x1080\n",
      "46-30-484x360\n",
      "49-30-1280x720\n",
      "52-30-1280x720\n",
      "6-30-1920x1080\n",
      "video10_1\n",
      "video2\n",
      "video29\n",
      "video49\n",
      "video5\n",
      "video55\n",
      "video59\n",
      "video74\n",
      "16 matches\n"
     ]
    }
   ],
   "source": [
    "# Function to remove file extensions\n",
    "def remove_extension(filename):\n",
    "    return os.path.splitext(filename)[0]\n",
    "\n",
    "# Remove extensions from both lists\n",
    "cleaned_train_wavs = [remove_extension(filename) for filename in sorted_train_wavs]\n",
    "cleaned_train_labels = [remove_extension(filename) for filename in sorted_train_labels]\n",
    "\n",
    "counter = 0\n",
    "matched_train_list = []\n",
    "\n",
    "# Check and print filenames\n",
    "for filename in cleaned_train_wavs:\n",
    "    if filename in cleaned_train_labels:\n",
    "        print(filename)\n",
    "        matched_train_list.append(filename)\n",
    "        counter = counter + 1\n",
    "    #else:\n",
    "    #    print(f\"{filename} not found in labels\")\n",
    "\n",
    "print( str(counter) + \" matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3868f663-8c31-4d1b-8043-1899ba62fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_df = pd.DataFrame(matched_train_list)\n",
    "matched_df.to_csv('matched_left_right.csv', index=False, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3c61364-0795-4eec-83fc-f84b46450533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'features/missing_left_right10-60-1280x720.wavlm'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_train_labels+matched_train_list[0]+'.'+extension_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92c8c914-a496-47e3-8c05-f7d89695567d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10-60-1280x720',\n",
       " '130-25-1280x720',\n",
       " '135-24-1920x1080',\n",
       " '30-30-1920x1080',\n",
       " '46-30-484x360',\n",
       " '49-30-1280x720',\n",
       " '52-30-1280x720',\n",
       " '6-30-1920x1080',\n",
       " 'video10_1',\n",
       " 'video2',\n",
       " 'video29',\n",
       " 'video49',\n",
       " 'video5',\n",
       " 'video55',\n",
       " 'video59',\n",
       " 'video74']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dfe7647-9a45-4f26-87d8-127eec0c2a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied rolling mean to 10-60-1280x720.wavlm  4167 2502 2502\n",
      "Features saved to 10-60-1280x720.wavlmpooled in features/missing_left_right_pooled\n",
      "Applied rolling mean to 130-25-1280x720.wavlm  14101 8462 8462\n",
      "Features saved to 130-25-1280x720.wavlmpooled in features/missing_left_right_pooled\n",
      "Applied rolling mean to 135-24-1920x1080.wavlm  15701 9424 9424\n",
      "Features saved to 135-24-1920x1080.wavlmpooled in features/missing_left_right_pooled\n",
      "Applied rolling mean to 30-30-1920x1080.wavlm  5106 3063 3063\n",
      "Features saved to 30-30-1920x1080.wavlmpooled in features/missing_left_right_pooled\n",
      "Applied rolling mean to 46-30-484x360.wavlm  4900 2945 2945\n",
      "Features saved to 46-30-484x360.wavlmpooled in features/missing_left_right_pooled\n",
      "Applied rolling mean to 49-30-1280x720.wavlm  1426 848 848\n",
      "Features saved to 49-30-1280x720.wavlmpooled in features/missing_left_right_pooled\n",
      "Applied rolling mean to 52-30-1280x720.wavlm  3208 1925 1925\n",
      "Features saved to 52-30-1280x720.wavlmpooled in features/missing_left_right_pooled\n",
      "Applied rolling mean to 6-30-1920x1080.wavlm  13308 7986 7986\n",
      "Features saved to 6-30-1920x1080.wavlmpooled in features/missing_left_right_pooled\n",
      "Applied rolling mean to video10_1.wavlm  7094 4257 4257\n",
      "Features saved to video10_1.wavlmpooled in features/missing_left_right_pooled\n",
      "Applied rolling mean to video2.wavlm  3440 2065 2065\n",
      "Features saved to video2.wavlmpooled in features/missing_left_right_pooled\n",
      "Applied rolling mean to video29.wavlm  18766 11260 11260\n",
      "Features saved to video29.wavlmpooled in features/missing_left_right_pooled\n",
      "Applied rolling mean to video49.wavlm  10211 6122 6122\n",
      "Features saved to video49.wavlmpooled in features/missing_left_right_pooled\n",
      "Applied rolling mean to video5.wavlm  4113 2469 2469\n",
      "Features saved to video5.wavlmpooled in features/missing_left_right_pooled\n",
      "Applied rolling mean to video55.wavlm  14032 8420 8420\n",
      "Features saved to video55.wavlmpooled in features/missing_left_right_pooled\n",
      "Applied rolling mean to video59.wavlm  11172 6703 6703\n",
      "Features saved to video59.wavlmpooled in features/missing_left_right_pooled\n",
      "Applied rolling mean to video74.wavlm  30313 18190 18190\n",
      "Features saved to video74.wavlmpooled in features/missing_left_right_pooled\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Placeholder lists (replace with your actual data)\n",
    "features_path = \"features/missing_left_right\"  # Path to the features folder\n",
    "labels_path = \"/projets2/AS84330/Datasets/Affwild/VA_annotations/Test_Set\"  # Path to the labels folder\n",
    "#labels_path = \"../Affwild/6thABAWAnnotations/VA_Estimation_Challenge/Train_Set\"  # Path to the labels folder\n",
    "labels_path = \"features/missing_left_right\"  # Path to the labels folder\n",
    "output_path = \"features/missing_left_right_pooled\"\n",
    "\n",
    "# Function to add extension to filenames\n",
    "def add_extension(filename):\n",
    "    return f\"{filename}.wavlm\"\n",
    "\n",
    "def add_extension2(filename):\n",
    "    return f\"{filename}.txt\"\n",
    "\n",
    "def add_extension3(filename):\n",
    "    return f\"{filename}.wavlmpooled\"\n",
    "\n",
    "# Iterate through the matched_train_list\n",
    "for filename in matched_train_list:\n",
    "    # Add extension to the filename\n",
    "    csv_filename  = add_extension(filename)\n",
    "    csv_filename2 = add_extension2(filename)\n",
    "    csv_filename3 = add_extension3(filename)\n",
    "\n",
    "    # Construct full paths for features and labels CSV files\n",
    "    features_file_path = os.path.join(features_path, csv_filename)\n",
    "    labels_file_path   = os.path.join(labels_path, csv_filename2)\n",
    "\n",
    "    # Check if both files exist\n",
    "    if os.path.exists(features_file_path) and os.path.exists(labels_file_path):\n",
    "        # Read data into dataframes\n",
    "        features_df = pd.read_csv(features_file_path )\n",
    "        labels_df = pd.read_csv(labels_file_path )\n",
    "\n",
    "        # Compute the length of dataframes\n",
    "        features_length = len(features_df)\n",
    "        labels_length = len(labels_df)\n",
    "\n",
    "        # Check if the feature dataframe is longer than the label dataframe\n",
    "        if features_length > labels_length:\n",
    "            #features_length\n",
    "            # Use .rolling or any other processing you want\n",
    "            # Here's an example of using .rolling with a window of 3\n",
    "            #rolling_mean = features_df.rolling(window=3).mean()\n",
    "            #print(f\"Applied rolling mean to {csv_filename} \" + \" \" + str(features_length) + \" \" + str(labels_length) + \" \" + str(len(rolling_mean) ) )\n",
    "        \n",
    "            # If the lengths are different, calculate the average of features\n",
    "            #if len(features_df) < len(labels_df):\n",
    "            ratio = features_length / labels_length\n",
    "            features_df = features_df.groupby(features_df.index // ratio).mean()\n",
    "        \n",
    "            print(f\"Applied rolling mean to {csv_filename} \" + \" \" + str(features_length) + \" \" + str(labels_length) + \" \" + str(len(features_df) ) )\n",
    "\n",
    "            features_df.to_csv(os.path.join(output_path, csv_filename3), index=False, header = None)\n",
    "            print(f\"Features saved to {csv_filename3} in {output_path}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"{csv_filename}: Feature dataframe length is not greater than label dataframe length\")\n",
    "    else:\n",
    "        print(f\"{csv_filename}: Features or labels file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "052fdd9d-89aa-4770-bebf-0385b4fdd01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'features/missing_left_right/video74.wavlm'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c378082-c27d-477f-9b53-eee74046c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4ec3c66-2954-4ea3-a252-cfd941184eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.091612</td>\n",
       "      <td>0.016768</td>\n",
       "      <td>-0.028088</td>\n",
       "      <td>0.011590</td>\n",
       "      <td>0.025280</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.249539</td>\n",
       "      <td>-0.049620</td>\n",
       "      <td>0.099974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006443</td>\n",
       "      <td>0.048401</td>\n",
       "      <td>-0.041436</td>\n",
       "      <td>0.705418</td>\n",
       "      <td>-0.065318</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.163939</td>\n",
       "      <td>-0.060558</td>\n",
       "      <td>-0.050333</td>\n",
       "      <td>0.211756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.048854</td>\n",
       "      <td>-0.045267</td>\n",
       "      <td>-0.021170</td>\n",
       "      <td>0.138877</td>\n",
       "      <td>0.007864</td>\n",
       "      <td>0.033854</td>\n",
       "      <td>0.161771</td>\n",
       "      <td>-0.094119</td>\n",
       "      <td>0.112792</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010188</td>\n",
       "      <td>0.080097</td>\n",
       "      <td>-0.076656</td>\n",
       "      <td>0.383499</td>\n",
       "      <td>-0.034185</td>\n",
       "      <td>-0.025057</td>\n",
       "      <td>0.133476</td>\n",
       "      <td>-0.028329</td>\n",
       "      <td>-0.076300</td>\n",
       "      <td>1.214561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.031575</td>\n",
       "      <td>-0.069188</td>\n",
       "      <td>-0.007771</td>\n",
       "      <td>0.127493</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.048240</td>\n",
       "      <td>0.141271</td>\n",
       "      <td>-0.134403</td>\n",
       "      <td>0.137320</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053306</td>\n",
       "      <td>0.115821</td>\n",
       "      <td>-0.139447</td>\n",
       "      <td>0.352307</td>\n",
       "      <td>-0.029890</td>\n",
       "      <td>-0.041244</td>\n",
       "      <td>0.111210</td>\n",
       "      <td>-0.083679</td>\n",
       "      <td>-0.097375</td>\n",
       "      <td>1.423105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>-0.084932</td>\n",
       "      <td>-0.035194</td>\n",
       "      <td>0.111344</td>\n",
       "      <td>-0.015048</td>\n",
       "      <td>0.006236</td>\n",
       "      <td>0.136626</td>\n",
       "      <td>-0.095470</td>\n",
       "      <td>0.065097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005684</td>\n",
       "      <td>0.122645</td>\n",
       "      <td>-0.136878</td>\n",
       "      <td>0.119333</td>\n",
       "      <td>-0.059054</td>\n",
       "      <td>-0.002727</td>\n",
       "      <td>0.123285</td>\n",
       "      <td>-0.037186</td>\n",
       "      <td>-0.088664</td>\n",
       "      <td>1.269558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.041039</td>\n",
       "      <td>-0.087452</td>\n",
       "      <td>-0.000629</td>\n",
       "      <td>0.098690</td>\n",
       "      <td>-0.021191</td>\n",
       "      <td>-0.014168</td>\n",
       "      <td>0.144734</td>\n",
       "      <td>-0.079154</td>\n",
       "      <td>0.074897</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017981</td>\n",
       "      <td>0.082626</td>\n",
       "      <td>-0.120028</td>\n",
       "      <td>0.440153</td>\n",
       "      <td>-0.068928</td>\n",
       "      <td>-0.046884</td>\n",
       "      <td>0.097714</td>\n",
       "      <td>-0.055474</td>\n",
       "      <td>-0.095424</td>\n",
       "      <td>1.739801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18185.0</th>\n",
       "      <td>30305.5</td>\n",
       "      <td>-0.057120</td>\n",
       "      <td>0.011766</td>\n",
       "      <td>0.074895</td>\n",
       "      <td>0.026012</td>\n",
       "      <td>0.168878</td>\n",
       "      <td>-0.097109</td>\n",
       "      <td>0.298676</td>\n",
       "      <td>-0.234427</td>\n",
       "      <td>-0.091703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044907</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.121175</td>\n",
       "      <td>0.350045</td>\n",
       "      <td>-0.055578</td>\n",
       "      <td>-0.049896</td>\n",
       "      <td>0.283309</td>\n",
       "      <td>-0.157848</td>\n",
       "      <td>-0.076121</td>\n",
       "      <td>-0.638387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18186.0</th>\n",
       "      <td>30307.5</td>\n",
       "      <td>-0.018141</td>\n",
       "      <td>-0.001769</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>-0.021800</td>\n",
       "      <td>0.111232</td>\n",
       "      <td>-0.105297</td>\n",
       "      <td>0.271074</td>\n",
       "      <td>-0.221772</td>\n",
       "      <td>-0.116560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041876</td>\n",
       "      <td>0.010371</td>\n",
       "      <td>0.135935</td>\n",
       "      <td>0.418278</td>\n",
       "      <td>0.016679</td>\n",
       "      <td>-0.047411</td>\n",
       "      <td>0.255227</td>\n",
       "      <td>-0.100238</td>\n",
       "      <td>-0.097188</td>\n",
       "      <td>-1.261223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18187.0</th>\n",
       "      <td>30309.0</td>\n",
       "      <td>0.056843</td>\n",
       "      <td>-0.132430</td>\n",
       "      <td>0.051144</td>\n",
       "      <td>-0.041708</td>\n",
       "      <td>0.098426</td>\n",
       "      <td>-0.141053</td>\n",
       "      <td>0.332225</td>\n",
       "      <td>-0.141775</td>\n",
       "      <td>-0.028454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091616</td>\n",
       "      <td>0.021070</td>\n",
       "      <td>0.181466</td>\n",
       "      <td>-0.011258</td>\n",
       "      <td>0.082789</td>\n",
       "      <td>-0.006059</td>\n",
       "      <td>0.103785</td>\n",
       "      <td>-0.118667</td>\n",
       "      <td>-0.095927</td>\n",
       "      <td>-1.098625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18188.0</th>\n",
       "      <td>30310.5</td>\n",
       "      <td>0.086072</td>\n",
       "      <td>-0.015643</td>\n",
       "      <td>-0.017705</td>\n",
       "      <td>0.012022</td>\n",
       "      <td>0.009483</td>\n",
       "      <td>-0.090173</td>\n",
       "      <td>0.163142</td>\n",
       "      <td>-0.075044</td>\n",
       "      <td>-0.036540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032985</td>\n",
       "      <td>0.073148</td>\n",
       "      <td>0.089673</td>\n",
       "      <td>-0.479751</td>\n",
       "      <td>0.109401</td>\n",
       "      <td>0.050138</td>\n",
       "      <td>0.216689</td>\n",
       "      <td>-0.053634</td>\n",
       "      <td>-0.035701</td>\n",
       "      <td>0.759893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18189.0</th>\n",
       "      <td>30312.0</td>\n",
       "      <td>-0.122367</td>\n",
       "      <td>0.050423</td>\n",
       "      <td>-0.030472</td>\n",
       "      <td>0.122802</td>\n",
       "      <td>0.038877</td>\n",
       "      <td>0.180143</td>\n",
       "      <td>0.183528</td>\n",
       "      <td>-0.129914</td>\n",
       "      <td>0.019353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094756</td>\n",
       "      <td>-0.005461</td>\n",
       "      <td>-0.034959</td>\n",
       "      <td>0.718164</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.073146</td>\n",
       "      <td>0.257717</td>\n",
       "      <td>-0.052904</td>\n",
       "      <td>-0.037684</td>\n",
       "      <td>0.841359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18190 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0         0         1         2         3         4  \\\n",
       "0.0             0.5  0.091612  0.016768 -0.028088  0.011590  0.025280   \n",
       "1.0             2.5  0.048854 -0.045267 -0.021170  0.138877  0.007864   \n",
       "2.0             4.0  0.031575 -0.069188 -0.007771  0.127493  0.002000   \n",
       "3.0             5.5  0.050565 -0.084932 -0.035194  0.111344 -0.015048   \n",
       "4.0             7.5  0.041039 -0.087452 -0.000629  0.098690 -0.021191   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "18185.0     30305.5 -0.057120  0.011766  0.074895  0.026012  0.168878   \n",
       "18186.0     30307.5 -0.018141 -0.001769  0.072464 -0.021800  0.111232   \n",
       "18187.0     30309.0  0.056843 -0.132430  0.051144 -0.041708  0.098426   \n",
       "18188.0     30310.5  0.086072 -0.015643 -0.017705  0.012022  0.009483   \n",
       "18189.0     30312.0 -0.122367  0.050423 -0.030472  0.122802  0.038877   \n",
       "\n",
       "                5         6         7         8  ...       758       759  \\\n",
       "0.0      0.001403  0.249539 -0.049620  0.099974  ... -0.006443  0.048401   \n",
       "1.0      0.033854  0.161771 -0.094119  0.112792  ... -0.010188  0.080097   \n",
       "2.0      0.048240  0.141271 -0.134403  0.137320  ... -0.053306  0.115821   \n",
       "3.0      0.006236  0.136626 -0.095470  0.065097  ... -0.005684  0.122645   \n",
       "4.0     -0.014168  0.144734 -0.079154  0.074897  ... -0.017981  0.082626   \n",
       "...           ...       ...       ...       ...  ...       ...       ...   \n",
       "18185.0 -0.097109  0.298676 -0.234427 -0.091703  ...  0.044907  0.026400   \n",
       "18186.0 -0.105297  0.271074 -0.221772 -0.116560  ...  0.041876  0.010371   \n",
       "18187.0 -0.141053  0.332225 -0.141775 -0.028454  ...  0.091616  0.021070   \n",
       "18188.0 -0.090173  0.163142 -0.075044 -0.036540  ...  0.032985  0.073148   \n",
       "18189.0  0.180143  0.183528 -0.129914  0.019353  ... -0.094756 -0.005461   \n",
       "\n",
       "              760       761       762       763       764       765       766  \\\n",
       "0.0     -0.041436  0.705418 -0.065318  0.001058  0.163939 -0.060558 -0.050333   \n",
       "1.0     -0.076656  0.383499 -0.034185 -0.025057  0.133476 -0.028329 -0.076300   \n",
       "2.0     -0.139447  0.352307 -0.029890 -0.041244  0.111210 -0.083679 -0.097375   \n",
       "3.0     -0.136878  0.119333 -0.059054 -0.002727  0.123285 -0.037186 -0.088664   \n",
       "4.0     -0.120028  0.440153 -0.068928 -0.046884  0.097714 -0.055474 -0.095424   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "18185.0  0.121175  0.350045 -0.055578 -0.049896  0.283309 -0.157848 -0.076121   \n",
       "18186.0  0.135935  0.418278  0.016679 -0.047411  0.255227 -0.100238 -0.097188   \n",
       "18187.0  0.181466 -0.011258  0.082789 -0.006059  0.103785 -0.118667 -0.095927   \n",
       "18188.0  0.089673 -0.479751  0.109401  0.050138  0.216689 -0.053634 -0.035701   \n",
       "18189.0 -0.034959  0.718164  0.023256  0.073146  0.257717 -0.052904 -0.037684   \n",
       "\n",
       "              767  \n",
       "0.0      0.211756  \n",
       "1.0      1.214561  \n",
       "2.0      1.423105  \n",
       "3.0      1.269558  \n",
       "4.0      1.739801  \n",
       "...           ...  \n",
       "18185.0 -0.638387  \n",
       "18186.0 -1.261223  \n",
       "18187.0 -1.098625  \n",
       "18188.0  0.759893  \n",
       "18189.0  0.841359  \n",
       "\n",
       "[18190 rows x 769 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08a25522-e1a5-4d72-8ff1-e8bbd42fd6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># timestamp format v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>133.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18185</th>\n",
       "      <td>606167.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18186</th>\n",
       "      <td>606200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18187</th>\n",
       "      <td>606233.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18188</th>\n",
       "      <td>606267.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18189</th>\n",
       "      <td>606300.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18190 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       # timestamp format v2\n",
       "0                   0.000000\n",
       "1                  33.000000\n",
       "2                  67.000000\n",
       "3                 100.000000\n",
       "4                 133.000000\n",
       "...                      ...\n",
       "18185          606167.000000\n",
       "18186          606200.000000\n",
       "18187          606233.000000\n",
       "18188          606267.000000\n",
       "18189          606300.333333\n",
       "\n",
       "[18190 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b014f98-6f30-44d0-a7ca-bb68e8780fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db760a-1c43-4142-9080-3b7c094696a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a81364-b258-43a1-9546-ab47ce90d0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886c6d9-46bd-40ef-b643-eb9331b301a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23410c-8e8a-49f2-b796-c60eda971f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for wavs in matched_train_list:\n",
    "\n",
    "    if os.path.exists(wavs) & os.path.exists(labs):\n",
    "\n",
    "        print(\"Processing: \" + wavs)\n",
    "\n",
    "        # Process audio signal\n",
    "        #signal, sr = sf.read(wavs) \n",
    "        #signal2    = signal.reshape(-1, 1).astype(np.float32)\n",
    "        #signal2    = min_max_scaler.fit_transform(signal2).reshape(-1)\n",
    "        #df         = pd.DataFrame(interface.process_signal(signal2, sampling_rate))\n",
    "             \n",
    "        \n",
    "        #df_arousal = df['arousal'].to_numpy()\n",
    "        #df_valence = df['valence'].to_numpy()\n",
    "        \n",
    "        #df_arousal = df[['arousal']].to_numpy()\n",
    "        #df_valence = df[['valence']].to_numpy()\n",
    "        #df_arousal = min_max_scaler.fit_transform(df_arousal).reshape(-1)\n",
    "        #df_valence = min_max_scaler.fit_transform(df_valence).reshape(-1)\n",
    "        \n",
    "        # Process ground-truth\n",
    "        #df_gt         = pd.read_csv(labs).astype(np.float32)\n",
    "        #df_arousal_gt = df_gt['arousal'].to_numpy()\n",
    "        #df_valence_gt = df_gt['valence'].to_numpy()\n",
    "\n",
    "        #if len(df_arousal_gt) > len(df_arousal):\n",
    "        #    df_arousal = np.pad(df_arousal, (0,len(df_arousal_gt)-len(df_arousal)), 'constant', constant_values=(0,1)) \n",
    "        #    df_valence = np.pad(df_valence, (0,len(df_valence_gt)-len(df_valence)), 'constant', constant_values=(0,1)) \n",
    "            \n",
    "        #if len(df_arousal_gt) < len(df_arousal):\n",
    "        #    df_arousal_gt = np.pad(df_arousal_gt, (0,len(df_arousal)-len(df_arousal_gt)), 'constant', constant_values=(0,1))   \n",
    "        #    df_valence_gt = np.pad(df_valence_gt, (0,len(df_valence)-len(df_valence_gt)), 'constant', constant_values=(0,1))   \n",
    "\n",
    "        input(\"Press Enter to continue...\")\n",
    "\n",
    "        \n",
    "        if len(df_arousal_gt) != len(df_arousal):\n",
    "            print(\"Different lengths: \" + wavs + \": \" + str(len(df_arousal_gt)) + \" and \" + str(len(df_arousal)) )\n",
    "        else:\n",
    "            print( audmetric.concordance_cc(df_arousal_gt, df_arousal), audmetric.concordance_cc(df_valence_gt, df_valence) )\n",
    "\n",
    "        i += 1\n",
    "        if (i%1 == 0.000):\n",
    "            print(\"Files read: \"+str(i) )\n",
    "\n",
    "    else: \n",
    "        if os.path.exists(wavs) == False: \n",
    "            print(\"Does not exist: \" + wavs)\n",
    "\n",
    "        if os.path.exists(labs) == False:\n",
    "            print(\"Does not exist: \" + labs )\n",
    "        else:\n",
    "            print(\"No idea\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f4d59-cf76-4f31-8add-d3a0429dfce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1393c8b9-9b5e-4426-984c-955353156186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ccae4-e18b-4d07-9137-f1740032420a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191efefe-904d-4b27-905a-103a17b37f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678490d0-7297-4dd9-9351-ae07316d2033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d09d4c-8145-491e-a6fb-8426df3e230b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97abd2cf-fdf3-4bdf-8b4d-4df9f9449f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to feature files\n",
    "path_train = 'features/valid'\n",
    "extension = 'wavlm'\n",
    "\n",
    "train_files = [file for file in os.listdir(path_train) if file.endswith(extension)]\n",
    "\n",
    "sorted_train_files = sorted(train_files)\n",
    "sorted_train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f349d9a-eb3d-4069-995f-db92de8ee13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to feature files\n",
    "path_devel = '../Affwild/6thABAWAnnotations/VA_Estimation_Challenge/Validation_Set'\n",
    "extension = 'txt'\n",
    "\n",
    "devel_files = [file for file in os.listdir(path_devel) if file.endswith(extension)]\n",
    "\n",
    "sorted_devel_files = sorted(devel_files)\n",
    "sorted_devel_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22481e4e-bad7-40e0-b0ed-87ae08153d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78443d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to feature files\n",
    "path_devel = 'Recola2018_16k/features/devel'\n",
    "extension = 'wavlmbasefeatpoolloso'\n",
    "\n",
    "devel_files = [file for file in os.listdir(path_devel) if file.endswith(extension)]\n",
    "\n",
    "sorted_devel_files = sorted(devel_files)\n",
    "sorted_devel_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55350230-a9a7-4200-bcc9-548f35b8dfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce8bdec-3aa7-4805-ad99-9d892e27f38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5599ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file in sorted_train_files:\n",
    "    df = pd.read_csv(os.path.join(path_train, file))\n",
    "    \n",
    "    # pad to make single feature files compatible with labels (2 lines for Recola)\n",
    "    # pad the dataframe with a copy of the first row \n",
    "    new_row = pd.DataFrame(df.loc[0])\n",
    "\n",
    "    # simply concatenate both dataframes\n",
    "    df = pd.concat([new_row.T, df]).reset_index(drop = True)\n",
    "    \n",
    "    # pad the dataframe with a copy of the last row \n",
    "    new_row2 = pd.DataFrame(df.loc[len(df)-1])\n",
    "    \n",
    "    # simply concatenate both dataframes\n",
    "    df = pd.concat([df,new_row2.T]).reset_index(drop = True)\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "df_train_feat = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Drop first index column (unamed 0)\n",
    "df_train_feat.drop(df_train_feat.columns[[0]], axis=1, inplace=True)\n",
    "\n",
    "df_train_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_devel = []\n",
    "for file in sorted_devel_files:\n",
    "    df_devel = pd.read_csv(os.path.join(path_devel, file))\n",
    "    \n",
    "    # pad to make single feature files compatible with labels (2 lines for Recola)\n",
    "    # pad the dataframe with a copy of the first row \n",
    "    new_row = pd.DataFrame(df_devel.loc[0])\n",
    "\n",
    "    # simply concatenate both dataframes\n",
    "    df_devel = pd.concat([new_row.T, df_devel]).reset_index(drop = True)\n",
    "    \n",
    "    # pad the dataframe with a copy of the last row \n",
    "    new_row2 = pd.DataFrame(df_devel.loc[len(df_devel)-1])\n",
    "    \n",
    "    # simply concatenate both dataframes\n",
    "    df_devel = pd.concat([df_devel,new_row2.T]).reset_index(drop = True)\n",
    "    \n",
    "    dfs_devel.append(df_devel)\n",
    "    \n",
    "df_devel_feat = pd.concat(dfs_devel, ignore_index=True)\n",
    "\n",
    "# Drop first index column (unamed 0)\n",
    "df_devel_feat.drop(df_devel_feat.columns[[0]], axis=1, inplace=True)\n",
    "\n",
    "df_devel_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a43bb",
   "metadata": {},
   "source": [
    "### Process label files - Train and Devel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dec438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to label files\n",
    "path_train_arousal_labels = 'Recola2018_16k/labels/arousal/Train/'\n",
    "extension = 'arff'\n",
    "\n",
    "train_files_arousal_labels = [file for file in os.listdir(path_train_arousal_labels) if file.endswith(extension)]\n",
    "\n",
    "sorted_train_arousal_labels = sorted(train_files_arousal_labels)\n",
    "sorted_train_arousal_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac9076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to label files\n",
    "path_train_valence_labels = 'Recola2018_16k/labels/valence/Train/'\n",
    "extension = 'arff'\n",
    "\n",
    "train_files_valence_labels = [file for file in os.listdir(path_train_valence_labels) if file.endswith(extension)]\n",
    "\n",
    "sorted_train_valence_labels = sorted(train_files_valence_labels)\n",
    "sorted_train_valence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b775647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to label files\n",
    "path_devel_arousal_labels = 'Recola2018_16k/labels/arousal/Devel/'\n",
    "extension = 'arff'\n",
    "\n",
    "devel_files_arousal_labels = [file for file in os.listdir(path_devel_arousal_labels) if file.endswith(extension)]\n",
    "\n",
    "sorted_devel_arousal_labels = sorted(devel_files_arousal_labels)\n",
    "sorted_devel_arousal_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c74198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to label files\n",
    "path_devel_valence_labels = 'Recola2018_16k/labels/valence/Devel/'\n",
    "extension = 'arff'\n",
    "\n",
    "devel_files_valence_labels = [file for file in os.listdir(path_devel_valence_labels) if file.endswith(extension)]\n",
    "\n",
    "sorted_devel_valence_labels = sorted(devel_files_valence_labels)\n",
    "sorted_devel_valence_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f848d6a",
   "metadata": {},
   "source": [
    "### Create dataframes for label files - Train and Devel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4396d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = []\n",
    "for file in sorted_train_arousal_labels:\n",
    "    df2 = pd.read_csv(os.path.join(path_train_arousal_labels, file), sep=\",\", header=None)\n",
    "    df2.drop(df2.columns[[0,1]], axis=1, inplace=True)\n",
    "\n",
    "    # Add subject column\n",
    "    subject = file.split('.')[0]\n",
    "    df2.insert(0, \"Subject\", subject, True)\n",
    "    \n",
    "    dfl.append(df2)\n",
    "\n",
    "df_train_arousal_lab = pd.concat(dfl, ignore_index=True)\n",
    "df_train_arousal_lab = df_train_arousal_lab.rename(columns={2:\"arousal\"})\n",
    "df_train_arousal_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d695381",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = []\n",
    "for file in sorted_train_valence_labels:\n",
    "    df2 = pd.read_csv(os.path.join(path_train_valence_labels, file), sep=\",\", header=None)\n",
    "    df2.drop(df2.columns[[0,1]], axis=1, inplace=True)\n",
    "    \n",
    "    # Add subject column\n",
    "    subject = file.split('.')[0]\n",
    "    df2.insert(0, \"Subject\", subject, True)\n",
    "\n",
    "    dfl.append(df2)\n",
    "    \n",
    "df_train_valence_lab = pd.concat(dfl, ignore_index=True)\n",
    "df_train_valence_lab = df_train_valence_lab.rename(columns={2:\"valence\"})\n",
    "df_train_valence_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38456f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = []\n",
    "for file in sorted_devel_arousal_labels:\n",
    "    df2 = pd.read_csv(os.path.join(path_devel_arousal_labels, file), sep=\",\", header=None)\n",
    "    df2.drop(df2.columns[[0,1]], axis=1, inplace=True)\n",
    "\n",
    "    # Add subject column\n",
    "    subject = file.split('.')[0]\n",
    "    df2.insert(0, \"Subject\", subject, True)\n",
    "\n",
    "    dfl.append(df2)\n",
    "    \n",
    "df_devel_arousal_lab = pd.concat(dfl, ignore_index=True)\n",
    "df_devel_arousal_lab = df_devel_arousal_lab.rename(columns={2:\"arousal\"})\n",
    "df_devel_arousal_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c8bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = []\n",
    "for file in sorted_devel_valence_labels:\n",
    "    df2 = pd.read_csv(os.path.join(path_devel_valence_labels, file), sep=\",\", header=None)\n",
    "    df2.drop(df2.columns[[0,1]], axis=1, inplace=True)\n",
    "    \n",
    "    # Add subject column\n",
    "    subject = file.split('.')[0]\n",
    "    df2.insert(0, \"Subject\", subject, True)\n",
    "\n",
    "    dfl.append(df2)\n",
    "    \n",
    "df_devel_valence_lab = pd.concat(dfl, ignore_index=True)\n",
    "df_devel_valence_lab = df_devel_valence_lab.rename(columns={2:\"valence\"})\n",
    "df_devel_valence_lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e572759e",
   "metadata": {},
   "source": [
    "## Train a GRU regression model for arousal / valence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73becbce",
   "metadata": {},
   "source": [
    "### Initialize PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a681dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a081309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30c879",
   "metadata": {},
   "source": [
    "### Define CCC loss function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199927ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CCCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CCCLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        mean_pred = torch.mean(pred)\n",
    "        mean_target = torch.mean(target)\n",
    "\n",
    "        covar = torch.mean((pred - mean_pred) * (target - mean_target))\n",
    "        var_pred = torch.var(pred)\n",
    "        var_target = torch.var(target)\n",
    "\n",
    "        ccc = 2 * covar / (var_pred + var_target + (mean_pred - mean_target)**2)\n",
    "        return (1 - ccc)  # Minimize 1 - CCC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed9e59",
   "metadata": {},
   "source": [
    "### Define a PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f58e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1be5d6",
   "metadata": {},
   "source": [
    "#### Must choose arousal or valence here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2814445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_train_feat.iloc[:, 1:770].values.astype(np.float32)\n",
    "\n",
    "# Arousal\n",
    "# labels   = df_train_arousal_lab['arousal'].values.astype(np.float32)\n",
    "# subjects = df_train_arousal_lab['Subject'].values\n",
    "\n",
    "# Valence\n",
    "labels   = df_train_valence_lab['valence'].values.astype(np.float32)\n",
    "subjects = df_train_valence_lab['Subject'].values\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "features_tensor = torch.from_numpy(features)\n",
    "labels_tensor   = torch.from_numpy(labels)\n",
    "\n",
    "# Assuming you want a sequence length of 1\n",
    "# features_tensor = features_tensor.unsqueeze(1)\n",
    "\n",
    "######\n",
    "# Reshape features tensor with sequence length of 50\n",
    "sequence_length = 1\n",
    "num_features    = features.shape[1]\n",
    "num_samples     = features.shape[0]\n",
    "\n",
    "# Calculate the number of sequences that can be formed\n",
    "num_sequences = num_samples // sequence_length\n",
    "\n",
    "# Truncate the tensor to fit the full sequences\n",
    "features_tensor = features_tensor[:num_sequences * sequence_length, :]\n",
    "labels_tensor = labels_tensor[:num_sequences * sequence_length]\n",
    "\n",
    "# Reshape the tensor\n",
    "features_tensor = features_tensor.view(num_sequences, sequence_length, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573014d",
   "metadata": {},
   "source": [
    "### Model parameters and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80750ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_size   = num_features\n",
    "hidden_size  = 32 #128, 64, 32, 16\n",
    "num_layers   = 5\n",
    "output_size  = 1  # Single output for regression between -1 and +1\n",
    "dropout_prob = 0.25 \n",
    "\n",
    "# Train the model\n",
    "num_epochs     = 1000\n",
    "batch_size     = 7501\n",
    "validate_every = 1  # Validate every 2 epochs\n",
    "patience       = 15  # Stop training if validation loss doesn't improve for 5 consecutive validations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f8871",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8260475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Define the GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, dropout_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        output = self.fc(gru_out[:, -1, :])  # Take the output from the last time step\n",
    "        return output\n",
    "\n",
    "# model = GRUModel(input_size, hidden_size, output_size, dropout_prob)\n",
    "\n",
    "# =======================\n",
    "# Define the Convolutional GRU model\n",
    "class ConvGRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
    "        super(ConvGRUModel, self).__init__()\n",
    "        self.convgru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.convgru(x)\n",
    "        output = self.fc(gru_out[:, -1, :])  # Take the output from the last time step\n",
    "        return output\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        # Reset parameters of the GRU layer\n",
    "        for name, param in self.convgru.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "        # Reset parameters of the fully connected layer\n",
    "        self.fc.reset_parameters()\n",
    "\n",
    "model = ConvGRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "#=============================\n",
    "# Define the Convolutional GRU model with Tanh activation at the output\n",
    "class ConvGRUModelTanh(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
    "        super(ConvGRUModelTanh, self).__init__()\n",
    "        self.convgru = nn.GRU(input_size=input_size, hidden_size=hidden_size, dropout=dropout_prob, num_layers=num_layers, batch_first=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.convgru(x)\n",
    "        output = self.fc(gru_out[:, -1, :])  # Take the output from the last time step\n",
    "        output = self.tanh(output)  # Apply Tanh activation\n",
    "        return output\n",
    "\n",
    "#model = ConvGRUModelTanh(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "#=============================\n",
    "# Define the Convolutional BLSTM model with dropout\n",
    "class ConvBLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob=0.2):\n",
    "        super(ConvBLSTMModel, self).__init__()\n",
    "        self.convblstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                                 batch_first=True, bidirectional=True, dropout=dropout_prob)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # Multiply by 2 for bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        blstm_out, _ = self.convblstm(x)\n",
    "        output = self.dropout(blstm_out[:, -1, :])  # Apply dropout before the fully connected layer\n",
    "        output = self.fc(output)  # Take the output from the last time step\n",
    "        return output\n",
    "\n",
    "# model = ConvBLSTMModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "#=============================\n",
    "\n",
    "# Move the selected model to the GPU\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911b020",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "#### Must choose arousal or valence here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0583a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = TensorDataset(X_train, y_train.unsqueeze(1))\n",
    "#train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "import csv\n",
    "\n",
    "def Model_Train(X_train, X_test, y_train, y_test, y_subject, batch_size, num_epochs, model): \n",
    "        \n",
    "    model.reset_parameters()\n",
    "    \n",
    "    # Move the selected model to the GPU\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Initialize a list to store the training loss values\n",
    "    train_loss_values      = []\n",
    "    validation_loss_values = []\n",
    "    \n",
    "    # Initialize a list to store the best training epoch and the best validation loss\n",
    "    best_validation_loss     = float('inf')\n",
    "    early_stop_counter       = 0\n",
    "    \n",
    "    # Aroural\n",
    "    # best_model_path          = 'best_model_recola_arousal_loso.pth'  # Define the path to save the best model\n",
    "    # best_model_epochs        = 'best_model_recola_arousal_loso_epochs.csv'  # Define the path to save the best model\n",
    "    \n",
    "    # Valence\n",
    "    best_model_path          = 'best_model_recola_valence_loso.pth'  # Define the path to save the best model\n",
    "    best_model_epochs        = 'best_model_recola_valence_loso_epochs.csv'  # Define the path to save the best model\n",
    "\n",
    "    #criterion = nn.MSELoss()\n",
    "    criterion = CCCLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    #train_dataset = TensorDataset(X_train, y_train.unsqueeze(1))\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "    train_loader  = DataLoader(train_dataset, batch_size = batch_size, shuffle=False)\n",
    "    \n",
    "    epoch_best = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "    \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X.to(device))\n",
    "            #loss    = criterion(outputs, batch_y.to(device))\n",
    "            loss    = criterion(outputs, batch_y.unsqueeze(1).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # print every 10 epochs only\n",
    "        #if epoch % 10 == 0:\n",
    "        \n",
    "        print(f'Training epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "        average_epoch_loss = epoch_loss / len(train_loader)\n",
    "        train_loss_values.append(average_epoch_loss)\n",
    "    \n",
    "        # Validate the model every validate_every epochs using the test partition\n",
    "        if epoch % validate_every == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_outputs = model(X_test.to(device))\n",
    "                validation_loss = criterion(test_outputs, y_test.unsqueeze(1).to(device))  # Adjust target size\n",
    "\n",
    "            validation_loss_values.append(validation_loss.item())\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {validation_loss.item():.4f}')\n",
    "        \n",
    "            if validation_loss < best_validation_loss:\n",
    "                best_validation_loss = validation_loss\n",
    "                early_stop_counter = 0\n",
    "            \n",
    "                # Save the model with the best validation loss\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f'Saved model with best validation loss to {best_model_path}')\n",
    "                epoch_best = epoch       \n",
    "                \n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1} as validation loss has not improved for {patience} consecutive validations.')\n",
    "                print(f'Best training epoch {epoch_best} at validation.')\n",
    "                break\n",
    "            \n",
    "            model.train()  # Set the model back to training mode\n",
    "            \n",
    "    # write to file subject / best epoch train        \n",
    "    df_temp = pd.DataFrame( [ [ y_subject, epoch_best, best_validation_loss.cpu() ] ] )\n",
    "    df_temp.to_csv(best_model_epochs, mode='a', header=False )        \n",
    "            \n",
    "    Plot_Train_Val_Curvs(train_loss_values, validation_loss_values, validate_every)         \n",
    "            \n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    model.to('cpu')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92af1d2",
   "metadata": {},
   "source": [
    "### Evaluate the trained model, selecting the best validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# Load the best model for testing\n",
    "#best_model = ConvGRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "def Model_Test_Best(X_test, y_test, best_model): \n",
    "    \n",
    "    criterion = CCCLoss()\n",
    "    \n",
    "    #best_model = ConvBLSTMModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "    #best_model.load_state_dict(torch.load(best_model_path))\n",
    "    best_model = best_model.to(device)\n",
    "\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = best_model(X_test.to(device))\n",
    "        test_loss    = criterion(test_outputs, y_test.unsqueeze(1).to(device))\n",
    "\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "    \n",
    "    return test_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763658a0",
   "metadata": {},
   "source": [
    "### Plot training and validation CCC loss X epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab1b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Plot_Train_Val_Curvs(train_loss_values, validation_loss_values, validate_every):\n",
    "    # Plot the training and validation loss values\n",
    "    epochs = range(1, len(train_loss_values) + 1)\n",
    "    plt.plot(epochs, train_loss_values, label='Training Loss')\n",
    "    plt.plot(range(0, len(validation_loss_values) * validate_every, validate_every), validation_loss_values, label='Validation Loss', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b9d74",
   "metadata": {},
   "source": [
    "### Define LOSO Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8428b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Create model and grouping object\n",
    "best_model = model \n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "def experiment(\n",
    "    feat,\n",
    "    targ,\n",
    "    groups,\n",
    "):        \n",
    "    truths = []\n",
    "    preds  = []\n",
    "\n",
    "    for train_index, test_index in logo.split(\n",
    "        feat, \n",
    "        targ, \n",
    "        groups=groups,\n",
    "    ):\n",
    "        train_x = feat.iloc[train_index]\n",
    "        train_y = targ[train_index]\n",
    "\n",
    "        test_x = feat.iloc[test_index]\n",
    "        test_y = targ[test_index]\n",
    "        \n",
    "        print(str(train_index) + \" \" + str(test_index)) \n",
    "        print(\"Validation subject: \" + str(groups[test_index[0]])) \n",
    "        \n",
    "        # Convert to pytorch\n",
    "        \n",
    "        ### TRAIN\n",
    "        \n",
    "        features = train_x.values.astype(np.float32)\n",
    "        labels   = train_y.values.astype(np.float32)\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        features_tensor = torch.from_numpy(features)\n",
    "        labels_tensor   = torch.from_numpy(labels)\n",
    "\n",
    "        # Assuming you want a sequence length of 1\n",
    "        # features_tensor = features_tensor.unsqueeze(1)\n",
    "\n",
    "        ######\n",
    "        # Reshape features tensor with sequence length of 50\n",
    "        sequence_length = 1\n",
    "        num_features    = features.shape[1]\n",
    "        num_samples     = features.shape[0]\n",
    "\n",
    "        # Calculate the number of sequences that can be formed\n",
    "        num_sequences = num_samples // sequence_length\n",
    "\n",
    "        # Truncate the tensor to fit the full sequences\n",
    "        features_tensor = features_tensor[:num_sequences * sequence_length, :]\n",
    "        labels_tensor = labels_tensor[:num_sequences * sequence_length]\n",
    "\n",
    "        # Reshape the tensor\n",
    "        features_tensor = features_tensor.view(num_sequences, sequence_length, num_features)\n",
    "\n",
    "        ### DEVEL\n",
    "\n",
    "        features2 = test_x.values.astype(np.float32)\n",
    "        labels2   = test_y.values.astype(np.float32)\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        features_tensor2 = torch.from_numpy(features2)\n",
    "        labels_tensor2   = torch.from_numpy(labels2)\n",
    "\n",
    "        ######\n",
    "        # Reshape features tensor with sequence length of 50\n",
    "        sequence_length = 1\n",
    "        num_features = features2.shape[1]\n",
    "        num_samples  = features2.shape[0]\n",
    "\n",
    "        # Calculate the number of sequences that can be formed\n",
    "        num_sequences = num_samples // sequence_length\n",
    "\n",
    "        # Truncate the tensor to fit the full sequences\n",
    "        features_tensor2 = features_tensor2[:num_sequences * sequence_length, :]\n",
    "        labels_tensor2   = labels_tensor2[:num_sequences * sequence_length]\n",
    "\n",
    "        # Reshape the tensor\n",
    "        features_tensor2 = features_tensor2.view(num_sequences, sequence_length, num_features)\n",
    "        ######\n",
    "        \n",
    "        # Train model\n",
    "        \n",
    "        subject = str(groups[test_index[0]])\n",
    "        \n",
    "        best_model = Model_Train(features_tensor, features_tensor2, labels_tensor, labels_tensor2, subject, batch_size, num_epochs, model)\n",
    "        \n",
    "        print(\"This is the best model\" + str(best_model))\n",
    "        \n",
    "        # Test model\n",
    "        predict_y =  Model_Test_Best(features_tensor2, labels_tensor2, best_model)\n",
    "        \n",
    "        truths.append(test_y)\n",
    "        preds.append(predict_y.cpu().squeeze(1))\n",
    "        \n",
    "    # combine subject folds\n",
    "    truth = pd.concat(truths)\n",
    "    truth.name = 'truth'\n",
    "    \n",
    "    \n",
    "    \n",
    "    pred = pd.Series(\n",
    "        np.concatenate(preds),\n",
    "        index=truth.index,\n",
    "        name='prediction',\n",
    "    )\n",
    "    \n",
    "    return truth, pred"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c78150c8",
   "metadata": {},
   "source": [
    "feat = df_train_feat.iloc[:, 1:770]\n",
    "targ = df_train_valence_lab['valence']\n",
    "groups = df_train_valence_lab['Subject']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2bf68870",
   "metadata": {},
   "source": [
    "    truths = []\n",
    "    preds  = []\n",
    "\n",
    "    for train_index, test_index in logo.split(\n",
    "        feat, \n",
    "        targ, \n",
    "        groups=groups,\n",
    "    ):\n",
    "        train_x = feat.iloc[train_index]\n",
    "        train_y = targ[train_index]\n",
    "\n",
    "        test_x = feat.iloc[test_index]\n",
    "        test_y = targ[test_index]\n",
    "        \n",
    "        # print(str(train_index) + \" \" + str(test_index)) \n",
    "        print(\"Validation subject: \" + str(groups[test_index[0]])) \n",
    "        \n",
    "        # Convert to pytorch\n",
    "        \n",
    "        ### TRAIN\n",
    "        \n",
    "        features = train_x.values.astype(np.float32)\n",
    "        labels   = train_y.values.astype(np.float32)\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        features_tensor = torch.from_numpy(features)\n",
    "        labels_tensor   = torch.from_numpy(labels)\n",
    "\n",
    "        # Assuming you want a sequence length of 1\n",
    "        # features_tensor = features_tensor.unsqueeze(1)\n",
    "\n",
    "        ######\n",
    "        # Reshape features tensor with sequence length of 50\n",
    "        sequence_length = 1\n",
    "        num_features    = features.shape[1]\n",
    "        num_samples     = features.shape[0]\n",
    "\n",
    "        # Calculate the number of sequences that can be formed\n",
    "        num_sequences = num_samples // sequence_length\n",
    "\n",
    "        # Truncate the tensor to fit the full sequences\n",
    "        features_tensor = features_tensor[:num_sequences * sequence_length, :]\n",
    "        labels_tensor = labels_tensor[:num_sequences * sequence_length]\n",
    "\n",
    "        # Reshape the tensor\n",
    "        features_tensor = features_tensor.view(num_sequences, sequence_length, num_features)\n",
    "\n",
    "        ### VALIDATION\n",
    "\n",
    "        features2 = test_x.values.astype(np.float32)\n",
    "        labels2   = test_y.values.astype(np.float32)\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        features_tensor2 = torch.from_numpy(features2)\n",
    "        labels_tensor2   = torch.from_numpy(labels2)\n",
    "\n",
    "        ######\n",
    "        # Reshape features tensor with sequence length of 50\n",
    "        sequence_length = 1\n",
    "        num_features = features2.shape[1]\n",
    "        num_samples  = features2.shape[0]\n",
    "\n",
    "        # Calculate the number of sequences that can be formed\n",
    "        num_sequences = num_samples // sequence_length\n",
    "\n",
    "        # Truncate the tensor to fit the full sequences\n",
    "        features_tensor2 = features_tensor2[:num_sequences * sequence_length, :]\n",
    "        labels_tensor2   = labels_tensor2[:num_sequences * sequence_length]\n",
    "\n",
    "        # Reshape the tensor\n",
    "        features_tensor2 = features_tensor2.view(num_sequences, sequence_length, num_features)\n",
    "        ######\n",
    "        \n",
    "        # Train model\n",
    "        \n",
    "        best_model = Model_Train(features_tensor, features_tensor2, labels_tensor, labels_tensor2, batch_size, num_epochs, model)\n",
    "        \n",
    "        print(\"This is the best model\" + str(best_model))\n",
    "        \n",
    "        # Test model\n",
    "        predict_y =  Model_Test_Best(features_tensor2, labels_tensor2, best_model)\n",
    "        \n",
    "        truths.append(test_y)\n",
    "        preds.append(predict_y.cpu().squeeze(1))\n",
    "        \n",
    "        \n",
    "    # combine subject folds\n",
    "    truth = pd.concat(truths)\n",
    "    truth.name = 'truth'\n",
    "    \n",
    "    \n",
    "    \n",
    "    pred = pd.Series( np.concatenate(preds),\n",
    "        index=truth.index,\n",
    "        name='prediction')\n",
    "    \n",
    "    \n",
    "    return truth, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206bebf",
   "metadata": {},
   "source": [
    "### Select Arousal or Valence"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ed60a1c",
   "metadata": {},
   "source": [
    "# Arousal\n",
    "\n",
    "truth_wavlm, pred_wavlm = experiment(\n",
    "    df_train_feat.iloc[:, 1:770],\n",
    "    df_train_arousal_lab['arousal'],\n",
    "    df_train_arousal_lab['Subject'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1fa969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valence\n",
    "\n",
    "truth_wavlm, pred_wavlm = experiment(\n",
    "    df_train_feat.iloc[:, 1:770],\n",
    "    df_train_valence_lab['valence'],\n",
    "    df_train_valence_lab['Subject'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import audmetric\n",
    "\n",
    "audmetric.concordance_cc(truth_wavlm, pred_wavlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3ef173",
   "metadata": {},
   "source": [
    "### Load devel dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b13114",
   "metadata": {},
   "source": [
    "#### Must select arousal or valence here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a7b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = df_devel_feat.iloc[:, 1:770].values.astype(np.float32)\n",
    "#features2 = df_devel_feat.values.astype(np.float32)\n",
    "\n",
    "\n",
    "# Arousal\n",
    "# labels2   = df_devel_arousal_lab['arousal'].values.astype(np.float32)\n",
    "\n",
    "# Valence\n",
    "labels2   = df_devel_valence_lab['valence'].values.astype(np.float32)\n",
    "\n",
    "# Normalize the features between -1 and 1 (adjust scaling based on your data)\n",
    "# features2 = (features - np.min(features)) / (np.max(features) - np.min(features)) * 2 - 1\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "features_tensor2 = torch.from_numpy(features2)\n",
    "labels_tensor2   = torch.from_numpy(labels2)\n",
    "\n",
    "######\n",
    "# Reshape features tensor with sequence length of 50\n",
    "sequence_length = 1\n",
    "num_features = features2.shape[1]\n",
    "num_samples  = features2.shape[0]\n",
    "\n",
    "# Calculate the number of sequences that can be formed\n",
    "num_sequences = num_samples // sequence_length\n",
    "\n",
    "# Truncate the tensor to fit the full sequences\n",
    "features_tensor2 = features_tensor2[:num_sequences * sequence_length, :]\n",
    "labels_tensor2   = labels_tensor2[:num_sequences * sequence_length]\n",
    "\n",
    "# Reshape the tensor\n",
    "features_tensor2 = features_tensor2.view(num_sequences, sequence_length, num_features)\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207694e1",
   "metadata": {},
   "source": [
    "### Load best model and predict\n",
    "\n",
    "#### Must select arousal or valence here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d3b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model for testing\n",
    "\n",
    "# Arousal\n",
    "# best_model_path = 'best_model_recola_arousal_loso.pth'  # Define the path to save the best model\n",
    "\n",
    "# Valence\n",
    "best_model_path = 'best_model_recola_valence_loso.pth'  # Define the path to save the best model\n",
    "\n",
    "criterion = CCCLoss()\n",
    "\n",
    "###############################################################################################\n",
    "best_model = ConvGRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "# best_model = ConvBLSTMModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "###############################################################################################\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "best_model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_outputs = best_model(features_tensor2.to(device))\n",
    "    test_loss    = criterion(test_outputs, labels_tensor2.unsqueeze(1).to(device))\n",
    "\n",
    "print(f'Test Loss: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab0d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = test_outputs.cpu().squeeze(1)\n",
    "truth = labels_tensor2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37240539",
   "metadata": {},
   "source": [
    "### Arousal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ba994",
   "metadata": {},
   "source": [
    "#### Smooth function to smooth predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b7dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "def F_Smooth(prediction, win_width):\n",
    "\n",
    "    df_pred = pd.DataFrame(prediction)\n",
    "    width = win_width\n",
    "    lag1 = df_pred.shift(1)\n",
    "    lag3 = df_pred.shift(width - 1)\n",
    "    window = lag3.rolling(window=width)\n",
    "    means = window.mean()\n",
    "    df_smoothed = concat([means, lag1, df_pred], axis=1)\n",
    "    df_smoothed.columns = ['mean', 't-1', 't+1']\n",
    "    df_smoothed['mean'] = df_smoothed['mean'].fillna(df_smoothed['t+1'])\n",
    "    \n",
    "    return df_smoothed['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117f67aa",
   "metadata": {},
   "source": [
    "#### Predict on Devel set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4462382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import audmetric\n",
    "\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, pred)))\n",
    "print(\"MSE = \" + str(audmetric.mean_absolute_error(truth, pred)))\n",
    "print(\"MAE = \" + str(audmetric.mean_squared_error(truth, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba17125",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 5)))  + \"  ( 5) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 10))) + \"  (10) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 15))) + \"  (15) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 20))) + \"  (20) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 25))) + \"  (25) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 30))) + \"  (30) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 35))) + \"  (35) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 40))) + \"  (40) \")\n",
    "\n",
    "print(\"MSE = \" + str(audmetric.mean_absolute_error(truth, F_Smooth(pred, 20)))+ \" (20)\")\n",
    "print(\"MAE = \" + str(audmetric.mean_squared_error(truth, F_Smooth(pred, 20)))+ \" (20)\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "785242d4",
   "metadata": {},
   "source": [
    "##### Best results so far Arousal Devel with LOSO at training:\n",
    "\n",
    "Test Loss: 0.2893\n",
    "/DEV/WavLM/Recola2018_16k/models/best_model_recola_arousal_loso_03032024.pth\n",
    "/DEV/WavLM/Recola2018_16k/models/best_model_recola_arousal_loso_epochs_03032024.csv\n",
    "\n",
    "CCC = 0.7107511144810469\n",
    "\n",
    "MSE = 0.11390312016010284\n",
    "MAE = 0.02063915506005287\n",
    "\n",
    "With smooth\n",
    "CCC = 0.7486076793101336  ( 5) \n",
    "CCC = 0.7687387950728284  (10) \n",
    "CCC = 0.7789464628838636  (15) \n",
    "CCC = 0.7829851476119157  (20) \n",
    "CCC = 0.7825649170753243  (25) \n",
    "CCC = 0.7789514500423856  (30) \n",
    "CCC = 0.7727902909048278  (35) \n",
    "CCC = 0.7648225374016705  (40)\n",
    "\n",
    "MSE = 0.09611338156505336 (20)\n",
    "MAE = 0.014646904760840154 (20)\n",
    "\n",
    "best_model = ConvGRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size   = num_features\n",
    "hidden_size  = 32 #128, 64, 32, 16\n",
    "num_layers   = 6\n",
    "output_size  = 1  # Single output for regression between -1 and +1\n",
    "dropout_prob = 0.20 \n",
    "\n",
    "# Train the model\n",
    "num_epochs     = 1000\n",
    "batch_size     = 7501\n",
    "validate_every = 1  # Validate every 2 epochs\n",
    "patience       = 15  # Stop training if validation loss doesn't improve for 5 consecutive validations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f63e3e",
   "metadata": {},
   "source": [
    "### Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import audmetric\n",
    "\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, pred)))\n",
    "print(\"MSE = \" + str(audmetric.mean_absolute_error(truth, pred)))\n",
    "print(\"MAE = \" + str(audmetric.mean_squared_error(truth, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7aa70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 5))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 10))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 15))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 20))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 25))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 30))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 35))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 40))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 45))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 50))))\n",
    "\n",
    "print(\"MSE = \" + str(audmetric.mean_absolute_error(truth, F_Smooth(pred, 25))))\n",
    "print(\"MAE = \" + str(audmetric.mean_squared_error(truth, F_Smooth(pred, 25))))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1b5d3f8",
   "metadata": {},
   "source": [
    "##### Best results so far Valence Devel:\n",
    "\n",
    "Test Loss: 0.6367\n",
    "/DEV/WavLM/Recola2018_16k/models/best_model_recola_valence_loso_03032024.pth\n",
    "/DEV/WavLM/Recola2018_16k/models/best_model_recola_valence_loso_epochs_03032024.csv\n",
    "\n",
    "CCC = 0.35387330344214085\n",
    "\n",
    "MSE = 0.10845673829317093\n",
    "MAE = 0.01992058753967285\n",
    "\n",
    "With Smooth\n",
    "CCC = 0.4158365728866413\n",
    "CCC = 0.4485323174502916\n",
    "CCC = 0.4689329307149778\n",
    "CCC = 0.48165148669266095\n",
    "CCC = 0.4888527510582969\n",
    "CCC = 0.4921716187434845\n",
    "CCC = 0.4926799810218907\n",
    "CCC = 0.49087949801229924\n",
    "CCC = 0.48723823316745624\n",
    "CCC = 0.4821973971633789\n",
    "\n",
    "MSE = 0.08750272508878654\n",
    "MAE = 0.012485581522185363\n",
    "\n",
    "best_model = ConvGRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size   = num_features\n",
    "hidden_size  = 32 #128, 64, 32, 16\n",
    "num_layers   = 5\n",
    "output_size  = 1  # Single output for regression between -1 and +1\n",
    "dropout_prob = 0.25 \n",
    "\n",
    "# Train the model\n",
    "num_epochs     = 1000\n",
    "batch_size     = 7501\n",
    "validate_every = 1  # Validate every 2 epochs\n",
    "patience       = 15  # Stop training if validation loss doesn't improve for 5 consecutive validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac44fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "predict_y = F_Smooth(pred, 5)\n",
    "\n",
    "dt = 1\n",
    "t = np.arange(0, len(predict_y), dt)\n",
    "s1 = df_devel_arousal_lab['arousal']\n",
    "s2 = predict_y                # white noise 2\n",
    "\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "axs[0].plot(t, s1, t, s2)\n",
    "axs[0].set_xlim(0, len(predict_y))\n",
    "axs[0].set_xlabel('time')\n",
    "axs[0].set_ylabel('s1 and s2')\n",
    "axs[0].grid(True)\n",
    "\n",
    "cxy, f = axs[1].cohere(s1, s2, 256, 1. / dt)\n",
    "axs[1].set_ylabel('coherence')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820cf17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "plt.figure(figsize=(200,6))\n",
    "#fig.tight_layout()\n",
    "\n",
    "predict_y = F_Smooth(pred, 20)\n",
    "\n",
    "dt = 1\n",
    "t = np.arange(0, len(predict_y), dt)\n",
    "s1 = df_devel_valence_lab['valence']\n",
    "s2 = predict_y               # white noise 2\n",
    "\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "axs[0].plot(t, s2, t, s1)\n",
    "axs[0].set_xlim(0, len(predict_y))\n",
    "axs[0].set_xlabel('time')\n",
    "axs[0].set_ylabel('s1 and s2')\n",
    "axs[0].grid(True)\n",
    "\n",
    "cxy, f = axs[1].cohere(s1, s2, 256, 1. / dt)\n",
    "axs[1].set_ylabel('coherence')\n",
    "\n",
    "#plt.figure(figsize=(20,6))\n",
    "#fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa1d665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f0378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436aea6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
