{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc547b1-84d4-4ba6-a8a1-8877423fe056",
   "metadata": {},
   "source": [
    "# WavLM Feature Extractor\n",
    "\n",
    "### for AffWild - 6th ABAW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760109a1-c818-49cf-abd3-377b65994a33",
   "metadata": {},
   "source": [
    "##### https://github.com/microsoft/unilm/tree/master/wavlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a2a1c8e-b2fd-416a-8b2d-73a3728923a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run WavLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8263d9b9-de2b-418e-b1a5-da5323499a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37b7839d-4542-4c68-ba35-35d34d64483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:2\n",
      "NVIDIA A100-SXM4-40GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/etsmtl/akoerich/anaconda3/envs/wavlm/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WavLM(\n",
       "  (feature_extractor): ConvFeatureExtractionModel(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        (3): GELU(approximate='none')\n",
       "      )\n",
       "      (1-4): 4 x Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "      (5-6): 2 x Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)\n",
       "  (dropout_input): Dropout(p=0.1, inplace=False)\n",
       "  (dropout_features): Dropout(p=0.1, inplace=False)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (pos_conv): Sequential(\n",
       "      (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "      (1): SamePad()\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (relative_attention_bias): Embedding(320, 12)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1-11): 11 x TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from WavLM import WavLM, WavLMConfig\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# load the pre-trained checkpoints\n",
    "checkpoint = torch.load('model/WavLM-Base+.pt')\n",
    "cfg = WavLMConfig(checkpoint['cfg'])\n",
    "model = WavLM(cfg)\n",
    "#model = model.to(device) #, dtype=torch.float32)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99264fb6-920e-4302-bdec-a8f391af8ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to source files\n",
    "path2 = '/home/etsmtl/akoerich/DEV/Affwild/Aff-Wild2-train-batch1-wav'\n",
    "#path2 = '/home/etsmtl/akoerich/DEV/Affwild/Aff-Wild2-validation-batch2-wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5009cede-ddf6-4e26-be23-bbba55a00573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Read a directory and put all files in a list\n",
    "file_list = []\n",
    "i = 0 \n",
    "for path, subdirs, files in os.walk( path2 ):\n",
    "    for name in files:\n",
    "        file_list.append( os.path.join( path, name) )\n",
    "        i += 1\n",
    "print(\"Files processed: \"+str(i) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0feee48-fbad-405e-8791-6811e58105dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "# sample rate = 16,000\n",
    "#  1s = 16,000 x 1 =  16,000\n",
    "# 60s = 16,000 x 60 = 960,500\n",
    "\n",
    "length = list()\n",
    "i      = 0\n",
    "avg    = 0\n",
    "\n",
    "for file in file_list:\n",
    "    data, samplerate = sf.read( file )\n",
    "    \n",
    "    if len(data) <= 960500:                         \n",
    "            print(\"Audio length: \"+str(len(data))+\" with less than 60 s: \"+str(file) )\n",
    "\n",
    "    if len(data) >= 64000000:                         \n",
    "            print(\"Audio length: \"+str(len(data))+\" higher than 7.5 min: \"+str(file) )\n",
    "\n",
    "    \n",
    "    #computer average lenght of files\n",
    "    avg = avg + len(data)\n",
    "    length.append(len(data))\n",
    "    i += 1\n",
    "\n",
    "print( \"Files processed: \"+str(i) )\n",
    "print( \"Average file length: \"+str(avg/i) + \" samples   \"+str(avg/i/samplerate)+\" s   \"+str(avg/i/samplerate/60)+\" min\" )\n",
    "print( \"Max length: \"+str(max(length))+ \" samples   \"+str(max(length)/samplerate)+\" s   \"+str(max(length)/samplerate/60)+\" min\" )\n",
    "print( \"Min length: \"+str(min(length))+ \" samples   \"+str(min(length)/samplerate)+\" s   \"+str(min(length)/samplerate/60)+\" min\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62bde6a-7e5d-4052-b21c-38723e7746f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 16000\n",
    "track_count = 0\n",
    "\n",
    "for file in file_list:\n",
    "    data , samplerate = sf.read( file )\n",
    "    print (\"--------------\")\n",
    "    print (\"Sample Rate: \" + str(samplerate) + \" Length: \" + str(data.shape) + \" \" + \"Time: \" + str(data.shape[0]/samplerate) + \" sec \" + str( file ) )\n",
    "\n",
    "    file_id = 'features/train/'+file.split('/')[6].split('.')[0]+'.wavlm' \n",
    "    \n",
    "    if not os.path.exists(file_id):     \n",
    "    \n",
    "        # extract the representation of last layer\n",
    "        wav_input = torch.from_numpy(data).float()\n",
    "        wav_input_16khz = torch.unsqueeze(wav_input,0)\n",
    "\n",
    "        del wav_input\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # wav_input_16khz = torch.randn(1,48000)\n",
    "        # wav_input_16khz = torch.randn(1,2786987)\n",
    "        if cfg.normalize:\n",
    "            wav_input_16khz = torch.nn.functional.layer_norm(wav_input_16khz , wav_input_16khz.shape)\n",
    "    \n",
    "        #rep = model.extract_features(wav_input_16khz.to(device, dtype=torch.float32))[0]\n",
    "        rep = model.extract_features(wav_input_16khz)[0]\n",
    "\n",
    "        del wav_input_16khz\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        rep[0].shape\n",
    "        rep_np = rep[0].detach().numpy()\n",
    "        rep_df = pd.DataFrame(rep_np)\n",
    "\n",
    "        del rep_np\n",
    "\n",
    "        file_id = 'features/valid/'+file.split('/')[6].split('.')[0]+'.wavlm'  \n",
    "        rep_df.to_csv(file_id)\n",
    "\n",
    "        del rep_df\n",
    "        \n",
    "        # Pooling \n",
    "        df_temp = pd.DataFrame(rep[0].detach().numpy())\n",
    "        df_pool = df_temp.rolling(2, step=2).mean().drop(index=0) \n",
    "\n",
    "        del df_temp, rep\n",
    "        \n",
    "        file_id = 'features/valid/'+file.split('/')[6].split('.')[0]+'.pool.wavlm'  \n",
    "        df_pool.to_csv(file_id)\n",
    "\n",
    "        del df_pool\n",
    "        \n",
    "        print( file )\n",
    "\n",
    "    else:\n",
    "        print(\"Already exists: \" + str(file_id))\n",
    "    \n",
    "    track_count += 1\n",
    "\n",
    "    # Release memory using gc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d4fd7-6fc6-42c5-bc81-e42d7c071bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.split('/')[6].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da1f3a0-76c8-460c-8393-a372c8464902",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.split('/')[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd971176-9b95-4293-b079-8f49ce71fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape[0]/samplerate/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79655959",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba7794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4794d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d3a438c",
   "metadata": {},
   "source": [
    "### Process Feature Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f108093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to feature files\n",
    "path_train = 'features/train'\n",
    "extension = 'wavlmbasefeatpool'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67436984",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = [file for file in os.listdir(path_train) if file.endswith(extension)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4c6425",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_train_files = sorted(train_files)\n",
    "sorted_train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577acc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file in sorted_train_files:\n",
    "    df = pd.read_csv(os.path.join(path_train, file))\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec2660",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074617de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60744de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat.drop(df_feat.columns[[0]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472c362c",
   "metadata": {},
   "source": [
    "### Process Label files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3528871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to label files\n",
    "path_train_labels = 'SEWA16/labels/Train/'\n",
    "extension = 'csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f5b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files_labels = [file for file in os.listdir(path_train_labels) if file.endswith(extension)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fbf416",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_train_labels = sorted(train_files_labels)\n",
    "sorted_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94feb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = []\n",
    "for file in sorted_train_labels:\n",
    "    df2 = pd.read_csv(os.path.join(path_train_labels, file), sep=\";\")\n",
    "    df2.drop(df2.columns[[0,1]], axis=1, inplace=True)\n",
    "    dfl.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaaaf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe82ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab = pd.concat(dfl, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a43d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30534e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e204f607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab.drop(df_lab.columns[[0,1,4]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e31af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ca41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4ba2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842464e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf883d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classifier and grouping object\n",
    "clf = make_pipeline(\n",
    "    StandardScaler(), \n",
    "    SVC(gamma='auto'),\n",
    ")\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "def experiment(\n",
    "    features,\n",
    "    targets,\n",
    "    groups,\n",
    "):        \n",
    "    truths = []\n",
    "    preds = []\n",
    "    \n",
    "    # leave-one-speaker loop    \n",
    "    pbar = audeer.progress_bar(\n",
    "        total=len(groups.unique()),\n",
    "        desc='Run experiment',\n",
    "    )\n",
    "    for train_index, test_index in logo.split(\n",
    "        features, \n",
    "        targets, \n",
    "        groups=groups,\n",
    "    ):\n",
    "        train_x = features.iloc[train_index]\n",
    "        train_y = targets[train_index]\n",
    "        clf.fit(train_x, train_y)\n",
    "        \n",
    "        truth_x = features.iloc[test_index]\n",
    "        truth_y = targets[test_index]\n",
    "        predict_y = clf.predict(truth_x)\n",
    "        \n",
    "        truths.append(truth_y)\n",
    "        preds.append(predict_y)\n",
    "        \n",
    "        pbar.update()\n",
    "        \n",
    "    # combine speaker folds\n",
    "    truth = pd.concat(truths)\n",
    "    truth.name = 'truth'\n",
    "    pred = pd.Series(\n",
    "        np.concatenate(preds),\n",
    "        index=truth.index,\n",
    "        name='prediction',\n",
    "    )\n",
    "    \n",
    "    return truth, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356040c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_w2v2, pred_w2v2 = experiment(\n",
    "    df,\n",
    "    emotion,\n",
    "    speaker,\n",
    ")\n",
    "audformat.utils.concat([truth_w2v2, pred_w2v2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4115fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c85d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df5dab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7a4e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
