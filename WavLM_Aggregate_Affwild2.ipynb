{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f42e82b-46c1-48bc-a9bd-eda21ca7a3c1",
   "metadata": {},
   "source": [
    "\n",
    "# Aggregate WavLM features to adapt to label files\n",
    "\n",
    "### For AffWild2 Train and Devel dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760109a1-c818-49cf-abd3-377b65994a33",
   "metadata": {},
   "source": [
    "##### https://github.com/microsoft/unilm/tree/master/wavlm\n",
    "##### https://github.com/audeering/w2v2-how-to/blob/main/notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8263d9b9-de2b-418e-b1a5-da5323499a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814ae24",
   "metadata": {},
   "source": [
    "### Process Feature Files - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98fa97-9848-495d-b793-ba9687c3dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191da55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to feature files\n",
    "path_train_wavs = 'features/test'\n",
    "#path_train_wavs = 'features/valid'\n",
    "\n",
    "extension_wav = 'wavlm'\n",
    "\n",
    "train_wavs = [file for file in os.listdir(path_train_wavs) if file.endswith(extension_wav)]\n",
    "\n",
    "sorted_train_wavs = sorted(train_wavs)\n",
    "sorted_train_wavs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aad4e9-9b3a-434a-b67b-f2b3380a8179",
   "metadata": {},
   "source": [
    "### Process Label Files - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca138a-b62d-4acf-9241-62894e5e91e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to feature files\n",
    "#path_train_labels = '../Affwild/6thABAWAnnotations/VA_Estimation_Challenge/Train_Set'\n",
    "path_train_labels = '../Affwild/6thABAWAnnotations/VA_Estimation_Challenge/Validation_Set'\n",
    "\n",
    "extension_lab = 'txt'\n",
    "\n",
    "train_labels = [file for file in os.listdir(path_train_labels) if file.endswith(extension_lab)]\n",
    "\n",
    "sorted_train_labels = sorted(train_labels)\n",
    "sorted_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66545e80-7bec-40ac-abf3-16951e80766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928f3079-ec46-46bf-8ff2-c43d76373270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove file extensions\n",
    "def remove_extension(filename):\n",
    "    return os.path.splitext(filename)[0]\n",
    "\n",
    "# Remove extensions from both lists\n",
    "cleaned_train_wavs = [remove_extension(filename) for filename in sorted_train_wavs]\n",
    "cleaned_train_labels = [remove_extension(filename) for filename in sorted_train_labels]\n",
    "\n",
    "counter = 0\n",
    "matched_train_list = []\n",
    "\n",
    "# Check and print filenames\n",
    "for filename in cleaned_train_wavs:\n",
    "    if filename in cleaned_train_labels:\n",
    "        print(filename)\n",
    "        matched_train_list.append(filename)\n",
    "        counter = counter + 1\n",
    "    #else:\n",
    "    #    print(f\"{filename} not found in labels\")\n",
    "\n",
    "print( str(counter) + \" matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3868f663-8c31-4d1b-8043-1899ba62fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_df = pd.DataFrame(matched_train_list)\n",
    "matched_df.to_csv('matched_new_valid_fold.csv', index=False, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c61364-0795-4eec-83fc-f84b46450533",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_labels+matched_train_list[0]+'.'+extension_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c8c914-a496-47e3-8c05-f7d89695567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfe7647-9a45-4f26-87d8-127eec0c2a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Placeholder lists (replace with your actual data)\n",
    "features_path = \"features/new_val\"  # Path to the features folder\n",
    "labels_path = \"../Affwild/6thABAWAnnotations/VA_Estimation_Challenge/Validation_Set\"  # Path to the labels folder\n",
    "#labels_path = \"../Affwild/6thABAWAnnotations/VA_Estimation_Challenge/Train_Set\"  # Path to the labels folder\n",
    "output_path = \"features/new_val_pooled\"\n",
    "\n",
    "# Function to add extension to filenames\n",
    "def add_extension(filename):\n",
    "    return f\"{filename}.wavlm\"\n",
    "\n",
    "def add_extension2(filename):\n",
    "    return f\"{filename}.txt\"\n",
    "\n",
    "def add_extension3(filename):\n",
    "    return f\"{filename}.wavlmpooled\"\n",
    "\n",
    "# Iterate through the matched_train_list\n",
    "for filename in matched_train_list:\n",
    "    # Add extension to the filename\n",
    "    csv_filename  = add_extension(filename)\n",
    "    csv_filename2 = add_extension2(filename)\n",
    "    csv_filename3 = add_extension3(filename)\n",
    "\n",
    "    # Construct full paths for features and labels CSV files\n",
    "    features_file_path = os.path.join(features_path, csv_filename)\n",
    "    labels_file_path   = os.path.join(labels_path, csv_filename2)\n",
    "\n",
    "    # Check if both files exist\n",
    "    if os.path.exists(features_file_path) and os.path.exists(labels_file_path):\n",
    "        # Read data into dataframes\n",
    "        features_df = pd.read_csv(features_file_path )\n",
    "        labels_df = pd.read_csv(labels_file_path )\n",
    "\n",
    "        # Compute the length of dataframes\n",
    "        features_length = len(features_df)\n",
    "        labels_length = len(labels_df)\n",
    "\n",
    "        # Check if the feature dataframe is longer than the label dataframe\n",
    "        if features_length > labels_length:\n",
    "            #features_length\n",
    "            # Use .rolling or any other processing you want\n",
    "            # Here's an example of using .rolling with a window of 3\n",
    "            #rolling_mean = features_df.rolling(window=3).mean()\n",
    "            #print(f\"Applied rolling mean to {csv_filename} \" + \" \" + str(features_length) + \" \" + str(labels_length) + \" \" + str(len(rolling_mean) ) )\n",
    "        \n",
    "            # If the lengths are different, calculate the average of features\n",
    "            #if len(features_df) < len(labels_df):\n",
    "            ratio = features_length / labels_length\n",
    "            features_df = features_df.groupby(features_df.index // ratio).mean()\n",
    "        \n",
    "            print(f\"Applied rolling mean to {csv_filename} \" + \" \" + str(features_length) + \" \" + str(labels_length) + \" \" + str(len(features_df) ) )\n",
    "\n",
    "            features_df.to_csv(os.path.join(output_path, csv_filename3), index=False, header = None)\n",
    "            print(f\"Features saved to {csv_filename3} in {output_path}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"{csv_filename}: Feature dataframe length is not greater than label dataframe length\")\n",
    "    else:\n",
    "        print(f\"{csv_filename}: Features or labels file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052fdd9d-89aa-4770-bebf-0385b4fdd01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c378082-c27d-477f-9b53-eee74046c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec3c66-2954-4ea3-a252-cfd941184eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b014f98-6f30-44d0-a7ca-bb68e8780fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db760a-1c43-4142-9080-3b7c094696a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a81364-b258-43a1-9546-ab47ce90d0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886c6d9-46bd-40ef-b643-eb9331b301a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23410c-8e8a-49f2-b796-c60eda971f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for wavs in matched_train_list:\n",
    "\n",
    "    if os.path.exists(wavs) & os.path.exists(labs):\n",
    "\n",
    "        print(\"Processing: \" + wavs)\n",
    "\n",
    "        # Process audio signal\n",
    "        #signal, sr = sf.read(wavs) \n",
    "        #signal2    = signal.reshape(-1, 1).astype(np.float32)\n",
    "        #signal2    = min_max_scaler.fit_transform(signal2).reshape(-1)\n",
    "        #df         = pd.DataFrame(interface.process_signal(signal2, sampling_rate))\n",
    "             \n",
    "        \n",
    "        #df_arousal = df['arousal'].to_numpy()\n",
    "        #df_valence = df['valence'].to_numpy()\n",
    "        \n",
    "        #df_arousal = df[['arousal']].to_numpy()\n",
    "        #df_valence = df[['valence']].to_numpy()\n",
    "        #df_arousal = min_max_scaler.fit_transform(df_arousal).reshape(-1)\n",
    "        #df_valence = min_max_scaler.fit_transform(df_valence).reshape(-1)\n",
    "        \n",
    "        # Process ground-truth\n",
    "        #df_gt         = pd.read_csv(labs).astype(np.float32)\n",
    "        #df_arousal_gt = df_gt['arousal'].to_numpy()\n",
    "        #df_valence_gt = df_gt['valence'].to_numpy()\n",
    "\n",
    "        #if len(df_arousal_gt) > len(df_arousal):\n",
    "        #    df_arousal = np.pad(df_arousal, (0,len(df_arousal_gt)-len(df_arousal)), 'constant', constant_values=(0,1)) \n",
    "        #    df_valence = np.pad(df_valence, (0,len(df_valence_gt)-len(df_valence)), 'constant', constant_values=(0,1)) \n",
    "            \n",
    "        #if len(df_arousal_gt) < len(df_arousal):\n",
    "        #    df_arousal_gt = np.pad(df_arousal_gt, (0,len(df_arousal)-len(df_arousal_gt)), 'constant', constant_values=(0,1))   \n",
    "        #    df_valence_gt = np.pad(df_valence_gt, (0,len(df_valence)-len(df_valence_gt)), 'constant', constant_values=(0,1))   \n",
    "\n",
    "        input(\"Press Enter to continue...\")\n",
    "\n",
    "        \n",
    "        if len(df_arousal_gt) != len(df_arousal):\n",
    "            print(\"Different lengths: \" + wavs + \": \" + str(len(df_arousal_gt)) + \" and \" + str(len(df_arousal)) )\n",
    "        else:\n",
    "            print( audmetric.concordance_cc(df_arousal_gt, df_arousal), audmetric.concordance_cc(df_valence_gt, df_valence) )\n",
    "\n",
    "        i += 1\n",
    "        if (i%1 == 0.000):\n",
    "            print(\"Files read: \"+str(i) )\n",
    "\n",
    "    else: \n",
    "        if os.path.exists(wavs) == False: \n",
    "            print(\"Does not exist: \" + wavs)\n",
    "\n",
    "        if os.path.exists(labs) == False:\n",
    "            print(\"Does not exist: \" + labs )\n",
    "        else:\n",
    "            print(\"No idea\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f4d59-cf76-4f31-8add-d3a0429dfce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1393c8b9-9b5e-4426-984c-955353156186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ccae4-e18b-4d07-9137-f1740032420a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191efefe-904d-4b27-905a-103a17b37f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678490d0-7297-4dd9-9351-ae07316d2033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d09d4c-8145-491e-a6fb-8426df3e230b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97abd2cf-fdf3-4bdf-8b4d-4df9f9449f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to feature files\n",
    "path_train = 'features/valid'\n",
    "extension = 'wavlm'\n",
    "\n",
    "train_files = [file for file in os.listdir(path_train) if file.endswith(extension)]\n",
    "\n",
    "sorted_train_files = sorted(train_files)\n",
    "sorted_train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f349d9a-eb3d-4069-995f-db92de8ee13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to feature files\n",
    "path_devel = '../Affwild/6thABAWAnnotations/VA_Estimation_Challenge/Validation_Set'\n",
    "extension = 'txt'\n",
    "\n",
    "devel_files = [file for file in os.listdir(path_devel) if file.endswith(extension)]\n",
    "\n",
    "sorted_devel_files = sorted(devel_files)\n",
    "sorted_devel_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22481e4e-bad7-40e0-b0ed-87ae08153d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78443d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to feature files\n",
    "path_devel = 'Recola2018_16k/features/devel'\n",
    "extension = 'wavlmbasefeatpoolloso'\n",
    "\n",
    "devel_files = [file for file in os.listdir(path_devel) if file.endswith(extension)]\n",
    "\n",
    "sorted_devel_files = sorted(devel_files)\n",
    "sorted_devel_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55350230-a9a7-4200-bcc9-548f35b8dfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce8bdec-3aa7-4805-ad99-9d892e27f38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5599ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file in sorted_train_files:\n",
    "    df = pd.read_csv(os.path.join(path_train, file))\n",
    "    \n",
    "    # pad to make single feature files compatible with labels (2 lines for Recola)\n",
    "    # pad the dataframe with a copy of the first row \n",
    "    new_row = pd.DataFrame(df.loc[0])\n",
    "\n",
    "    # simply concatenate both dataframes\n",
    "    df = pd.concat([new_row.T, df]).reset_index(drop = True)\n",
    "    \n",
    "    # pad the dataframe with a copy of the last row \n",
    "    new_row2 = pd.DataFrame(df.loc[len(df)-1])\n",
    "    \n",
    "    # simply concatenate both dataframes\n",
    "    df = pd.concat([df,new_row2.T]).reset_index(drop = True)\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "df_train_feat = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Drop first index column (unamed 0)\n",
    "df_train_feat.drop(df_train_feat.columns[[0]], axis=1, inplace=True)\n",
    "\n",
    "df_train_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_devel = []\n",
    "for file in sorted_devel_files:\n",
    "    df_devel = pd.read_csv(os.path.join(path_devel, file))\n",
    "    \n",
    "    # pad to make single feature files compatible with labels (2 lines for Recola)\n",
    "    # pad the dataframe with a copy of the first row \n",
    "    new_row = pd.DataFrame(df_devel.loc[0])\n",
    "\n",
    "    # simply concatenate both dataframes\n",
    "    df_devel = pd.concat([new_row.T, df_devel]).reset_index(drop = True)\n",
    "    \n",
    "    # pad the dataframe with a copy of the last row \n",
    "    new_row2 = pd.DataFrame(df_devel.loc[len(df_devel)-1])\n",
    "    \n",
    "    # simply concatenate both dataframes\n",
    "    df_devel = pd.concat([df_devel,new_row2.T]).reset_index(drop = True)\n",
    "    \n",
    "    dfs_devel.append(df_devel)\n",
    "    \n",
    "df_devel_feat = pd.concat(dfs_devel, ignore_index=True)\n",
    "\n",
    "# Drop first index column (unamed 0)\n",
    "df_devel_feat.drop(df_devel_feat.columns[[0]], axis=1, inplace=True)\n",
    "\n",
    "df_devel_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a43bb",
   "metadata": {},
   "source": [
    "### Process label files - Train and Devel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dec438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to label files\n",
    "path_train_arousal_labels = 'Recola2018_16k/labels/arousal/Train/'\n",
    "extension = 'arff'\n",
    "\n",
    "train_files_arousal_labels = [file for file in os.listdir(path_train_arousal_labels) if file.endswith(extension)]\n",
    "\n",
    "sorted_train_arousal_labels = sorted(train_files_arousal_labels)\n",
    "sorted_train_arousal_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac9076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to label files\n",
    "path_train_valence_labels = 'Recola2018_16k/labels/valence/Train/'\n",
    "extension = 'arff'\n",
    "\n",
    "train_files_valence_labels = [file for file in os.listdir(path_train_valence_labels) if file.endswith(extension)]\n",
    "\n",
    "sorted_train_valence_labels = sorted(train_files_valence_labels)\n",
    "sorted_train_valence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b775647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to label files\n",
    "path_devel_arousal_labels = 'Recola2018_16k/labels/arousal/Devel/'\n",
    "extension = 'arff'\n",
    "\n",
    "devel_files_arousal_labels = [file for file in os.listdir(path_devel_arousal_labels) if file.endswith(extension)]\n",
    "\n",
    "sorted_devel_arousal_labels = sorted(devel_files_arousal_labels)\n",
    "sorted_devel_arousal_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c74198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to label files\n",
    "path_devel_valence_labels = 'Recola2018_16k/labels/valence/Devel/'\n",
    "extension = 'arff'\n",
    "\n",
    "devel_files_valence_labels = [file for file in os.listdir(path_devel_valence_labels) if file.endswith(extension)]\n",
    "\n",
    "sorted_devel_valence_labels = sorted(devel_files_valence_labels)\n",
    "sorted_devel_valence_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f848d6a",
   "metadata": {},
   "source": [
    "### Create dataframes for label files - Train and Devel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4396d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = []\n",
    "for file in sorted_train_arousal_labels:\n",
    "    df2 = pd.read_csv(os.path.join(path_train_arousal_labels, file), sep=\",\", header=None)\n",
    "    df2.drop(df2.columns[[0,1]], axis=1, inplace=True)\n",
    "\n",
    "    # Add subject column\n",
    "    subject = file.split('.')[0]\n",
    "    df2.insert(0, \"Subject\", subject, True)\n",
    "    \n",
    "    dfl.append(df2)\n",
    "\n",
    "df_train_arousal_lab = pd.concat(dfl, ignore_index=True)\n",
    "df_train_arousal_lab = df_train_arousal_lab.rename(columns={2:\"arousal\"})\n",
    "df_train_arousal_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d695381",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = []\n",
    "for file in sorted_train_valence_labels:\n",
    "    df2 = pd.read_csv(os.path.join(path_train_valence_labels, file), sep=\",\", header=None)\n",
    "    df2.drop(df2.columns[[0,1]], axis=1, inplace=True)\n",
    "    \n",
    "    # Add subject column\n",
    "    subject = file.split('.')[0]\n",
    "    df2.insert(0, \"Subject\", subject, True)\n",
    "\n",
    "    dfl.append(df2)\n",
    "    \n",
    "df_train_valence_lab = pd.concat(dfl, ignore_index=True)\n",
    "df_train_valence_lab = df_train_valence_lab.rename(columns={2:\"valence\"})\n",
    "df_train_valence_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38456f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = []\n",
    "for file in sorted_devel_arousal_labels:\n",
    "    df2 = pd.read_csv(os.path.join(path_devel_arousal_labels, file), sep=\",\", header=None)\n",
    "    df2.drop(df2.columns[[0,1]], axis=1, inplace=True)\n",
    "\n",
    "    # Add subject column\n",
    "    subject = file.split('.')[0]\n",
    "    df2.insert(0, \"Subject\", subject, True)\n",
    "\n",
    "    dfl.append(df2)\n",
    "    \n",
    "df_devel_arousal_lab = pd.concat(dfl, ignore_index=True)\n",
    "df_devel_arousal_lab = df_devel_arousal_lab.rename(columns={2:\"arousal\"})\n",
    "df_devel_arousal_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c8bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = []\n",
    "for file in sorted_devel_valence_labels:\n",
    "    df2 = pd.read_csv(os.path.join(path_devel_valence_labels, file), sep=\",\", header=None)\n",
    "    df2.drop(df2.columns[[0,1]], axis=1, inplace=True)\n",
    "    \n",
    "    # Add subject column\n",
    "    subject = file.split('.')[0]\n",
    "    df2.insert(0, \"Subject\", subject, True)\n",
    "\n",
    "    dfl.append(df2)\n",
    "    \n",
    "df_devel_valence_lab = pd.concat(dfl, ignore_index=True)\n",
    "df_devel_valence_lab = df_devel_valence_lab.rename(columns={2:\"valence\"})\n",
    "df_devel_valence_lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e572759e",
   "metadata": {},
   "source": [
    "## Train a GRU regression model for arousal / valence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73becbce",
   "metadata": {},
   "source": [
    "### Initialize PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a681dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a081309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30c879",
   "metadata": {},
   "source": [
    "### Define CCC loss function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199927ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CCCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CCCLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        mean_pred = torch.mean(pred)\n",
    "        mean_target = torch.mean(target)\n",
    "\n",
    "        covar = torch.mean((pred - mean_pred) * (target - mean_target))\n",
    "        var_pred = torch.var(pred)\n",
    "        var_target = torch.var(target)\n",
    "\n",
    "        ccc = 2 * covar / (var_pred + var_target + (mean_pred - mean_target)**2)\n",
    "        return (1 - ccc)  # Minimize 1 - CCC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed9e59",
   "metadata": {},
   "source": [
    "### Define a PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f58e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1be5d6",
   "metadata": {},
   "source": [
    "#### Must choose arousal or valence here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2814445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_train_feat.iloc[:, 1:770].values.astype(np.float32)\n",
    "\n",
    "# Arousal\n",
    "# labels   = df_train_arousal_lab['arousal'].values.astype(np.float32)\n",
    "# subjects = df_train_arousal_lab['Subject'].values\n",
    "\n",
    "# Valence\n",
    "labels   = df_train_valence_lab['valence'].values.astype(np.float32)\n",
    "subjects = df_train_valence_lab['Subject'].values\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "features_tensor = torch.from_numpy(features)\n",
    "labels_tensor   = torch.from_numpy(labels)\n",
    "\n",
    "# Assuming you want a sequence length of 1\n",
    "# features_tensor = features_tensor.unsqueeze(1)\n",
    "\n",
    "######\n",
    "# Reshape features tensor with sequence length of 50\n",
    "sequence_length = 1\n",
    "num_features    = features.shape[1]\n",
    "num_samples     = features.shape[0]\n",
    "\n",
    "# Calculate the number of sequences that can be formed\n",
    "num_sequences = num_samples // sequence_length\n",
    "\n",
    "# Truncate the tensor to fit the full sequences\n",
    "features_tensor = features_tensor[:num_sequences * sequence_length, :]\n",
    "labels_tensor = labels_tensor[:num_sequences * sequence_length]\n",
    "\n",
    "# Reshape the tensor\n",
    "features_tensor = features_tensor.view(num_sequences, sequence_length, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573014d",
   "metadata": {},
   "source": [
    "### Model parameters and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80750ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_size   = num_features\n",
    "hidden_size  = 32 #128, 64, 32, 16\n",
    "num_layers   = 5\n",
    "output_size  = 1  # Single output for regression between -1 and +1\n",
    "dropout_prob = 0.25 \n",
    "\n",
    "# Train the model\n",
    "num_epochs     = 1000\n",
    "batch_size     = 7501\n",
    "validate_every = 1  # Validate every 2 epochs\n",
    "patience       = 15  # Stop training if validation loss doesn't improve for 5 consecutive validations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f8871",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8260475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Define the GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, dropout_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        output = self.fc(gru_out[:, -1, :])  # Take the output from the last time step\n",
    "        return output\n",
    "\n",
    "# model = GRUModel(input_size, hidden_size, output_size, dropout_prob)\n",
    "\n",
    "# =======================\n",
    "# Define the Convolutional GRU model\n",
    "class ConvGRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
    "        super(ConvGRUModel, self).__init__()\n",
    "        self.convgru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.convgru(x)\n",
    "        output = self.fc(gru_out[:, -1, :])  # Take the output from the last time step\n",
    "        return output\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        # Reset parameters of the GRU layer\n",
    "        for name, param in self.convgru.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "        # Reset parameters of the fully connected layer\n",
    "        self.fc.reset_parameters()\n",
    "\n",
    "model = ConvGRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "#=============================\n",
    "# Define the Convolutional GRU model with Tanh activation at the output\n",
    "class ConvGRUModelTanh(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
    "        super(ConvGRUModelTanh, self).__init__()\n",
    "        self.convgru = nn.GRU(input_size=input_size, hidden_size=hidden_size, dropout=dropout_prob, num_layers=num_layers, batch_first=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.convgru(x)\n",
    "        output = self.fc(gru_out[:, -1, :])  # Take the output from the last time step\n",
    "        output = self.tanh(output)  # Apply Tanh activation\n",
    "        return output\n",
    "\n",
    "#model = ConvGRUModelTanh(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "#=============================\n",
    "# Define the Convolutional BLSTM model with dropout\n",
    "class ConvBLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob=0.2):\n",
    "        super(ConvBLSTMModel, self).__init__()\n",
    "        self.convblstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                                 batch_first=True, bidirectional=True, dropout=dropout_prob)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # Multiply by 2 for bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        blstm_out, _ = self.convblstm(x)\n",
    "        output = self.dropout(blstm_out[:, -1, :])  # Apply dropout before the fully connected layer\n",
    "        output = self.fc(output)  # Take the output from the last time step\n",
    "        return output\n",
    "\n",
    "# model = ConvBLSTMModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "#=============================\n",
    "\n",
    "# Move the selected model to the GPU\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911b020",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "#### Must choose arousal or valence here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0583a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = TensorDataset(X_train, y_train.unsqueeze(1))\n",
    "#train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "import csv\n",
    "\n",
    "def Model_Train(X_train, X_test, y_train, y_test, y_subject, batch_size, num_epochs, model): \n",
    "        \n",
    "    model.reset_parameters()\n",
    "    \n",
    "    # Move the selected model to the GPU\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Initialize a list to store the training loss values\n",
    "    train_loss_values      = []\n",
    "    validation_loss_values = []\n",
    "    \n",
    "    # Initialize a list to store the best training epoch and the best validation loss\n",
    "    best_validation_loss     = float('inf')\n",
    "    early_stop_counter       = 0\n",
    "    \n",
    "    # Aroural\n",
    "    # best_model_path          = 'best_model_recola_arousal_loso.pth'  # Define the path to save the best model\n",
    "    # best_model_epochs        = 'best_model_recola_arousal_loso_epochs.csv'  # Define the path to save the best model\n",
    "    \n",
    "    # Valence\n",
    "    best_model_path          = 'best_model_recola_valence_loso.pth'  # Define the path to save the best model\n",
    "    best_model_epochs        = 'best_model_recola_valence_loso_epochs.csv'  # Define the path to save the best model\n",
    "\n",
    "    #criterion = nn.MSELoss()\n",
    "    criterion = CCCLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    #train_dataset = TensorDataset(X_train, y_train.unsqueeze(1))\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "    train_loader  = DataLoader(train_dataset, batch_size = batch_size, shuffle=False)\n",
    "    \n",
    "    epoch_best = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "    \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X.to(device))\n",
    "            #loss    = criterion(outputs, batch_y.to(device))\n",
    "            loss    = criterion(outputs, batch_y.unsqueeze(1).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # print every 10 epochs only\n",
    "        #if epoch % 10 == 0:\n",
    "        \n",
    "        print(f'Training epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "        average_epoch_loss = epoch_loss / len(train_loader)\n",
    "        train_loss_values.append(average_epoch_loss)\n",
    "    \n",
    "        # Validate the model every validate_every epochs using the test partition\n",
    "        if epoch % validate_every == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_outputs = model(X_test.to(device))\n",
    "                validation_loss = criterion(test_outputs, y_test.unsqueeze(1).to(device))  # Adjust target size\n",
    "\n",
    "            validation_loss_values.append(validation_loss.item())\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {validation_loss.item():.4f}')\n",
    "        \n",
    "            if validation_loss < best_validation_loss:\n",
    "                best_validation_loss = validation_loss\n",
    "                early_stop_counter = 0\n",
    "            \n",
    "                # Save the model with the best validation loss\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f'Saved model with best validation loss to {best_model_path}')\n",
    "                epoch_best = epoch       \n",
    "                \n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1} as validation loss has not improved for {patience} consecutive validations.')\n",
    "                print(f'Best training epoch {epoch_best} at validation.')\n",
    "                break\n",
    "            \n",
    "            model.train()  # Set the model back to training mode\n",
    "            \n",
    "    # write to file subject / best epoch train        \n",
    "    df_temp = pd.DataFrame( [ [ y_subject, epoch_best, best_validation_loss.cpu() ] ] )\n",
    "    df_temp.to_csv(best_model_epochs, mode='a', header=False )        \n",
    "            \n",
    "    Plot_Train_Val_Curvs(train_loss_values, validation_loss_values, validate_every)         \n",
    "            \n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    model.to('cpu')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92af1d2",
   "metadata": {},
   "source": [
    "### Evaluate the trained model, selecting the best validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# Load the best model for testing\n",
    "#best_model = ConvGRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "def Model_Test_Best(X_test, y_test, best_model): \n",
    "    \n",
    "    criterion = CCCLoss()\n",
    "    \n",
    "    #best_model = ConvBLSTMModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "    #best_model.load_state_dict(torch.load(best_model_path))\n",
    "    best_model = best_model.to(device)\n",
    "\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = best_model(X_test.to(device))\n",
    "        test_loss    = criterion(test_outputs, y_test.unsqueeze(1).to(device))\n",
    "\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "    \n",
    "    return test_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763658a0",
   "metadata": {},
   "source": [
    "### Plot training and validation CCC loss X epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab1b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Plot_Train_Val_Curvs(train_loss_values, validation_loss_values, validate_every):\n",
    "    # Plot the training and validation loss values\n",
    "    epochs = range(1, len(train_loss_values) + 1)\n",
    "    plt.plot(epochs, train_loss_values, label='Training Loss')\n",
    "    plt.plot(range(0, len(validation_loss_values) * validate_every, validate_every), validation_loss_values, label='Validation Loss', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b9d74",
   "metadata": {},
   "source": [
    "### Define LOSO Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8428b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Create model and grouping object\n",
    "best_model = model \n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "def experiment(\n",
    "    feat,\n",
    "    targ,\n",
    "    groups,\n",
    "):        \n",
    "    truths = []\n",
    "    preds  = []\n",
    "\n",
    "    for train_index, test_index in logo.split(\n",
    "        feat, \n",
    "        targ, \n",
    "        groups=groups,\n",
    "    ):\n",
    "        train_x = feat.iloc[train_index]\n",
    "        train_y = targ[train_index]\n",
    "\n",
    "        test_x = feat.iloc[test_index]\n",
    "        test_y = targ[test_index]\n",
    "        \n",
    "        print(str(train_index) + \" \" + str(test_index)) \n",
    "        print(\"Validation subject: \" + str(groups[test_index[0]])) \n",
    "        \n",
    "        # Convert to pytorch\n",
    "        \n",
    "        ### TRAIN\n",
    "        \n",
    "        features = train_x.values.astype(np.float32)\n",
    "        labels   = train_y.values.astype(np.float32)\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        features_tensor = torch.from_numpy(features)\n",
    "        labels_tensor   = torch.from_numpy(labels)\n",
    "\n",
    "        # Assuming you want a sequence length of 1\n",
    "        # features_tensor = features_tensor.unsqueeze(1)\n",
    "\n",
    "        ######\n",
    "        # Reshape features tensor with sequence length of 50\n",
    "        sequence_length = 1\n",
    "        num_features    = features.shape[1]\n",
    "        num_samples     = features.shape[0]\n",
    "\n",
    "        # Calculate the number of sequences that can be formed\n",
    "        num_sequences = num_samples // sequence_length\n",
    "\n",
    "        # Truncate the tensor to fit the full sequences\n",
    "        features_tensor = features_tensor[:num_sequences * sequence_length, :]\n",
    "        labels_tensor = labels_tensor[:num_sequences * sequence_length]\n",
    "\n",
    "        # Reshape the tensor\n",
    "        features_tensor = features_tensor.view(num_sequences, sequence_length, num_features)\n",
    "\n",
    "        ### DEVEL\n",
    "\n",
    "        features2 = test_x.values.astype(np.float32)\n",
    "        labels2   = test_y.values.astype(np.float32)\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        features_tensor2 = torch.from_numpy(features2)\n",
    "        labels_tensor2   = torch.from_numpy(labels2)\n",
    "\n",
    "        ######\n",
    "        # Reshape features tensor with sequence length of 50\n",
    "        sequence_length = 1\n",
    "        num_features = features2.shape[1]\n",
    "        num_samples  = features2.shape[0]\n",
    "\n",
    "        # Calculate the number of sequences that can be formed\n",
    "        num_sequences = num_samples // sequence_length\n",
    "\n",
    "        # Truncate the tensor to fit the full sequences\n",
    "        features_tensor2 = features_tensor2[:num_sequences * sequence_length, :]\n",
    "        labels_tensor2   = labels_tensor2[:num_sequences * sequence_length]\n",
    "\n",
    "        # Reshape the tensor\n",
    "        features_tensor2 = features_tensor2.view(num_sequences, sequence_length, num_features)\n",
    "        ######\n",
    "        \n",
    "        # Train model\n",
    "        \n",
    "        subject = str(groups[test_index[0]])\n",
    "        \n",
    "        best_model = Model_Train(features_tensor, features_tensor2, labels_tensor, labels_tensor2, subject, batch_size, num_epochs, model)\n",
    "        \n",
    "        print(\"This is the best model\" + str(best_model))\n",
    "        \n",
    "        # Test model\n",
    "        predict_y =  Model_Test_Best(features_tensor2, labels_tensor2, best_model)\n",
    "        \n",
    "        truths.append(test_y)\n",
    "        preds.append(predict_y.cpu().squeeze(1))\n",
    "        \n",
    "    # combine subject folds\n",
    "    truth = pd.concat(truths)\n",
    "    truth.name = 'truth'\n",
    "    \n",
    "    \n",
    "    \n",
    "    pred = pd.Series(\n",
    "        np.concatenate(preds),\n",
    "        index=truth.index,\n",
    "        name='prediction',\n",
    "    )\n",
    "    \n",
    "    return truth, pred"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c78150c8",
   "metadata": {},
   "source": [
    "feat = df_train_feat.iloc[:, 1:770]\n",
    "targ = df_train_valence_lab['valence']\n",
    "groups = df_train_valence_lab['Subject']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2bf68870",
   "metadata": {},
   "source": [
    "    truths = []\n",
    "    preds  = []\n",
    "\n",
    "    for train_index, test_index in logo.split(\n",
    "        feat, \n",
    "        targ, \n",
    "        groups=groups,\n",
    "    ):\n",
    "        train_x = feat.iloc[train_index]\n",
    "        train_y = targ[train_index]\n",
    "\n",
    "        test_x = feat.iloc[test_index]\n",
    "        test_y = targ[test_index]\n",
    "        \n",
    "        # print(str(train_index) + \" \" + str(test_index)) \n",
    "        print(\"Validation subject: \" + str(groups[test_index[0]])) \n",
    "        \n",
    "        # Convert to pytorch\n",
    "        \n",
    "        ### TRAIN\n",
    "        \n",
    "        features = train_x.values.astype(np.float32)\n",
    "        labels   = train_y.values.astype(np.float32)\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        features_tensor = torch.from_numpy(features)\n",
    "        labels_tensor   = torch.from_numpy(labels)\n",
    "\n",
    "        # Assuming you want a sequence length of 1\n",
    "        # features_tensor = features_tensor.unsqueeze(1)\n",
    "\n",
    "        ######\n",
    "        # Reshape features tensor with sequence length of 50\n",
    "        sequence_length = 1\n",
    "        num_features    = features.shape[1]\n",
    "        num_samples     = features.shape[0]\n",
    "\n",
    "        # Calculate the number of sequences that can be formed\n",
    "        num_sequences = num_samples // sequence_length\n",
    "\n",
    "        # Truncate the tensor to fit the full sequences\n",
    "        features_tensor = features_tensor[:num_sequences * sequence_length, :]\n",
    "        labels_tensor = labels_tensor[:num_sequences * sequence_length]\n",
    "\n",
    "        # Reshape the tensor\n",
    "        features_tensor = features_tensor.view(num_sequences, sequence_length, num_features)\n",
    "\n",
    "        ### VALIDATION\n",
    "\n",
    "        features2 = test_x.values.astype(np.float32)\n",
    "        labels2   = test_y.values.astype(np.float32)\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        features_tensor2 = torch.from_numpy(features2)\n",
    "        labels_tensor2   = torch.from_numpy(labels2)\n",
    "\n",
    "        ######\n",
    "        # Reshape features tensor with sequence length of 50\n",
    "        sequence_length = 1\n",
    "        num_features = features2.shape[1]\n",
    "        num_samples  = features2.shape[0]\n",
    "\n",
    "        # Calculate the number of sequences that can be formed\n",
    "        num_sequences = num_samples // sequence_length\n",
    "\n",
    "        # Truncate the tensor to fit the full sequences\n",
    "        features_tensor2 = features_tensor2[:num_sequences * sequence_length, :]\n",
    "        labels_tensor2   = labels_tensor2[:num_sequences * sequence_length]\n",
    "\n",
    "        # Reshape the tensor\n",
    "        features_tensor2 = features_tensor2.view(num_sequences, sequence_length, num_features)\n",
    "        ######\n",
    "        \n",
    "        # Train model\n",
    "        \n",
    "        best_model = Model_Train(features_tensor, features_tensor2, labels_tensor, labels_tensor2, batch_size, num_epochs, model)\n",
    "        \n",
    "        print(\"This is the best model\" + str(best_model))\n",
    "        \n",
    "        # Test model\n",
    "        predict_y =  Model_Test_Best(features_tensor2, labels_tensor2, best_model)\n",
    "        \n",
    "        truths.append(test_y)\n",
    "        preds.append(predict_y.cpu().squeeze(1))\n",
    "        \n",
    "        \n",
    "    # combine subject folds\n",
    "    truth = pd.concat(truths)\n",
    "    truth.name = 'truth'\n",
    "    \n",
    "    \n",
    "    \n",
    "    pred = pd.Series( np.concatenate(preds),\n",
    "        index=truth.index,\n",
    "        name='prediction')\n",
    "    \n",
    "    \n",
    "    return truth, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206bebf",
   "metadata": {},
   "source": [
    "### Select Arousal or Valence"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ed60a1c",
   "metadata": {},
   "source": [
    "# Arousal\n",
    "\n",
    "truth_wavlm, pred_wavlm = experiment(\n",
    "    df_train_feat.iloc[:, 1:770],\n",
    "    df_train_arousal_lab['arousal'],\n",
    "    df_train_arousal_lab['Subject'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1fa969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valence\n",
    "\n",
    "truth_wavlm, pred_wavlm = experiment(\n",
    "    df_train_feat.iloc[:, 1:770],\n",
    "    df_train_valence_lab['valence'],\n",
    "    df_train_valence_lab['Subject'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import audmetric\n",
    "\n",
    "audmetric.concordance_cc(truth_wavlm, pred_wavlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3ef173",
   "metadata": {},
   "source": [
    "### Load devel dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b13114",
   "metadata": {},
   "source": [
    "#### Must select arousal or valence here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a7b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = df_devel_feat.iloc[:, 1:770].values.astype(np.float32)\n",
    "#features2 = df_devel_feat.values.astype(np.float32)\n",
    "\n",
    "\n",
    "# Arousal\n",
    "# labels2   = df_devel_arousal_lab['arousal'].values.astype(np.float32)\n",
    "\n",
    "# Valence\n",
    "labels2   = df_devel_valence_lab['valence'].values.astype(np.float32)\n",
    "\n",
    "# Normalize the features between -1 and 1 (adjust scaling based on your data)\n",
    "# features2 = (features - np.min(features)) / (np.max(features) - np.min(features)) * 2 - 1\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "features_tensor2 = torch.from_numpy(features2)\n",
    "labels_tensor2   = torch.from_numpy(labels2)\n",
    "\n",
    "######\n",
    "# Reshape features tensor with sequence length of 50\n",
    "sequence_length = 1\n",
    "num_features = features2.shape[1]\n",
    "num_samples  = features2.shape[0]\n",
    "\n",
    "# Calculate the number of sequences that can be formed\n",
    "num_sequences = num_samples // sequence_length\n",
    "\n",
    "# Truncate the tensor to fit the full sequences\n",
    "features_tensor2 = features_tensor2[:num_sequences * sequence_length, :]\n",
    "labels_tensor2   = labels_tensor2[:num_sequences * sequence_length]\n",
    "\n",
    "# Reshape the tensor\n",
    "features_tensor2 = features_tensor2.view(num_sequences, sequence_length, num_features)\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207694e1",
   "metadata": {},
   "source": [
    "### Load best model and predict\n",
    "\n",
    "#### Must select arousal or valence here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d3b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model for testing\n",
    "\n",
    "# Arousal\n",
    "# best_model_path = 'best_model_recola_arousal_loso.pth'  # Define the path to save the best model\n",
    "\n",
    "# Valence\n",
    "best_model_path = 'best_model_recola_valence_loso.pth'  # Define the path to save the best model\n",
    "\n",
    "criterion = CCCLoss()\n",
    "\n",
    "###############################################################################################\n",
    "best_model = ConvGRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "# best_model = ConvBLSTMModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "###############################################################################################\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "best_model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_outputs = best_model(features_tensor2.to(device))\n",
    "    test_loss    = criterion(test_outputs, labels_tensor2.unsqueeze(1).to(device))\n",
    "\n",
    "print(f'Test Loss: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab0d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = test_outputs.cpu().squeeze(1)\n",
    "truth = labels_tensor2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37240539",
   "metadata": {},
   "source": [
    "### Arousal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ba994",
   "metadata": {},
   "source": [
    "#### Smooth function to smooth predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b7dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "def F_Smooth(prediction, win_width):\n",
    "\n",
    "    df_pred = pd.DataFrame(prediction)\n",
    "    width = win_width\n",
    "    lag1 = df_pred.shift(1)\n",
    "    lag3 = df_pred.shift(width - 1)\n",
    "    window = lag3.rolling(window=width)\n",
    "    means = window.mean()\n",
    "    df_smoothed = concat([means, lag1, df_pred], axis=1)\n",
    "    df_smoothed.columns = ['mean', 't-1', 't+1']\n",
    "    df_smoothed['mean'] = df_smoothed['mean'].fillna(df_smoothed['t+1'])\n",
    "    \n",
    "    return df_smoothed['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117f67aa",
   "metadata": {},
   "source": [
    "#### Predict on Devel set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4462382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import audmetric\n",
    "\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, pred)))\n",
    "print(\"MSE = \" + str(audmetric.mean_absolute_error(truth, pred)))\n",
    "print(\"MAE = \" + str(audmetric.mean_squared_error(truth, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba17125",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 5)))  + \"  ( 5) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 10))) + \"  (10) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 15))) + \"  (15) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 20))) + \"  (20) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 25))) + \"  (25) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 30))) + \"  (30) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 35))) + \"  (35) \")\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 40))) + \"  (40) \")\n",
    "\n",
    "print(\"MSE = \" + str(audmetric.mean_absolute_error(truth, F_Smooth(pred, 20)))+ \" (20)\")\n",
    "print(\"MAE = \" + str(audmetric.mean_squared_error(truth, F_Smooth(pred, 20)))+ \" (20)\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "785242d4",
   "metadata": {},
   "source": [
    "##### Best results so far Arousal Devel with LOSO at training:\n",
    "\n",
    "Test Loss: 0.2893\n",
    "/DEV/WavLM/Recola2018_16k/models/best_model_recola_arousal_loso_03032024.pth\n",
    "/DEV/WavLM/Recola2018_16k/models/best_model_recola_arousal_loso_epochs_03032024.csv\n",
    "\n",
    "CCC = 0.7107511144810469\n",
    "\n",
    "MSE = 0.11390312016010284\n",
    "MAE = 0.02063915506005287\n",
    "\n",
    "With smooth\n",
    "CCC = 0.7486076793101336  ( 5) \n",
    "CCC = 0.7687387950728284  (10) \n",
    "CCC = 0.7789464628838636  (15) \n",
    "CCC = 0.7829851476119157  (20) \n",
    "CCC = 0.7825649170753243  (25) \n",
    "CCC = 0.7789514500423856  (30) \n",
    "CCC = 0.7727902909048278  (35) \n",
    "CCC = 0.7648225374016705  (40)\n",
    "\n",
    "MSE = 0.09611338156505336 (20)\n",
    "MAE = 0.014646904760840154 (20)\n",
    "\n",
    "best_model = ConvGRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size   = num_features\n",
    "hidden_size  = 32 #128, 64, 32, 16\n",
    "num_layers   = 6\n",
    "output_size  = 1  # Single output for regression between -1 and +1\n",
    "dropout_prob = 0.20 \n",
    "\n",
    "# Train the model\n",
    "num_epochs     = 1000\n",
    "batch_size     = 7501\n",
    "validate_every = 1  # Validate every 2 epochs\n",
    "patience       = 15  # Stop training if validation loss doesn't improve for 5 consecutive validations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f63e3e",
   "metadata": {},
   "source": [
    "### Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import audmetric\n",
    "\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, pred)))\n",
    "print(\"MSE = \" + str(audmetric.mean_absolute_error(truth, pred)))\n",
    "print(\"MAE = \" + str(audmetric.mean_squared_error(truth, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7aa70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 5))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 10))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 15))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 20))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 25))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 30))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 35))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 40))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 45))))\n",
    "print(\"CCC = \" + str(audmetric.concordance_cc(truth, F_Smooth(pred, 50))))\n",
    "\n",
    "print(\"MSE = \" + str(audmetric.mean_absolute_error(truth, F_Smooth(pred, 25))))\n",
    "print(\"MAE = \" + str(audmetric.mean_squared_error(truth, F_Smooth(pred, 25))))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1b5d3f8",
   "metadata": {},
   "source": [
    "##### Best results so far Valence Devel:\n",
    "\n",
    "Test Loss: 0.6367\n",
    "/DEV/WavLM/Recola2018_16k/models/best_model_recola_valence_loso_03032024.pth\n",
    "/DEV/WavLM/Recola2018_16k/models/best_model_recola_valence_loso_epochs_03032024.csv\n",
    "\n",
    "CCC = 0.35387330344214085\n",
    "\n",
    "MSE = 0.10845673829317093\n",
    "MAE = 0.01992058753967285\n",
    "\n",
    "With Smooth\n",
    "CCC = 0.4158365728866413\n",
    "CCC = 0.4485323174502916\n",
    "CCC = 0.4689329307149778\n",
    "CCC = 0.48165148669266095\n",
    "CCC = 0.4888527510582969\n",
    "CCC = 0.4921716187434845\n",
    "CCC = 0.4926799810218907\n",
    "CCC = 0.49087949801229924\n",
    "CCC = 0.48723823316745624\n",
    "CCC = 0.4821973971633789\n",
    "\n",
    "MSE = 0.08750272508878654\n",
    "MAE = 0.012485581522185363\n",
    "\n",
    "best_model = ConvGRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size   = num_features\n",
    "hidden_size  = 32 #128, 64, 32, 16\n",
    "num_layers   = 5\n",
    "output_size  = 1  # Single output for regression between -1 and +1\n",
    "dropout_prob = 0.25 \n",
    "\n",
    "# Train the model\n",
    "num_epochs     = 1000\n",
    "batch_size     = 7501\n",
    "validate_every = 1  # Validate every 2 epochs\n",
    "patience       = 15  # Stop training if validation loss doesn't improve for 5 consecutive validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac44fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "predict_y = F_Smooth(pred, 5)\n",
    "\n",
    "dt = 1\n",
    "t = np.arange(0, len(predict_y), dt)\n",
    "s1 = df_devel_arousal_lab['arousal']\n",
    "s2 = predict_y                # white noise 2\n",
    "\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "axs[0].plot(t, s1, t, s2)\n",
    "axs[0].set_xlim(0, len(predict_y))\n",
    "axs[0].set_xlabel('time')\n",
    "axs[0].set_ylabel('s1 and s2')\n",
    "axs[0].grid(True)\n",
    "\n",
    "cxy, f = axs[1].cohere(s1, s2, 256, 1. / dt)\n",
    "axs[1].set_ylabel('coherence')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820cf17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "plt.figure(figsize=(200,6))\n",
    "#fig.tight_layout()\n",
    "\n",
    "predict_y = F_Smooth(pred, 20)\n",
    "\n",
    "dt = 1\n",
    "t = np.arange(0, len(predict_y), dt)\n",
    "s1 = df_devel_valence_lab['valence']\n",
    "s2 = predict_y               # white noise 2\n",
    "\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "axs[0].plot(t, s2, t, s1)\n",
    "axs[0].set_xlim(0, len(predict_y))\n",
    "axs[0].set_xlabel('time')\n",
    "axs[0].set_ylabel('s1 and s2')\n",
    "axs[0].grid(True)\n",
    "\n",
    "cxy, f = axs[1].cohere(s1, s2, 256, 1. / dt)\n",
    "axs[1].set_ylabel('coherence')\n",
    "\n",
    "#plt.figure(figsize=(20,6))\n",
    "#fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa1d665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f0378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436aea6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
